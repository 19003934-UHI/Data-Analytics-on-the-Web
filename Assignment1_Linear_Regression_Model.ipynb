{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19003934-UHI/Data_Analytics/blob/main/Assignment1_Linear_Regression_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h64yz-ke5sWB"
      },
      "source": [
        "## Linear Regression Model \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "e1uLMJsMb2Hg",
        "outputId": "5da3510c-aad4-4000-bad0-f50f9c42b7f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     year  mo  da collision_date  temp  dewp  visib  wdsp  mxpsd   gust   max  \\\n",
              "day                                                                             \n",
              "7    2018   9  23     2018-09-23  59.8  50.2   10.0   3.0    5.1  999.9  78.1   \n",
              "4    2018  12  20     2018-12-20  38.6  34.4    9.6   5.0    7.0  999.9  48.0   \n",
              "2    2013  11   5     2013-11-05  43.5  30.4   10.0   3.9    7.0  999.9  50.0   \n",
              "3    2012   7  11     2012-07-11  77.1  62.0   10.0   1.9    7.0  999.9  84.0   \n",
              "4    2012   8   9     2012-08-09  78.2  69.6    9.3   2.3    7.0   15.0  88.0   \n",
              "4    2012   7  12     2012-07-12  77.4  60.2   10.0   2.3    7.0  999.9  88.0   \n",
              "2    2017   9  26     2017-09-26  65.9  65.3    2.6   3.4    7.0  999.9  77.0   \n",
              "1    2012   9  17     2012-09-17  66.5  48.0   10.0   2.2    7.0  999.9  77.0   \n",
              "4    2020   9  10     2020-09-10  70.8  69.5    0.8   4.4    7.0  999.9  77.0   \n",
              "1    2014  11  10     2014-11-10  48.2  42.2    9.6   4.0    7.0  999.9  59.0   \n",
              "\n",
              "      min  prcp   sndp  fog  NUM_COLLISIONS  \n",
              "day                                          \n",
              "7    53.1  0.00  999.9    0             475  \n",
              "4    21.0  0.00  999.9    0             806  \n",
              "2    37.9  0.00  999.9    0             510  \n",
              "3    64.9  0.00  999.9    0             565  \n",
              "4    61.0  0.00  999.9    0             581  \n",
              "4    57.0  0.00  999.9    0             577  \n",
              "2    59.0  0.12  999.9    1             656  \n",
              "1    48.2  0.00  999.9    0             452  \n",
              "4    66.9  0.00  999.9    1             347  \n",
              "1    39.0  0.00  999.9    0             560  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b553e8d-5604-40da-9b9a-091b75c85f90\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>collision_date</th>\n",
              "      <th>temp</th>\n",
              "      <th>dewp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>gust</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>prcp</th>\n",
              "      <th>sndp</th>\n",
              "      <th>fog</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2018</td>\n",
              "      <td>9</td>\n",
              "      <td>23</td>\n",
              "      <td>2018-09-23</td>\n",
              "      <td>59.8</td>\n",
              "      <td>50.2</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>999.9</td>\n",
              "      <td>78.1</td>\n",
              "      <td>53.1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018</td>\n",
              "      <td>12</td>\n",
              "      <td>20</td>\n",
              "      <td>2018-12-20</td>\n",
              "      <td>38.6</td>\n",
              "      <td>34.4</td>\n",
              "      <td>9.6</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>48.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>2013-11-05</td>\n",
              "      <td>43.5</td>\n",
              "      <td>30.4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>50.0</td>\n",
              "      <td>37.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2012-07-11</td>\n",
              "      <td>77.1</td>\n",
              "      <td>62.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>84.0</td>\n",
              "      <td>64.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>2012-08-09</td>\n",
              "      <td>78.2</td>\n",
              "      <td>69.6</td>\n",
              "      <td>9.3</td>\n",
              "      <td>2.3</td>\n",
              "      <td>7.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>2012-07-12</td>\n",
              "      <td>77.4</td>\n",
              "      <td>60.2</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>88.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>9</td>\n",
              "      <td>26</td>\n",
              "      <td>2017-09-26</td>\n",
              "      <td>65.9</td>\n",
              "      <td>65.3</td>\n",
              "      <td>2.6</td>\n",
              "      <td>3.4</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>77.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>999.9</td>\n",
              "      <td>1</td>\n",
              "      <td>656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>2012-09-17</td>\n",
              "      <td>66.5</td>\n",
              "      <td>48.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>77.0</td>\n",
              "      <td>48.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>2020-09-10</td>\n",
              "      <td>70.8</td>\n",
              "      <td>69.5</td>\n",
              "      <td>0.8</td>\n",
              "      <td>4.4</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>77.0</td>\n",
              "      <td>66.9</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>1</td>\n",
              "      <td>347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>2014-11-10</td>\n",
              "      <td>48.2</td>\n",
              "      <td>42.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>59.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b553e8d-5604-40da-9b9a-091b75c85f90')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b553e8d-5604-40da-9b9a-091b75c85f90 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b553e8d-5604-40da-9b9a-091b75c85f90');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# For data frames\n",
        "import pandas as pd\n",
        "# For fast calculations\n",
        "import numpy as np\n",
        "# Create data frame from dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/19003934-UHI/Data_Analytics/main/bquxjob_49d5e1e3_1872d26a08d.csv\", index_col=0,)\n",
        "\n",
        "# Print out data\n",
        "df[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "C7kq0dDHmFis",
        "outputId": "efb94bdb-1c7d-44fe-8945-561bad617a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     year  mo  da collision_date  temp  dewp  visib  wdsp  mxpsd   gust   max  \\\n",
              "day                                                                             \n",
              "3    2012   7  11     2012-07-11  77.1  62.0   10.0   1.9    7.0  999.9  84.0   \n",
              "4    2012   8   9     2012-08-09  78.2  69.6    9.3   2.3    7.0   15.0  88.0   \n",
              "4    2012   7  12     2012-07-12  77.4  60.2   10.0   2.3    7.0  999.9  88.0   \n",
              "1    2012   9  17     2012-09-17  66.5  48.0   10.0   2.2    7.0  999.9  77.0   \n",
              "5    2012   8  24     2012-08-24  77.3  60.9    9.3   2.0    7.0  999.9  87.1   \n",
              "5    2012   7  13     2012-07-13  78.9  60.7    9.9   3.0    7.0  999.9  87.1   \n",
              "5    2012  10  26     2012-10-26  60.8  53.4   10.0   1.5    7.0  999.9  64.9   \n",
              "6    2012   7  28     2012-07-28  75.5  69.6    7.0   3.9    8.0   15.9  84.0   \n",
              "7    2012   7  22     2012-07-22  72.5  59.4   10.0   2.0    8.0  999.9  82.0   \n",
              "5    2012   9   7     2012-09-07  77.0  67.8    8.7   2.7    8.0  999.9  86.0   \n",
              "\n",
              "      min  prcp   sndp  fog  NUM_COLLISIONS  \n",
              "day                                          \n",
              "3    64.9   0.0  999.9    0             565  \n",
              "4    61.0   0.0  999.9    0             581  \n",
              "4    57.0   0.0  999.9    0             577  \n",
              "1    48.2   0.0  999.9    0             452  \n",
              "5    66.0   0.0  999.9    0             601  \n",
              "5    63.0   0.0  999.9    0             603  \n",
              "5    39.2   0.0  999.9    0             598  \n",
              "6    68.0   0.0  999.9    1             513  \n",
              "7    54.0   0.0  999.9    0             498  \n",
              "5    64.4   0.0  999.9    0             680  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97793984-335f-4d5b-a8eb-b7b3bdbceaa6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>mo</th>\n",
              "      <th>da</th>\n",
              "      <th>collision_date</th>\n",
              "      <th>temp</th>\n",
              "      <th>dewp</th>\n",
              "      <th>visib</th>\n",
              "      <th>wdsp</th>\n",
              "      <th>mxpsd</th>\n",
              "      <th>gust</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>prcp</th>\n",
              "      <th>sndp</th>\n",
              "      <th>fog</th>\n",
              "      <th>NUM_COLLISIONS</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>2012-07-11</td>\n",
              "      <td>77.1</td>\n",
              "      <td>62.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>84.0</td>\n",
              "      <td>64.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>2012-08-09</td>\n",
              "      <td>78.2</td>\n",
              "      <td>69.6</td>\n",
              "      <td>9.3</td>\n",
              "      <td>2.3</td>\n",
              "      <td>7.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>2012-07-12</td>\n",
              "      <td>77.4</td>\n",
              "      <td>60.2</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>88.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012</td>\n",
              "      <td>9</td>\n",
              "      <td>17</td>\n",
              "      <td>2012-09-17</td>\n",
              "      <td>66.5</td>\n",
              "      <td>48.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>77.0</td>\n",
              "      <td>48.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2012</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>2012-08-24</td>\n",
              "      <td>77.3</td>\n",
              "      <td>60.9</td>\n",
              "      <td>9.3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>87.1</td>\n",
              "      <td>66.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>2012-07-13</td>\n",
              "      <td>78.9</td>\n",
              "      <td>60.7</td>\n",
              "      <td>9.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>87.1</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2012</td>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "      <td>2012-10-26</td>\n",
              "      <td>60.8</td>\n",
              "      <td>53.4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>7.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>64.9</td>\n",
              "      <td>39.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>2012-07-28</td>\n",
              "      <td>75.5</td>\n",
              "      <td>69.6</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>8.0</td>\n",
              "      <td>15.9</td>\n",
              "      <td>84.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>1</td>\n",
              "      <td>513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "      <td>22</td>\n",
              "      <td>2012-07-22</td>\n",
              "      <td>72.5</td>\n",
              "      <td>59.4</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>82.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2012</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>2012-09-07</td>\n",
              "      <td>77.0</td>\n",
              "      <td>67.8</td>\n",
              "      <td>8.7</td>\n",
              "      <td>2.7</td>\n",
              "      <td>8.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>86.0</td>\n",
              "      <td>64.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>999.9</td>\n",
              "      <td>0</td>\n",
              "      <td>680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97793984-335f-4d5b-a8eb-b7b3bdbceaa6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97793984-335f-4d5b-a8eb-b7b3bdbceaa6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97793984-335f-4d5b-a8eb-b7b3bdbceaa6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Converting data to include indexing for easier manipulation, done for each year between 2012 and 2020.\n",
        "print(2012)\n",
        "collisions2012temp = df[df['year'] == 2012]\n",
        "dayofyear = range(1, len(collisions2012temp)+1)\n",
        "dayindex = range(1, len(collisions2012temp)+1)\n",
        "collisions2012 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2012temp.reset_index(drop=True)], axis=1)\n",
        "collisions2012temp[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkQ9I4DbmrJX",
        "outputId": "046d1811-ce22-4820-bb6a-bb7b6fb603ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    365.000000\n",
              "mean     558.175342\n",
              "std       83.273922\n",
              "min      257.000000\n",
              "25%      505.000000\n",
              "50%      558.000000\n",
              "75%      613.000000\n",
              "max      867.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "print(2013)\n",
        "collisions2013temp = df[df['year'] == 2013]\n",
        "dayofyear = range(1, len(collisions2013temp)+1)\n",
        "dayindex = range(1, len(collisions2013temp)+1)\n",
        "collisions2013 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2013temp.reset_index(drop=True)], axis=1)\n",
        "collisions2013['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gw38J0tmtxL",
        "outputId": "0e5eb95b-29d5-4a5f-feca-6a5dfe78a6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     424.000000\n",
              "mean      564.257075\n",
              "std        90.034285\n",
              "min       303.000000\n",
              "25%       512.750000\n",
              "50%       567.500000\n",
              "75%       620.250000\n",
              "max      1161.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "print(2014)\n",
        "collisions2014temp = df[df['year'] == 2014]\n",
        "dayofyear = range(1, len(collisions2014temp)+1)\n",
        "dayindex = range(1, len(collisions2014temp)+1)\n",
        "collisions2014 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2014temp.reset_index(drop=True)], axis=1)\n",
        "collisions2014['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuAmvDEFmvDL",
        "outputId": "6447189a-0e4d-4085-f54f-0adbcb580c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    365.000000\n",
              "mean     596.421918\n",
              "std      102.801594\n",
              "min      188.000000\n",
              "25%      532.000000\n",
              "50%      601.000000\n",
              "75%      666.000000\n",
              "max      961.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "print(2015)\n",
        "collisions2015temp = df[df['year'] == 2015]\n",
        "dayofyear = range(1, len(collisions2015temp)+1)\n",
        "dayindex = range(1, len(collisions2015temp)+1)\n",
        "collisions2015 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2015temp.reset_index(drop=True)], axis=1)\n",
        "collisions2015['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW8SE_44m0Me",
        "outputId": "7ec212ad-d9e3-4a4c-be4e-29dd9a6d2c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    366.000000\n",
              "mean     627.953552\n",
              "std       97.433957\n",
              "min      291.000000\n",
              "25%      568.250000\n",
              "50%      635.000000\n",
              "75%      696.750000\n",
              "max      872.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "print(2016)\n",
        "collisions2016temp = df[df['year'] == 2016]\n",
        "dayofyear = range(1, len(collisions2016temp)+1)\n",
        "dayindex = range(1, len(collisions2016temp)+1)\n",
        "collisions2016 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2016temp.reset_index(drop=True)], axis=1)\n",
        "collisions2016['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g4icwVQm6i3",
        "outputId": "6ec609a8-65da-4416-c4a8-6782edab4126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    365.000000\n",
              "mean     632.895890\n",
              "std      102.339261\n",
              "min      333.000000\n",
              "25%      570.000000\n",
              "50%      636.000000\n",
              "75%      700.000000\n",
              "max      999.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "print(2017)\n",
        "collisions2017temp = df[df['year'] == 2017]\n",
        "dayofyear = range(1, len(collisions2017temp)+1)\n",
        "dayindex = range(1, len(collisions2017temp)+1)\n",
        "collisions2017 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2017temp.reset_index(drop=True)], axis=1)\n",
        "collisions2017['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqBK4ShXm_Ji",
        "outputId": "76d1030c-7d1d-465f-98c9-9bd28f632b86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     365.000000\n",
              "mean      634.421918\n",
              "std        99.876531\n",
              "min       328.000000\n",
              "25%       562.000000\n",
              "50%       634.000000\n",
              "75%       706.000000\n",
              "max      1065.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(2018)\n",
        "collisions2018temp = df[df['year'] == 2018]\n",
        "dayofyear = range(1, len(collisions2018temp)+1)\n",
        "dayindex = range(1, len(collisions2018temp)+1)\n",
        "collisions2018 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2018temp.reset_index(drop=True)], axis=1)\n",
        "collisions2018['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjQcXUTRnDlg",
        "outputId": "3d6335ce-0773-4dd6-b3b9-04e9a84ba36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2019\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    365.000000\n",
              "mean     579.413699\n",
              "std       88.947085\n",
              "min      264.000000\n",
              "25%      522.000000\n",
              "50%      589.000000\n",
              "75%      641.000000\n",
              "max      803.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print(2019)\n",
        "collisions2019temp = df[df['year'] == 2019]\n",
        "dayofyear = range(1, len(collisions2019temp)+1)\n",
        "dayindex = range(1, len(collisions2019temp)+1)\n",
        "collisions2019 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2019temp.reset_index(drop=True)], axis=1)\n",
        "collisions2019['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lap5mnP8nHWc",
        "outputId": "8b3f667a-77e3-4e80-888d-c48ca9b30d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    366.000000\n",
              "mean     308.510929\n",
              "std      110.188996\n",
              "min       94.000000\n",
              "25%      242.250000\n",
              "50%      301.000000\n",
              "75%      348.000000\n",
              "max      774.000000\n",
              "Name: NUM_COLLISIONS, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print(2020)\n",
        "collisions2020temp = df[df['year'] == 2020]\n",
        "dayofyear = range(1, len(collisions2020temp)+1)\n",
        "dayindex = range(1, len(collisions2020temp)+1)\n",
        "collisions2020 = pd.concat([pd.DataFrame(dayindex), pd.DataFrame(dayofyear), collisions2020temp.reset_index(drop=True)], axis=1)\n",
        "collisions2020['NUM_COLLISIONS'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbTayFb1ndhP",
        "outputId": "ac84079e-7b95-41aa-c4e7-ce95c45f1f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0  0  year  mo  da collision_date  temp  dewp  visib  wdsp  mxpsd   gust  \\\n",
            "0  1  1  2012   7  11     2012-07-11  77.1  62.0   10.0   1.9    7.0  999.9   \n",
            "1  2  2  2012   8   9     2012-08-09  78.2  69.6    9.3   2.3    7.0   15.0   \n",
            "2  3  3  2012   7  12     2012-07-12  77.4  60.2   10.0   2.3    7.0  999.9   \n",
            "3  4  4  2012   9  17     2012-09-17  66.5  48.0   10.0   2.2    7.0  999.9   \n",
            "4  5  5  2012   8  24     2012-08-24  77.3  60.9    9.3   2.0    7.0  999.9   \n",
            "\n",
            "    max   min  prcp   sndp  fog  NUM_COLLISIONS  \n",
            "0  84.0  64.9   0.0  999.9    0             565  \n",
            "1  88.0  61.0   0.0  999.9    0             581  \n",
            "2  88.0  57.0   0.0  999.9    0             577  \n",
            "3  77.0  48.2   0.0  999.9    0             452  \n",
            "4  87.1  66.0   0.0  999.9    0             601  \n"
          ]
        }
      ],
      "source": [
        "# Add years together into one dataset\n",
        "collisions_2012_to_2020 = pd.concat([collisions2012, collisions2013, collisions2014, collisions2015, collisions2016, collisions2017, collisions2018, collisions2019, collisions2020], axis=0)\n",
        "print(collisions_2012_to_2020.head())\n",
        "\n",
        "df = collisions_2012_to_2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysXEof1WXkXy"
      },
      "outputs": [],
      "source": [
        "# Imports for creating validation sets and linear regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJj7ysfYsUup"
      },
      "source": [
        "## Validation Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUP6VhgTnFQB"
      },
      "source": [
        "Before training a linear regression model, some validations sets will need to be created to find out which sets will be used for training. The variables found through the above analysis will be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCnAlOQYbFHj"
      },
      "source": [
        "MAX/MIN/DA/FOG vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8owrWcvZbL7"
      },
      "outputs": [],
      "source": [
        "X = df[['max', 'min', 'da', 'fog']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3M5gY16amql"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Fa7k0lahPQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-MefQ0eavf0"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqavH77vaz5q"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzgfou2sa5Ks",
        "outputId": "bffcb918-7759-4630-e77f-a8ba78f26d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 514.0362292517908\n",
            "Coefficients: [ 1.21111082 -0.33105002 -0.70367072 -6.79773952]\n"
          ]
        }
      ],
      "source": [
        "model_max_min_da_fog = LinearRegression()\n",
        "model_max_min_da_fog.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_max_min_da_fog.intercept_)\n",
        "print(\"Coefficients:\", model_max_min_da_fog.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2T1dyUma6EG"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_max_min_da_fog.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYiTyJjwa7Ih",
        "outputId": "e5ccd2fd-9651-4497-b504-a5005c5d2d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0031\n"
          ]
        }
      ],
      "source": [
        "score = model_max_min_da_fog.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT1v29gQkoLf"
      },
      "source": [
        "A mixture of all four variables provide a relatively high R2 score of 0.0033, hinting that some of the variables play a role in the number of collisions which occur. To find out which variables impact this number, further validation sets will be created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJHC_S0PbI8v"
      },
      "source": [
        "MAX/MIN/DA vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09cOgdSJbBTS"
      },
      "outputs": [],
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggRnWV5sbBTS"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScCS1nXpbBTS"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhoiUmV_bBTS"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8mfsEpSbBTT"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdnF2TG4bBTT",
        "outputId": "938e822f-c674-4f30-e815-60eb6f58c28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 514.0081308425891\n",
            "Coefficients: [ 1.23034254 -0.39931252 -0.69155655]\n"
          ]
        }
      ],
      "source": [
        "model_max_min_da = LinearRegression()\n",
        "model_max_min_da.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_max_min_da.intercept_)\n",
        "print(\"Coefficients:\", model_max_min_da.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GngqKjTibBTT"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_max_min_da.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_DK5A1VbBTT",
        "outputId": "483cdc35-4a16-4482-8619-dc3f619f9d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0001\n"
          ]
        }
      ],
      "source": [
        "score = model_max_min_da.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGJMtAZ_k4B-"
      },
      "source": [
        "Removing the 'fog' variable pushes the correlation higher by 0.0004, proving that the max and min temperature play a role in the number of collisions. As stated earlier though, the day does not seem to affect the number of collisions which occur, so removing this may increase the R2 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIeB7McgbMVG"
      },
      "source": [
        "MAX/MIN vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYgWjinhbPHW"
      },
      "outputs": [],
      "source": [
        "X = df[['max', 'min']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEClFlSDbPHW"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7nfoifmbPHW"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsHbVbjEbPHW"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Was5rUOLbPHW"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NldZIilrbPHW",
        "outputId": "3bd77d79-61c1-471a-d54a-cc7629fb7dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 502.38911357759855\n",
            "Coefficients: [ 1.26465772 -0.4311661 ]\n"
          ]
        }
      ],
      "source": [
        "model_max_min = LinearRegression()\n",
        "model_max_min.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_max_min.intercept_)\n",
        "print(\"Coefficients:\", model_max_min.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh_AbnBObPHX"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_max_min.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-IkPEjsbPHX",
        "outputId": "9ac574e1-4fbc-478c-b6f2-712739eceea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0008\n"
          ]
        }
      ],
      "source": [
        "score = model_max_min.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906scXKElNzC"
      },
      "source": [
        "The R2 score plummeted to 0.0009 after removing the day variable. This could be due to instances of crashes occuring at high temperatures compared to lower temperatures. To get a more accurate R2 score, validation sets of both max and min temperatures will be created independent of each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VwCu5a0bUcB"
      },
      "source": [
        "MAX vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmYw2anZbWfL"
      },
      "outputs": [],
      "source": [
        "X = df[['max']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOtLCd6IbWfL"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEg84HNtbWfL"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAe_wfNtbWfL"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFebSv7dbWfL"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "iZtFHpY0bWfL",
        "outputId": "8bd74889-9df5-4328-f226-0aa3c2bc9eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 507.9329856859529\n",
            "Coefficients: [0.85426852]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_max = LinearRegression()\n",
        "model_max.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_max.intercept_)\n",
        "print(\"Coefficients:\", model_max.coef_)\n",
        "\n",
        "plt.scatter(X_test['max'], Y_test, color=\"blue\")\n",
        "plt.plot(X_test['max'], model_max.predict(X_test), color=\"red\")\n",
        "plt.xlabel(\"Max temp (f)\")\n",
        "plt.ylabel(\"Number of Collisions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fvqJfWkl7cE"
      },
      "source": [
        "The above graph continues to show the increasing trend as found within the 'Analysis' section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1fyIEGGbWfM"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_max.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFLCe7jVbWfM",
        "outputId": "3cf7b191-bf3b-4004-d10a-cc4669c6c7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0020\n"
          ]
        }
      ],
      "source": [
        "score = model_max.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xabN3RYulg5D"
      },
      "source": [
        "Removing the 'min' variable has improved the R2 score by doubling it, further proving that the temperature on the day impacts the number of collisions occuring. Next, the opposite will be tested to see whether a minimum temperature will affect the number of collisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz-rT4pVbZSE"
      },
      "source": [
        "MIN vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0uJGNohba-1"
      },
      "outputs": [],
      "source": [
        "X = df[['min']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li25E7d3ba-1"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn8vfbjtba-1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWjHv95gba-2"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFBKB7Diba-2"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "qZxMr6u4ba-2",
        "outputId": "52aef4a4-7546-4cb6-c4d2-c8192a1291cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 527.6893719989168\n",
            "Coefficients: [0.71323053]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_min = LinearRegression()\n",
        "model_min.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_min.intercept_)\n",
        "print(\"Coefficients:\", model_min.coef_)\n",
        "\n",
        "plt.scatter(X_test['min'], Y_test, color=\"blue\")\n",
        "plt.plot(X_test['min'], model_min.predict(X_test), color=\"red\")\n",
        "plt.xlabel(\"Min temp (f)\")\n",
        "plt.ylabel(\"Number of Collisions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EewVXZiYmFgZ"
      },
      "source": [
        "As with the graph for the max temperature, the upward trend continues in this graph. This makes sense due to the graph with the higher temperatures already continuing the trend established here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy4hYj91ba-2"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_min.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0wiiWkSba-2",
        "outputId": "8f8a6d2c-1b1e-420d-e654-c981c0ca209d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0046\n"
          ]
        }
      ],
      "source": [
        "score = model_min.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1xRet2EmZh5"
      },
      "source": [
        "While not as high as the max temperature, the R2 score for the min temperature is showing a positive correlation with the number of collisions occuring. Since fog occurs at around 4 degrees, the fog variable only has an effect at lower temperatures. So, the two variables will be tested as a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ6RqDPKbf74"
      },
      "source": [
        "MIN/FOG vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zks30_2-bhuN"
      },
      "outputs": [],
      "source": [
        "X = df[['min','fog']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UdEzXQwbhuN"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HvBvJUBbhuN"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_75WBIbhuN"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7Kih2dYbhuN"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D9_zCXLbhuO",
        "outputId": "c0497ea0-98a4-46b8-b3ff-2249255db833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 527.1417596958632\n",
            "Coefficients: [ 0.76402608 -6.79105177]\n"
          ]
        }
      ],
      "source": [
        "model_min_fog = LinearRegression()\n",
        "model_min_fog.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_min_fog.intercept_)\n",
        "print(\"Coefficients:\", model_min_fog.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMwxf6MLbhuO"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_min_fog.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSy2RizObhuO",
        "outputId": "f4ac2c4c-98e2-4af9-ee86-252a186a1108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0074\n"
          ]
        }
      ],
      "source": [
        "score = model_min_fog.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHPJZR61mvXy"
      },
      "source": [
        "A validation set of both min temperature and fog depth lower the score of the independent min temp by almost half. Due to the rarity of fog occuring and the circumstances needed for it to form, it is not surprising to find that the R2 score (and therefore the correlaton) to drop slightly.\n",
        "\n",
        "Although stated earlier, that the day has no correlation between the number of crashes, predictions may be made that the day could play a role in collision numbers. This will be paired with max temperatures to find a correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpe4IYHEb0K7"
      },
      "source": [
        "MAX/DA vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB_9rn5lb8Ob"
      },
      "outputs": [],
      "source": [
        "X = df[['max','da']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIurImcqb8Oc"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m73Cv-Lqb8Oc"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHD3q1wMb8Oc"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "467kF2DLb8Oc"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deE4v0l9b8Oc",
        "outputId": "d26dda65-6725-421e-cdab-18547435c6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 519.2561165241457\n",
            "Coefficients: [ 0.85023144 -0.69857008]\n"
          ]
        }
      ],
      "source": [
        "model_max_da = LinearRegression()\n",
        "model_max_da.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_max_da.intercept_)\n",
        "print(\"Coefficients:\", model_max_da.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmeiKMzdb8Oc"
      },
      "outputs": [],
      "source": [
        "Y_pred = model_max_da.predict(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFeQ1Tqrb8Oc",
        "outputId": "f95ac01f-c830-4618-bc27-b9a3f901ad86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0015\n"
          ]
        }
      ],
      "source": [
        "score = model_max_da.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WKdwdi-nrdW"
      },
      "source": [
        "The R2 score jumped up quite signficantly when both variables were put together. The max temp on its own only score 0.0020, with it now increasing by around 250% to 0.0050. Perhaps the day of the week does play a role in the number of collisions with higher tempratures. This could be attributed to better weather making people want to travel more, therefore increasing the number of cars on the road an potientially increasing the number of collisions.\n",
        "\n",
        "Although the same cannot be said for lower temperatures, perhaps min temperatures and the day can have a higher crash rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyvv5Elxb3n1"
      },
      "source": [
        "MIN/DA vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP4mlLvRb_DX"
      },
      "outputs": [],
      "source": [
        "X = df[['min', 'da']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecpuvl8jb_DX"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKhTzb_7b_DX"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb-Kse0cb_DX"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wVs45Hzb_DX"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IOCKZGDb_DY",
        "outputId": "ac631fc0-d39e-40bf-b7b2-2adf6249a8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 538.971746526589\n",
            "Coefficients: [ 0.71405775 -0.71368718]\n"
          ]
        }
      ],
      "source": [
        "model_min_da = LinearRegression()\n",
        "model_min_da.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_min_da.intercept_)\n",
        "print(\"Coefficients:\", model_min_da.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFrXm6WAb_DY"
      },
      "outputs": [],
      "source": [
        "Y_pred = model_min_da.predict(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKzl7M3tb_DY",
        "outputId": "df3fcf3b-49d4-4dce-8c31-d9c562040351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0047\n"
          ]
        }
      ],
      "source": [
        "# Predict y-value with x-values\n",
        "score = model_min_da.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8A96LDtoggI"
      },
      "source": [
        "With only 0.0005 of a difference compared to max temp and the day, the day variable continues to play a role in the number of collisions when paired with min temp. Perhaps the colder weather leads to more people driving to escape the cold, therefore putting more cars on the road. \n",
        "\n",
        "Finally, fog depth has shown to be somewhat of a cause for collisions, so a validation set will be created for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjDF4vWjizVP"
      },
      "source": [
        "FOG vs. NUM_COLLISIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsEHS2pji_3F"
      },
      "outputs": [],
      "source": [
        "X = df[['fog']]\n",
        "Y = df['NUM_COLLISIONS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD6GVtkzi_3F"
      },
      "source": [
        "Split the data into training and testing sets using the 'train_test_split()' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWTRnL7Ai_3F"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDPwq3vMi_3G"
      },
      "source": [
        "Split the data again, creating a validation set of 20% of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rlSwj0ai_3G"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "3p7S2kmwi_3G",
        "outputId": "5c744600-2e7c-4c31-eb9f-0765aa2fc337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 559.5419695968275\n",
            "Coefficients: [-0.2138446]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_fog = LinearRegression()\n",
        "model_fog.fit(X_train, Y_train)\n",
        "print(\"Intercept:\", model_fog.intercept_)\n",
        "print(\"Coefficients:\", model_fog.coef_)\n",
        "\n",
        "plt.scatter(X_test['fog'], Y_test, color=\"blue\")\n",
        "plt.plot(X_test['fog'], model_fog.predict(X_test), color=\"red\")\n",
        "plt.xlabel(\"Fog Depth\")\n",
        "plt.ylabel(\"Number of Collisions\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeLZX_k1pDW2"
      },
      "source": [
        "There is a very small positive correlation (as seen earlier) however as fog rarely forms the prediction is bound to be lower than a more consistent variable like the temperature or the day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CraGdtm4i_3G"
      },
      "outputs": [],
      "source": [
        "# Predict y-value with x-values\n",
        "Y_pred = model_fog.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MZoshx0i_3G",
        "outputId": "0a776194-2ad1-44e3-9365-aba911e54fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score: -0.0052\n"
          ]
        }
      ],
      "source": [
        "score = model_fog.score(X_val, Y_val)\n",
        "print(f\"R^2 score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Np_ox_dpV5a"
      },
      "source": [
        "As predicted from the graph, there is a negative prediciton rate with the fog depth as a cause of collision. While collisions do occur when visibility is reduced, it also requires a lot of ingredients for fog to form consistently, therefore proving to be a lesser cause of collisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXf9djthcDVL"
      },
      "source": [
        "VALIDATION SET SCORES:\n",
        "* MAX/MIN/DA/FOG vs.NUM_COLLISIONS: 0.0033\n",
        "* MAX/MIN/DA vs.-------- NUM_COLLISIONS: 0.0039\n",
        "* MAX/MIN vs.---------------NUM_COLLISIONS: 0.0009\n",
        "* MAX vs.-----------------------NUM_COLLISIONS: 0.0020\n",
        "* MIN vs.------------------------NUM_COLLISIONS: 0.0015\n",
        "* MIN/FOG vs.----------------NUM_COLLISIONS: 0.0009\n",
        "* MAX/DA vs.-----------------NUM_COLLISIONS: 0.0050\n",
        "* MIN/DA vs.------------------NUM_COLLISIONS: 0.0045\n",
        "* FOG vs.------------------------NUM_COLLISIONS: -0.0029\n",
        "\n",
        "From the validation set scores, fog has a negative correlation between itself and the number of collisions. The strongest correlations are a mixture of max/min temperatures and the day of the week for the number of recorded collisions. Max temperature and the day come to a R2 score of 0.0050, min temperature and the day come to 0.0045 with both max and min along with the day score 0.0039. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKkSzluZsZbn"
      },
      "source": [
        "# The Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, in order to determine if the year plays a role, there will be three sets of models created for each year. The three sets of models are; 'Max and Day vs. NUM_COLLISIONS', 'Min and Day vs. NUM_COLLISIONS', and 'Max / Min and Day vs. NUM_COLLISIONS'. These same sets will be used when creating the models with Tensorflow. If splitting the dataset into years does not yield any results, then it can be assumed that the year holds no weight on the increase or decreased amount of collision which occur. Therefore, there is no reason to split the dataset into years for the Tensorflow model."
      ],
      "metadata": {
        "id": "dYY4Vh-13fbM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEkyVgoyp8IF"
      },
      "outputs": [],
      "source": [
        "SCALE_NUM_COLLISIONS = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29T3bCKsoWd5",
        "outputId": "4c104f9f-2840-411a-f402-e90a62462bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import keras & layers\n",
        "from tensorflow import keras  \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Print current version of tensorflow\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2012"
      ],
      "metadata": {
        "id": "UghKC0MVztfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "aBjI0wSM638g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2012\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2012 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2012)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUe986-zGm_P",
        "outputId": "42a7e276-6f22-4fca-b544-1a498e6dd9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 5s 536ms/step - loss: 548.6144 - val_loss: 561.0357\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 94ms/step - loss: 548.3831 - val_loss: 560.7908\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 179ms/step - loss: 548.1443 - val_loss: 560.5237\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 1s 215ms/step - loss: 547.8774 - val_loss: 560.2287\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 123ms/step - loss: 547.5808 - val_loss: 559.8882\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 547.2352 - val_loss: 559.4858\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 185ms/step - loss: 546.8342 - val_loss: 559.0103\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 184ms/step - loss: 546.3514 - val_loss: 558.4531\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 74ms/step - loss: 545.7945 - val_loss: 557.7948\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 545.1167 - val_loss: 557.0193\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 117ms/step - loss: 544.3193 - val_loss: 556.0941\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 543.3840 - val_loss: 554.9886\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 542.2570 - val_loss: 553.6649\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 82ms/step - loss: 540.8860 - val_loss: 552.0925\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 108ms/step - loss: 539.2896 - val_loss: 550.2223\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 105ms/step - loss: 537.4150 - val_loss: 548.0130\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 80ms/step - loss: 535.1993 - val_loss: 545.4080\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 532.5573 - val_loss: 542.3510\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 529.4303 - val_loss: 538.7866\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 525.9024 - val_loss: 534.6292\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 521.6891 - val_loss: 529.8124\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 516.8226 - val_loss: 524.2604\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 511.2791 - val_loss: 517.8585\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 504.7833 - val_loss: 510.5413\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 497.5346 - val_loss: 502.1944\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 489.1581 - val_loss: 492.7063\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 479.7540 - val_loss: 481.9563\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 468.8341 - val_loss: 469.8381\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 456.6695 - val_loss: 456.1766\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 443.1177 - val_loss: 440.8387\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 84ms/step - loss: 427.7791 - val_loss: 423.6957\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 410.6831 - val_loss: 404.5701\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 391.6958 - val_loss: 383.3025\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 370.5876 - val_loss: 359.7610\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 347.2221 - val_loss: 333.7827\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 321.5240 - val_loss: 305.2718\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 293.1234 - val_loss: 273.8941\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 262.5777 - val_loss: 239.8902\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 60ms/step - loss: 234.8795 - val_loss: 206.0361\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 205.8808 - val_loss: 177.6843\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 179.9719 - val_loss: 153.4605\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 159.6420 - val_loss: 136.1618\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 142.0803 - val_loss: 127.2093\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 85ms/step - loss: 132.9780 - val_loss: 123.5411\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 128.6101 - val_loss: 124.6977\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 86ms/step - loss: 128.8589 - val_loss: 124.2513\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 128.3838 - val_loss: 123.0495\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 126.4669 - val_loss: 120.8703\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 123.3973 - val_loss: 118.1708\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 120.4995 - val_loss: 114.7800\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 117.4535 - val_loss: 111.2928\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 113.6689 - val_loss: 109.1070\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 69ms/step - loss: 111.2302 - val_loss: 107.2682\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 109.9188 - val_loss: 105.8396\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 108.5613 - val_loss: 103.9041\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 106.6274 - val_loss: 101.0681\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 104.4258 - val_loss: 99.2347\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 103.2569 - val_loss: 97.6726\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 102.0996 - val_loss: 96.2590\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 100.9630 - val_loss: 95.1744\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 99.5393 - val_loss: 93.9887\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 98.4768 - val_loss: 93.0489\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 97.2693 - val_loss: 92.1802\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 96.3851 - val_loss: 91.1649\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 95.5964 - val_loss: 90.3108\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 94.6858 - val_loss: 89.3665\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 94.1565 - val_loss: 88.3588\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 93.1824 - val_loss: 87.4233\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 92.5121 - val_loss: 86.2824\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 91.9406 - val_loss: 85.2592\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 91.3376 - val_loss: 84.4296\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 90.7336 - val_loss: 83.7498\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 90.2175 - val_loss: 83.3255\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 89.8352 - val_loss: 82.9560\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 89.2432 - val_loss: 82.1794\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 88.7329 - val_loss: 81.5341\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 42ms/step - loss: 88.1950 - val_loss: 80.9657\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 88.1227 - val_loss: 80.2202\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 87.2709 - val_loss: 79.8946\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 86.7068 - val_loss: 79.5477\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 86.3128 - val_loss: 79.1604\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 85.9040 - val_loss: 78.8642\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 85.5188 - val_loss: 78.7758\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 84.8538 - val_loss: 78.3448\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 84.3865 - val_loss: 77.9925\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 83.9256 - val_loss: 77.6548\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 83.4794 - val_loss: 77.2999\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 83.0898 - val_loss: 76.9573\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 82.5196 - val_loss: 76.6646\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 82.1016 - val_loss: 76.3270\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 81.8498 - val_loss: 75.8812\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 81.2790 - val_loss: 75.6163\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 66ms/step - loss: 80.8067 - val_loss: 75.2899\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 80.5682 - val_loss: 74.9237\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 81ms/step - loss: 80.0862 - val_loss: 74.6387\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 79.7350 - val_loss: 74.4919\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 79.4381 - val_loss: 74.1888\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 79.0314 - val_loss: 74.0767\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 78.7019 - val_loss: 74.0048\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 78.2847 - val_loss: 73.6772\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 78.2987 - val_loss: 73.7250\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 77.7641 - val_loss: 73.5500\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 77.5056 - val_loss: 73.4152\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 77.4199 - val_loss: 73.6988\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 76.9374 - val_loss: 73.3980\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 76.6536 - val_loss: 73.2774\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 76.4017 - val_loss: 73.1852\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 76.1937 - val_loss: 73.0770\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 62ms/step - loss: 76.0304 - val_loss: 72.7309\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 75.7016 - val_loss: 72.6116\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 75.5382 - val_loss: 72.5935\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 75.2528 - val_loss: 72.4562\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 75.1979 - val_loss: 72.2238\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 74.9498 - val_loss: 72.4286\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 71ms/step - loss: 74.7891 - val_loss: 72.0895\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 87ms/step - loss: 74.4149 - val_loss: 71.9729\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 74.2140 - val_loss: 71.8786\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 74.0013 - val_loss: 71.6891\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 114ms/step - loss: 73.8204 - val_loss: 71.5085\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 127ms/step - loss: 73.7105 - val_loss: 71.3663\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 96ms/step - loss: 73.4291 - val_loss: 71.2807\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 73.2558 - val_loss: 71.1016\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 73.0718 - val_loss: 71.0157\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 72.9617 - val_loss: 70.7862\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 72.7063 - val_loss: 70.7268\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 72.5926 - val_loss: 70.6962\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 72.4875 - val_loss: 70.8071\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 72.3695 - val_loss: 70.5575\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 72.1513 - val_loss: 70.8236\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 71.9140 - val_loss: 71.1558\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 71.8944 - val_loss: 71.3824\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 71.7501 - val_loss: 71.6779\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 71.7206 - val_loss: 72.0987\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 71.6423 - val_loss: 71.5325\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 71.4407 - val_loss: 71.3231\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 71.5850 - val_loss: 72.0982\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 71.3079 - val_loss: 71.5773\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 71.1993 - val_loss: 71.8055\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 72.4545\n",
            "72.45454406738281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "j-hsFl117MJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2012 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2012)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947f5fff-494f-47fd-c466-cec98fb2a2db",
        "id": "zsm-HNCVuPUl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 2s 147ms/step - loss: 548.8239 - val_loss: 561.2870\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 548.6512 - val_loss: 561.1346\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 548.4947 - val_loss: 560.9825\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 548.3365 - val_loss: 560.8187\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 548.1605 - val_loss: 560.6302\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 547.9618 - val_loss: 560.4146\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 547.7315 - val_loss: 560.1678\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 547.4644 - val_loss: 559.8820\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 547.1552 - val_loss: 559.5435\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 546.7842 - val_loss: 559.1412\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 546.3472 - val_loss: 558.6630\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 545.8251 - val_loss: 558.0905\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 545.2012 - val_loss: 557.3992\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 544.4319 - val_loss: 556.5718\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 543.5470 - val_loss: 555.5762\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 542.4601 - val_loss: 554.3887\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 541.1769 - val_loss: 552.9732\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 539.6611 - val_loss: 551.2950\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 537.8693 - val_loss: 549.3181\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 535.7510 - val_loss: 547.0084\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 533.2702 - val_loss: 544.3079\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 530.3691 - val_loss: 541.1716\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 527.0291 - val_loss: 537.5365\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 523.1806 - val_loss: 533.3419\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 518.7627 - val_loss: 528.5306\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 513.6379 - val_loss: 523.0435\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 507.8240 - val_loss: 516.7874\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 501.2446 - val_loss: 509.6766\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 493.7834 - val_loss: 501.6282\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 485.2779 - val_loss: 492.5452\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 475.8370 - val_loss: 482.3102\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 465.1657 - val_loss: 470.8280\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 452.9853 - val_loss: 458.0621\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 439.6883 - val_loss: 443.8231\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 424.9167 - val_loss: 427.9771\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 408.3104 - val_loss: 410.4186\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 390.2789 - val_loss: 390.9910\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 370.0855 - val_loss: 369.5930\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 347.8760 - val_loss: 346.0298\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 323.3166 - val_loss: 320.1970\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 296.9875 - val_loss: 291.8576\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 268.2694 - val_loss: 261.0449\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 238.9565 - val_loss: 228.1006\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 209.9032 - val_loss: 197.6786\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 180.8528 - val_loss: 171.0412\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 153.7249 - val_loss: 148.6913\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 134.3753 - val_loss: 134.2509\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 125.8128 - val_loss: 128.0024\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 123.1947 - val_loss: 128.1122\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 122.5319 - val_loss: 127.7680\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 121.8160 - val_loss: 126.1181\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 120.3043 - val_loss: 123.7936\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 118.0822 - val_loss: 120.8443\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 115.2497 - val_loss: 117.9263\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 111.9351 - val_loss: 114.9917\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 41ms/step - loss: 109.6533 - val_loss: 112.3727\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 108.1200 - val_loss: 110.8115\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 107.1977 - val_loss: 109.7552\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 106.1251 - val_loss: 109.1898\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 104.7755 - val_loss: 108.0938\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 103.4541 - val_loss: 106.9670\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 102.3892 - val_loss: 105.7058\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 101.1449 - val_loss: 104.5727\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 99.8211 - val_loss: 103.4077\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 98.9785 - val_loss: 102.3734\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 98.0897 - val_loss: 101.3857\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 97.2136 - val_loss: 100.3282\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 96.2361 - val_loss: 99.2774\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 95.3864 - val_loss: 98.2690\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 94.6389 - val_loss: 97.1450\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 93.7866 - val_loss: 96.3058\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 92.7822 - val_loss: 95.5846\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 92.0580 - val_loss: 94.9604\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 91.3533 - val_loss: 94.3939\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 90.5719 - val_loss: 93.8418\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 90.0518 - val_loss: 93.3194\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 89.3745 - val_loss: 93.0208\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 88.6563 - val_loss: 92.6602\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 67ms/step - loss: 88.0442 - val_loss: 92.2882\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 87.5283 - val_loss: 91.9611\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 86.8953 - val_loss: 91.3342\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 86.2653 - val_loss: 90.7618\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 56ms/step - loss: 85.8441 - val_loss: 90.3535\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 85.2995 - val_loss: 90.1399\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 84.8162 - val_loss: 89.4367\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 84.4894 - val_loss: 88.8740\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 83.9103 - val_loss: 88.4731\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 83.4325 - val_loss: 88.2468\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 83.2186 - val_loss: 87.9252\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 51ms/step - loss: 82.7864 - val_loss: 87.0483\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 82.2594 - val_loss: 86.3918\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 81.9638 - val_loss: 86.0149\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 58ms/step - loss: 81.5735 - val_loss: 85.5515\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 54ms/step - loss: 81.3885 - val_loss: 85.5027\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 52ms/step - loss: 81.2664 - val_loss: 84.8377\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 48ms/step - loss: 80.6818 - val_loss: 84.2742\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 49ms/step - loss: 80.4496 - val_loss: 83.8695\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 80.0666 - val_loss: 83.4839\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 79.8105 - val_loss: 83.0180\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 53ms/step - loss: 79.6684 - val_loss: 82.4536\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 79.3152 - val_loss: 82.2074\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 79.1167 - val_loss: 82.1705\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 78.8476 - val_loss: 82.1500\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 78.6944 - val_loss: 81.6819\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 78.3598 - val_loss: 80.9710\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 40ms/step - loss: 78.0948 - val_loss: 80.6383\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 77.9075 - val_loss: 80.4770\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 77.7743 - val_loss: 80.6325\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 77.4153 - val_loss: 80.2828\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 77.1918 - val_loss: 79.7260\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 76.8927 - val_loss: 79.6441\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 76.6625 - val_loss: 79.3057\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 76.4165 - val_loss: 79.1355\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 76.3494 - val_loss: 78.7704\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 76.1333 - val_loss: 78.2386\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 75.8849 - val_loss: 77.9345\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 75.7043 - val_loss: 77.7736\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 75.5061 - val_loss: 77.3838\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 75.2548 - val_loss: 77.1482\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 75.0614 - val_loss: 76.9449\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 74.9084 - val_loss: 76.9248\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 74.7059 - val_loss: 76.9241\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 74.5104 - val_loss: 76.7621\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 74.4480 - val_loss: 76.3972\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 74.1904 - val_loss: 76.2209\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 74.0752 - val_loss: 75.9637\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 73.9020 - val_loss: 75.7929\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 73.5881 - val_loss: 75.8482\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 73.4616 - val_loss: 75.5834\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 73.2219 - val_loss: 75.4343\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 73.1649 - val_loss: 75.2066\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 73.1061 - val_loss: 75.1534\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 72.8698 - val_loss: 74.8678\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 72.6458 - val_loss: 74.9582\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 72.7397 - val_loss: 74.8757\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 72.4666 - val_loss: 74.5882\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 72.5502 - val_loss: 74.2512\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 72.2613 - val_loss: 74.4268\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 72.2210 - val_loss: 74.5771\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 72.1103 - val_loss: 74.4293\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 72.0801 - val_loss: 74.2730\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 71.8992 - val_loss: 73.9208\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 71.8014 - val_loss: 73.7483\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 71.8226 - val_loss: 73.9327\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 71.5356 - val_loss: 73.8184\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 71.4290 - val_loss: 73.7906\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 71.3166 - val_loss: 73.8288\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 71.2427 - val_loss: 73.6606\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 71.1853 - val_loss: 73.5355\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 71.0730 - val_loss: 73.1866\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 71.0628 - val_loss: 73.0351\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 70.9061 - val_loss: 72.9885\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 70.8239 - val_loss: 72.9368\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 70.6600 - val_loss: 73.0834\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 70.5710 - val_loss: 73.3012\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 70.4788 - val_loss: 73.3704\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 70.3827 - val_loss: 73.1815\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 70.2934 - val_loss: 72.9728\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 70.2938 - val_loss: 72.7168\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 70.1515 - val_loss: 72.9966\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 70.1321 - val_loss: 73.3721\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 70.0574 - val_loss: 73.0682\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 70.0322 - val_loss: 73.4798\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 69.7921 - val_loss: 73.4062\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 69.6623 - val_loss: 73.1311\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 69.7139 - val_loss: 72.6637\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 69.7170 - val_loss: 72.5293\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 69.5872 - val_loss: 73.2015\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 69.3753 - val_loss: 73.4807\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 69.2252 - val_loss: 73.9288\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 69.4353 - val_loss: 74.3111\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 69.2979 - val_loss: 73.5788\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 69.1637 - val_loss: 73.1960\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 69.0624 - val_loss: 73.1560\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 69.0298 - val_loss: 73.2669\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 68.9031 - val_loss: 73.4701\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 68.8685 - val_loss: 73.9151\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 72.1453\n",
            "72.14530944824219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "Spq1bktW7Q2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2012 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2012)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a00da4b-24d4-4f45-a3d2-be9221cb3019",
        "id": "gINXktDmvvO4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 1s 94ms/step - loss: 548.8517 - val_loss: 561.2518\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 548.6245 - val_loss: 561.0248\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 548.4005 - val_loss: 560.7947\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 548.1712 - val_loss: 560.5499\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 547.9318 - val_loss: 560.2786\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 547.6562 - val_loss: 559.9666\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 547.3468 - val_loss: 559.6008\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 546.9702 - val_loss: 559.1569\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 546.5070 - val_loss: 558.6122\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 545.9481 - val_loss: 557.9438\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 545.2618 - val_loss: 557.1309\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 544.4100 - val_loss: 556.1350\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 543.3734 - val_loss: 554.9186\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 542.1376 - val_loss: 553.4365\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 540.6094 - val_loss: 551.6671\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 538.8169 - val_loss: 549.5439\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 536.6185 - val_loss: 547.0153\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 534.0393 - val_loss: 544.0062\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 530.9894 - val_loss: 540.4532\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 527.3680 - val_loss: 536.2840\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 523.1557 - val_loss: 531.4019\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 518.2195 - val_loss: 525.7458\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 512.4941 - val_loss: 519.2438\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 505.9778 - val_loss: 511.7565\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 498.3011 - val_loss: 503.1997\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 489.7881 - val_loss: 493.3586\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 479.6951 - val_loss: 482.1307\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 468.4145 - val_loss: 469.3499\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 455.6649 - val_loss: 454.8601\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 441.1931 - val_loss: 438.5056\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 424.8854 - val_loss: 420.1264\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 406.2630 - val_loss: 399.5652\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 385.7735 - val_loss: 376.6204\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 362.5146 - val_loss: 351.1943\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 337.9609 - val_loss: 322.8473\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 309.3281 - val_loss: 291.6347\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 278.4732 - val_loss: 259.9493\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 245.8533 - val_loss: 227.9892\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 215.7477 - val_loss: 197.1002\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 191.3949 - val_loss: 167.4809\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 173.5828 - val_loss: 148.3481\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 159.0693 - val_loss: 136.6048\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 150.3455 - val_loss: 136.7551\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 148.2486 - val_loss: 138.9601\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 147.2201 - val_loss: 141.6753\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 145.2325 - val_loss: 141.9574\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 142.3632 - val_loss: 140.4380\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 139.0498 - val_loss: 137.8519\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 136.0878 - val_loss: 133.8616\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 132.3785 - val_loss: 129.8794\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 129.8098 - val_loss: 127.2935\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 128.0857 - val_loss: 124.9005\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 126.4641 - val_loss: 122.9809\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 124.7302 - val_loss: 121.8841\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 123.2993 - val_loss: 121.0329\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 121.5681 - val_loss: 119.6706\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 120.4086 - val_loss: 118.2652\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 118.8230 - val_loss: 116.2744\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 117.6335 - val_loss: 113.9009\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 116.3131 - val_loss: 111.5590\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 115.7831 - val_loss: 109.2022\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 114.2211 - val_loss: 108.4436\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 113.0855 - val_loss: 107.5059\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 112.0167 - val_loss: 106.5725\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 111.2191 - val_loss: 106.4126\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 109.8329 - val_loss: 105.2693\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 108.9545 - val_loss: 104.7846\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 108.0694 - val_loss: 104.2580\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 106.9590 - val_loss: 103.3437\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 106.0193 - val_loss: 103.0409\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 105.0483 - val_loss: 102.3030\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 104.1257 - val_loss: 101.4069\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 103.2198 - val_loss: 100.5275\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 102.5221 - val_loss: 99.9397\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 101.5101 - val_loss: 98.3793\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 100.6873 - val_loss: 97.2685\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 100.1228 - val_loss: 96.7864\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 99.1436 - val_loss: 96.1037\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 98.3019 - val_loss: 95.5240\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 97.4683 - val_loss: 94.9989\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 96.6946 - val_loss: 94.4242\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 95.8107 - val_loss: 93.8834\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 94.9921 - val_loss: 93.3066\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 94.5021 - val_loss: 92.6573\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 93.6197 - val_loss: 92.1144\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 93.1019 - val_loss: 91.6416\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 92.3175 - val_loss: 91.0620\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 91.6222 - val_loss: 90.5532\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 91.0033 - val_loss: 90.1506\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 90.3526 - val_loss: 89.7645\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 89.9968 - val_loss: 89.4477\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 89.4262 - val_loss: 89.0001\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 88.6092 - val_loss: 88.5910\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 88.1737 - val_loss: 88.1681\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 87.8034 - val_loss: 87.8936\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 87.4086 - val_loss: 87.4597\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 86.9072 - val_loss: 87.0139\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 86.4081 - val_loss: 86.6222\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 86.0240 - val_loss: 86.1969\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 85.6758 - val_loss: 85.8052\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 85.3057 - val_loss: 85.6019\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 85.1094 - val_loss: 85.6225\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 84.5376 - val_loss: 85.3354\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 84.1149 - val_loss: 85.0537\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 83.7849 - val_loss: 84.6231\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 83.3716 - val_loss: 84.4130\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 83.0643 - val_loss: 84.3905\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 82.7227 - val_loss: 84.1866\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 82.3492 - val_loss: 83.9058\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 81.9461 - val_loss: 83.5125\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 81.7984 - val_loss: 83.1943\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 81.7176 - val_loss: 83.2363\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 81.3557 - val_loss: 83.0381\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.9247 - val_loss: 82.8535\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.6407 - val_loss: 82.6240\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 80.4988 - val_loss: 82.3726\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 80.2458 - val_loss: 82.3652\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 79.8585 - val_loss: 82.1029\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 79.6419 - val_loss: 81.8403\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 79.3675 - val_loss: 81.6456\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 79.2997 - val_loss: 81.5307\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 78.9534 - val_loss: 81.3083\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 78.8016 - val_loss: 80.9734\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 78.4862 - val_loss: 80.8143\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 78.1682 - val_loss: 80.6252\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 78.0832 - val_loss: 80.4602\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.8608 - val_loss: 80.4461\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.8385 - val_loss: 80.4305\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 77.4928 - val_loss: 80.1444\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 77.4333 - val_loss: 79.7814\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.0192 - val_loss: 79.8230\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 76.8015 - val_loss: 79.7946\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 76.7596 - val_loss: 79.8303\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 76.5599 - val_loss: 79.6051\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 76.1899 - val_loss: 79.3545\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 76.0624 - val_loss: 79.1614\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 75.8666 - val_loss: 79.0208\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 75.6735 - val_loss: 78.8904\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 75.5640 - val_loss: 78.8204\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 75.3348 - val_loss: 78.7905\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 75.2624 - val_loss: 78.5677\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 75.0903 - val_loss: 78.4260\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 74.9369 - val_loss: 78.0366\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 74.9408 - val_loss: 77.8551\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 74.5860 - val_loss: 77.6842\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.4999 - val_loss: 77.8302\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 74.3599 - val_loss: 77.6267\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.1916 - val_loss: 77.4618\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 73.9870 - val_loss: 77.0775\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 73.8228 - val_loss: 76.8575\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 73.7778 - val_loss: 76.6629\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 73.5758 - val_loss: 76.4431\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 73.4197 - val_loss: 76.4163\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 73.2002 - val_loss: 76.4332\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.1615 - val_loss: 76.3073\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 73.0350 - val_loss: 76.0081\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.8316 - val_loss: 75.6219\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.8565 - val_loss: 75.3147\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 72.6886 - val_loss: 75.4239\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 72.3835 - val_loss: 75.2896\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.3051 - val_loss: 74.9941\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.1785 - val_loss: 74.8872\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.0735 - val_loss: 75.0964\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 72.0951 - val_loss: 74.7165\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.0154 - val_loss: 74.3604\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.5082 - val_loss: 73.8239\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 71.7559 - val_loss: 73.3002\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 71.5595 - val_loss: 73.3599\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 71.1791 - val_loss: 73.5036\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 71.2802 - val_loss: 73.8241\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 71.1490 - val_loss: 73.6699\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 70.9273 - val_loss: 73.3397\n",
            "Epoch 173/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 71.0215 - val_loss: 73.0977\n",
            "Epoch 174/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 70.6220 - val_loss: 72.5684\n",
            "Epoch 175/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 70.7335 - val_loss: 72.3453\n",
            "Epoch 176/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 70.5189 - val_loss: 72.4343\n",
            "Epoch 177/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 70.4004 - val_loss: 72.7959\n",
            "Epoch 178/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 70.3267 - val_loss: 72.7727\n",
            "Epoch 179/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 70.1859 - val_loss: 72.4272\n",
            "Epoch 180/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 69.9952 - val_loss: 72.0970\n",
            "Epoch 181/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 69.7515 - val_loss: 71.5506\n",
            "Epoch 182/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 69.8644 - val_loss: 71.4464\n",
            "Epoch 183/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 69.6363 - val_loss: 71.6952\n",
            "Epoch 184/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 69.3919 - val_loss: 71.7049\n",
            "Epoch 185/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 69.7683 - val_loss: 71.6082\n",
            "Epoch 186/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 69.3313 - val_loss: 71.0641\n",
            "Epoch 187/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 69.1482 - val_loss: 71.0116\n",
            "Epoch 188/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 69.1291 - val_loss: 71.2248\n",
            "Epoch 189/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 69.1083 - val_loss: 71.3446\n",
            "Epoch 190/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 68.9798 - val_loss: 70.9759\n",
            "Epoch 191/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 68.8651 - val_loss: 70.6515\n",
            "Epoch 192/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 68.7531 - val_loss: 70.6687\n",
            "Epoch 193/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 68.6709 - val_loss: 70.6172\n",
            "Epoch 194/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 68.6499 - val_loss: 70.6965\n",
            "Epoch 195/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 68.4850 - val_loss: 70.4173\n",
            "Epoch 196/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 68.4708 - val_loss: 70.7858\n",
            "Epoch 197/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 68.4221 - val_loss: 70.6047\n",
            "Epoch 198/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 68.2323 - val_loss: 70.4663\n",
            "Epoch 199/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 68.1665 - val_loss: 70.4485\n",
            "Epoch 200/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 68.3708 - val_loss: 70.9906\n",
            "Epoch 201/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 68.0762 - val_loss: 70.7089\n",
            "Epoch 202/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 67.9556 - val_loss: 70.9029\n",
            "Epoch 203/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 67.9580 - val_loss: 71.1666\n",
            "Epoch 204/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 67.8866 - val_loss: 71.5792\n",
            "Epoch 205/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 67.8632 - val_loss: 71.8392\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 69.5425\n",
            "69.5425033569336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2013"
      ],
      "metadata": {
        "id": "MSG5qLJBzy8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "KL577hdTwbZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2013\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2013 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2013)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720b45bb-58b9-435e-ffeb-ccfbcf5b2a34",
        "id": "si81R0aQwbZ9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 52ms/step - loss: 555.4934 - val_loss: 563.4311\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 555.0595 - val_loss: 563.0308\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 554.5839 - val_loss: 562.5231\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 553.9586 - val_loss: 561.8330\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 553.1099 - val_loss: 560.8944\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 551.9379 - val_loss: 559.5623\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 550.2485 - val_loss: 557.6527\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 547.8428 - val_loss: 554.9294\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 544.4863 - val_loss: 551.1140\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 539.7657 - val_loss: 545.8052\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 533.3160 - val_loss: 538.4924\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 524.5120 - val_loss: 528.7081\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 512.8099 - val_loss: 515.8179\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 497.4629 - val_loss: 499.0529\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 477.8551 - val_loss: 477.6096\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 452.8913 - val_loss: 450.6423\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 421.7571 - val_loss: 417.0072\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 383.0650 - val_loss: 375.5619\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 335.4219 - val_loss: 325.1328\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 278.9754 - val_loss: 264.1964\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 214.9756 - val_loss: 194.6916\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 153.0422 - val_loss: 136.5071\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 116.1918 - val_loss: 120.3143\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 114.4040 - val_loss: 119.2516\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 116.5749 - val_loss: 117.9178\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 111.2224 - val_loss: 116.5556\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 105.9171 - val_loss: 117.0367\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 104.3982 - val_loss: 116.6088\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 103.3343 - val_loss: 114.3066\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 101.2517 - val_loss: 111.6478\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 99.6683 - val_loss: 110.1361\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 98.5645 - val_loss: 108.6001\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 96.8296 - val_loss: 106.8932\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 95.5208 - val_loss: 105.4479\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.4440 - val_loss: 103.7500\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 93.0888 - val_loss: 102.4666\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 91.7947 - val_loss: 101.2486\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 90.7679 - val_loss: 100.1469\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 89.6711 - val_loss: 98.7234\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.6172 - val_loss: 97.3848\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.6342 - val_loss: 95.8438\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 86.2536 - val_loss: 94.8834\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 85.4345 - val_loss: 93.7588\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.0458 - val_loss: 92.2881\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.4147 - val_loss: 91.4640\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.2755 - val_loss: 89.9064\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.3871 - val_loss: 88.7412\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.7415 - val_loss: 87.7193\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 79.1322 - val_loss: 87.3305\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.5038 - val_loss: 86.5897\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 77.8303 - val_loss: 85.6715\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.8766 - val_loss: 84.7201\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.3218 - val_loss: 84.6078\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.4406 - val_loss: 83.6656\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 74.5291 - val_loss: 82.3317\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 73.8460 - val_loss: 81.4782\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 73.4765 - val_loss: 80.9385\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 72.7346 - val_loss: 80.3381\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 72.1711 - val_loss: 79.9354\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 71.9356 - val_loss: 79.3458\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 71.3620 - val_loss: 78.0496\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 70.8804 - val_loss: 77.8116\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 70.0648 - val_loss: 78.0209\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 70.3363 - val_loss: 78.0120\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 69.2222 - val_loss: 75.8837\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 69.5649 - val_loss: 75.2974\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 68.9541 - val_loss: 75.9168\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 68.5305 - val_loss: 75.5001\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 68.1678 - val_loss: 74.6624\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 67.9429 - val_loss: 73.4559\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 67.9262 - val_loss: 73.3194\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 67.5199 - val_loss: 73.4606\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 67.2495 - val_loss: 73.4872\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 67.0033 - val_loss: 73.2270\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.8555 - val_loss: 72.3129\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.9067 - val_loss: 72.8839\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 66.2821 - val_loss: 72.2911\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 66.2083 - val_loss: 72.0070\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 66.1688 - val_loss: 71.6862\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 66.0196 - val_loss: 71.3131\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.7558 - val_loss: 71.8333\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.7776 - val_loss: 71.1981\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.6649 - val_loss: 70.9811\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.5350 - val_loss: 70.8200\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.3827 - val_loss: 71.2935\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 65.3784 - val_loss: 70.6604\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.2222 - val_loss: 70.7816\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 64.9479 - val_loss: 70.5491\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 65.2401 - val_loss: 70.6785\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.9665 - val_loss: 70.7282\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 65.0429 - val_loss: 69.8248\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 65.1166 - val_loss: 69.4938\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.8998 - val_loss: 70.3600\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.9778 - val_loss: 70.0873\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.8279 - val_loss: 69.9693\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 64.7515 - val_loss: 69.3470\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.6854 - val_loss: 69.6747\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 64.8082 - val_loss: 70.0386\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.4864 - val_loss: 69.4583\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.9230 - val_loss: 69.4200\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.4011 - val_loss: 69.6105\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.8544 - val_loss: 69.5898\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 65.1781 - val_loss: 69.2856\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.9114 - val_loss: 69.8941\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.5222 - val_loss: 68.8655\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4130 - val_loss: 68.1492\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.8112 - val_loss: 69.0299\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.3026 - val_loss: 68.8367\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.0724 - val_loss: 68.6312\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.2565 - val_loss: 69.6871\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.2899 - val_loss: 69.1965\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.2558 - val_loss: 69.1829\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 63.9019 - val_loss: 68.8250\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.2775 - val_loss: 68.7923\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 63.9534 - val_loss: 68.7695\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 63.9307 - val_loss: 69.1918\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 69.9958\n",
            "69.99578857421875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "mWbUtdw5wbZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2013 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2013)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PcMDmRqwbZ9",
        "outputId": "7321b1e7-d755-4ea8-ab02-989a15048621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 41ms/step - loss: 555.5489 - val_loss: 563.5369\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 555.2635 - val_loss: 563.2741\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 554.9741 - val_loss: 562.9638\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 554.6152 - val_loss: 562.5559\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 554.1381 - val_loss: 562.0039\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 553.4661 - val_loss: 561.2117\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 552.4856 - val_loss: 560.0657\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 551.0878 - val_loss: 558.4127\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 549.0691 - val_loss: 556.0285\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 546.1766 - val_loss: 552.6844\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 542.1536 - val_loss: 548.0815\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 536.7052 - val_loss: 541.8375\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 529.2846 - val_loss: 533.5024\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 519.5983 - val_loss: 522.4824\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 506.6223 - val_loss: 508.2253\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 490.2178 - val_loss: 489.8498\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 468.7882 - val_loss: 466.6921\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 442.2917 - val_loss: 437.7791\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 409.4647 - val_loss: 401.8442\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 368.5743 - val_loss: 358.1856\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 319.5676 - val_loss: 305.2572\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 260.8408 - val_loss: 244.0038\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 194.9278 - val_loss: 183.8649\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 39ms/step - loss: 137.8762 - val_loss: 136.8905\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 57ms/step - loss: 111.9043 - val_loss: 116.9262\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 49ms/step - loss: 111.7773 - val_loss: 116.4385\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 46ms/step - loss: 112.9928 - val_loss: 114.6943\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 108.8504 - val_loss: 113.8601\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 105.3028 - val_loss: 115.5649\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 103.2278 - val_loss: 116.8372\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 46ms/step - loss: 101.8648 - val_loss: 116.2377\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 60ms/step - loss: 100.8210 - val_loss: 114.6388\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 74ms/step - loss: 99.5902 - val_loss: 113.0500\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 65ms/step - loss: 98.3604 - val_loss: 111.2440\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 97.3311 - val_loss: 109.7079\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 96.6417 - val_loss: 109.7335\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.0876 - val_loss: 108.2768\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 94.3033 - val_loss: 105.5998\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 93.2033 - val_loss: 105.2591\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 91.8848 - val_loss: 103.6117\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 91.0076 - val_loss: 101.5706\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 90.1437 - val_loss: 100.9516\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 88.9823 - val_loss: 99.5157\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 88.0744 - val_loss: 97.6704\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.2152 - val_loss: 97.3016\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.5368 - val_loss: 96.3153\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.6949 - val_loss: 93.8742\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.5924 - val_loss: 92.3192\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.1826 - val_loss: 92.0655\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 83.3235 - val_loss: 91.4528\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 82.4775 - val_loss: 90.4948\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.5506 - val_loss: 89.2269\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.9203 - val_loss: 87.9288\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.3183 - val_loss: 87.1955\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 79.4269 - val_loss: 86.7230\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.5059 - val_loss: 85.9477\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.8915 - val_loss: 84.9819\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.9925 - val_loss: 83.8180\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.3929 - val_loss: 83.0298\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 75.9564 - val_loss: 82.1333\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.1163 - val_loss: 81.7122\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.8893 - val_loss: 81.3445\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 73.8342 - val_loss: 81.1428\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.7489 - val_loss: 80.3053\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 72.9337 - val_loss: 79.8034\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 72.4380 - val_loss: 79.0577\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.0139 - val_loss: 78.4144\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 71.4846 - val_loss: 78.0711\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 71.0886 - val_loss: 77.3179\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 71.0559 - val_loss: 76.8091\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 70.5314 - val_loss: 76.3747\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 70.2442 - val_loss: 76.3633\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 69.7232 - val_loss: 76.2917\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 69.7309 - val_loss: 75.8028\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 69.2342 - val_loss: 75.1059\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 69.1698 - val_loss: 75.2525\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 68.5842 - val_loss: 74.2049\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 68.3517 - val_loss: 74.3628\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 68.2064 - val_loss: 73.8894\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 68.3843 - val_loss: 73.3974\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 68.3641 - val_loss: 73.6859\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 67.9355 - val_loss: 73.4813\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 68.0047 - val_loss: 73.6219\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 67.3320 - val_loss: 72.9998\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 67.3890 - val_loss: 73.2208\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 67.2952 - val_loss: 73.3752\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.9831 - val_loss: 72.6889\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 66.8927 - val_loss: 72.6238\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.6234 - val_loss: 72.7395\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.9363 - val_loss: 72.5625\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.4378 - val_loss: 72.7013\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.5087 - val_loss: 72.6455\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.5237 - val_loss: 71.8757\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 66.3349 - val_loss: 72.2246\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 66.1994 - val_loss: 71.3457\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 66.1018 - val_loss: 71.2699\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 66.0079 - val_loss: 71.6635\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 66.0101 - val_loss: 71.8970\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.9116 - val_loss: 71.5176\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.8785 - val_loss: 71.5247\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.6988 - val_loss: 71.5060\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 65.6279 - val_loss: 71.0763\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 65.5416 - val_loss: 70.9466\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.6152 - val_loss: 71.1375\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.4036 - val_loss: 71.1736\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.3838 - val_loss: 70.8219\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.3190 - val_loss: 70.8940\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 65.2204 - val_loss: 71.0465\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.1698 - val_loss: 70.7516\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 65.1140 - val_loss: 70.9223\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 65.1406 - val_loss: 70.7822\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 65.1401 - val_loss: 69.8595\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 65.1475 - val_loss: 69.9660\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.9023 - val_loss: 70.5396\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 65.9028 - val_loss: 71.7675\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.1669 - val_loss: 70.3515\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.1519 - val_loss: 69.9527\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.7315 - val_loss: 70.2385\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.5919 - val_loss: 70.6282\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.8194 - val_loss: 70.2215\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.5978 - val_loss: 69.7971\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.5201 - val_loss: 70.0045\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.8213 - val_loss: 70.6525\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.6031 - val_loss: 70.9470\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.4980 - val_loss: 70.3450\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.5858 - val_loss: 69.9606\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.5025 - val_loss: 69.4150\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4673 - val_loss: 70.0870\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.3429 - val_loss: 69.9081\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.3111 - val_loss: 69.5892\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.3283 - val_loss: 70.0550\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.1140 - val_loss: 69.7150\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.1990 - val_loss: 70.0304\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.2669 - val_loss: 69.8558\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.0973 - val_loss: 69.1300\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.1739 - val_loss: 69.3760\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 64.0348 - val_loss: 69.9631\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.4096 - val_loss: 70.0797\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 63.9733 - val_loss: 69.4669\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.0763 - val_loss: 69.3350\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.9502 - val_loss: 69.9878\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.1851 - val_loss: 69.5502\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.9192 - val_loss: 68.6713\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.3562 - val_loss: 68.4128\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 63.9943 - val_loss: 69.8478\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.9514 - val_loss: 69.7601\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 63.9249 - val_loss: 69.4429\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4066 - val_loss: 68.3746\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 63.7504 - val_loss: 69.6100\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 63.9509 - val_loss: 69.4625\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 63.6499 - val_loss: 69.5201\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.2485 - val_loss: 69.7919\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 63.8747 - val_loss: 69.6690\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.9251 - val_loss: 68.9338\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.0262 - val_loss: 68.6290\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 63.6903 - val_loss: 68.8878\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 63.6681 - val_loss: 68.7672\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.6680 - val_loss: 68.9407\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 69.1366\n",
            "69.13655853271484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "6KiZoYwnwbZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2013 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2013)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358e25fd-d7e4-41d0-9244-cda9569ee519",
        "id": "EF-wt_MZwbZ9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 43ms/step - loss: 555.5512 - val_loss: 563.4540\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 555.1176 - val_loss: 563.0220\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 554.6157 - val_loss: 562.4616\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 553.9651 - val_loss: 561.6738\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 553.0170 - val_loss: 560.5135\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 551.6561 - val_loss: 558.8165\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 549.6937 - val_loss: 556.3930\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 546.9019 - val_loss: 552.9760\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 542.9726 - val_loss: 548.2239\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 537.5522 - val_loss: 541.7420\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 530.3064 - val_loss: 533.0032\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 520.4986 - val_loss: 521.4464\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 507.8753 - val_loss: 506.4155\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 491.3361 - val_loss: 487.1029\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 470.1740 - val_loss: 462.6510\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 443.6319 - val_loss: 432.1237\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 410.8810 - val_loss: 394.2814\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 370.6750 - val_loss: 348.1584\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 321.6730 - val_loss: 293.6572\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 264.7110 - val_loss: 240.7821\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 208.4323 - val_loss: 201.7872\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 169.2664 - val_loss: 184.7964\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 154.2985 - val_loss: 171.1297\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 143.6453 - val_loss: 161.0291\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 137.1172 - val_loss: 155.3721\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 132.8906 - val_loss: 149.4519\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 128.2579 - val_loss: 144.8148\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 123.8842 - val_loss: 141.7304\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 121.1621 - val_loss: 139.7794\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 118.5289 - val_loss: 137.9395\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 115.9831 - val_loss: 135.3105\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 113.8464 - val_loss: 132.9901\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 111.7403 - val_loss: 130.1225\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 109.6360 - val_loss: 127.6555\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 108.3716 - val_loss: 125.8210\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 106.3897 - val_loss: 123.3526\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 104.8002 - val_loss: 120.8010\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 103.6395 - val_loss: 118.4612\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 102.1406 - val_loss: 117.0542\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 100.7976 - val_loss: 115.6748\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.4105 - val_loss: 113.6719\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 98.2026 - val_loss: 112.1148\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 97.0760 - val_loss: 110.6524\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.9755 - val_loss: 109.4461\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 94.9090 - val_loss: 107.4026\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 93.6859 - val_loss: 105.7156\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.6705 - val_loss: 104.7518\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.6041 - val_loss: 103.5093\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 90.7611 - val_loss: 102.2990\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 89.5173 - val_loss: 100.9844\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.7672 - val_loss: 99.7354\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 87.7943 - val_loss: 98.0875\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.5935 - val_loss: 97.9020\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.8587 - val_loss: 97.0922\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.9190 - val_loss: 95.8070\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 83.6861 - val_loss: 94.1583\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.6921 - val_loss: 92.6425\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.8503 - val_loss: 91.3334\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.8481 - val_loss: 90.6305\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.9882 - val_loss: 90.2706\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.0581 - val_loss: 88.4389\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 78.1061 - val_loss: 86.7475\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 77.2974 - val_loss: 85.5041\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.3184 - val_loss: 84.2252\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.6699 - val_loss: 83.3806\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.8768 - val_loss: 82.0738\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 74.0095 - val_loss: 80.8799\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.4129 - val_loss: 79.8414\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 72.6360 - val_loss: 78.5703\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.3535 - val_loss: 77.7641\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 71.6541 - val_loss: 78.2634\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 71.5146 - val_loss: 77.2348\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 70.7285 - val_loss: 76.1322\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 70.6485 - val_loss: 77.0753\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 69.9380 - val_loss: 76.5946\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 69.4771 - val_loss: 75.7232\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 69.5018 - val_loss: 74.4897\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 68.8035 - val_loss: 74.8934\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 68.7239 - val_loss: 75.0014\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 68.5873 - val_loss: 74.2786\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 68.1622 - val_loss: 73.5350\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 67.9381 - val_loss: 73.7066\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 67.7817 - val_loss: 73.7230\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 67.7924 - val_loss: 73.4542\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 67.1848 - val_loss: 72.2633\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 67.4012 - val_loss: 71.4045\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 67.5060 - val_loss: 72.2638\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 66.9925 - val_loss: 71.2176\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 67.2429 - val_loss: 71.0997\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 66.5361 - val_loss: 70.8557\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 66.6716 - val_loss: 71.1721\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 66.6108 - val_loss: 70.2755\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 66.2974 - val_loss: 70.5695\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 66.2219 - val_loss: 70.4391\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.3980 - val_loss: 70.5632\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 66.0568 - val_loss: 69.8542\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 66.3399 - val_loss: 70.0407\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.9279 - val_loss: 69.8004\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 65.9187 - val_loss: 69.1878\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 65.3154 - val_loss: 69.1291\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 65.4607 - val_loss: 69.0697\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 65.4847 - val_loss: 69.1105\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 65.0880 - val_loss: 69.0939\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 65.0968 - val_loss: 69.0266\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 65.1880 - val_loss: 68.7872\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 64.8935 - val_loss: 68.5991\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 65.5586 - val_loss: 68.2987\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 65.1836 - val_loss: 68.8187\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 65.2793 - val_loss: 68.4469\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 64.9198 - val_loss: 67.9329\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 65.1575 - val_loss: 68.2862\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 64.6978 - val_loss: 68.6093\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.6994 - val_loss: 68.3849\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.6491 - val_loss: 68.0694\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.6181 - val_loss: 68.1711\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4291 - val_loss: 68.2387\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.4479 - val_loss: 68.0142\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4259 - val_loss: 67.9103\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.7491 - val_loss: 67.7362\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4816 - val_loss: 68.4850\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 65.0038 - val_loss: 68.1717\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.2919 - val_loss: 67.9824\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 64.3553 - val_loss: 67.8221\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.3619 - val_loss: 67.4787\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.4492 - val_loss: 67.2777\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.3080 - val_loss: 67.4460\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.8736 - val_loss: 67.3038\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 64.2662 - val_loss: 67.8262\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 64.2445 - val_loss: 67.2008\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.1900 - val_loss: 66.9818\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.2290 - val_loss: 67.1941\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.1832 - val_loss: 67.4417\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 64.6028 - val_loss: 67.1131\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 64.0377 - val_loss: 67.5859\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.0356 - val_loss: 67.7752\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 64.0270 - val_loss: 67.6358\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 64.1313 - val_loss: 67.1658\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.9758 - val_loss: 66.9849\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 63.9614 - val_loss: 67.3223\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 63.9114 - val_loss: 67.4021\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 69.7901\n",
            "69.79010009765625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2014"
      ],
      "metadata": {
        "id": "Q-yqMBp90LlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "xpCSoo08wk4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2014\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2014 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2014)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5caba747-9f96-4ad9-9ac8-f7254c96f570",
        "id": "rtCmyndIwk4u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "7/7 [==============================] - 1s 37ms/step - loss: 566.6396 - val_loss: 562.5914\n",
            "Epoch 2/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 566.2859 - val_loss: 562.2432\n",
            "Epoch 3/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 565.8747 - val_loss: 561.7975\n",
            "Epoch 4/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 565.3190 - val_loss: 561.1694\n",
            "Epoch 5/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 564.5075 - val_loss: 560.2188\n",
            "Epoch 6/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 563.2653 - val_loss: 558.7854\n",
            "Epoch 7/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 561.4062 - val_loss: 556.6245\n",
            "Epoch 8/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 558.5993 - val_loss: 553.4224\n",
            "Epoch 9/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 554.5159 - val_loss: 548.7574\n",
            "Epoch 10/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 548.5881 - val_loss: 542.0179\n",
            "Epoch 11/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 540.1226 - val_loss: 532.4288\n",
            "Epoch 12/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 528.0584 - val_loss: 519.1643\n",
            "Epoch 13/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 511.7302 - val_loss: 501.1318\n",
            "Epoch 14/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 489.7347 - val_loss: 477.1373\n",
            "Epoch 15/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 460.3807 - val_loss: 445.8070\n",
            "Epoch 16/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 422.8887 - val_loss: 405.3871\n",
            "Epoch 17/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 374.5241 - val_loss: 354.3158\n",
            "Epoch 18/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 313.7974 - val_loss: 293.0356\n",
            "Epoch 19/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 243.0210 - val_loss: 229.2430\n",
            "Epoch 20/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 175.1761 - val_loss: 174.0833\n",
            "Epoch 21/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 135.1824 - val_loss: 135.0032\n",
            "Epoch 22/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 119.6079 - val_loss: 126.6862\n",
            "Epoch 23/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 115.9466 - val_loss: 120.6702\n",
            "Epoch 24/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 108.7209 - val_loss: 117.2483\n",
            "Epoch 25/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 103.7512 - val_loss: 117.3812\n",
            "Epoch 26/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 103.2802 - val_loss: 115.1016\n",
            "Epoch 27/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 100.5300 - val_loss: 111.5222\n",
            "Epoch 28/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 98.3706 - val_loss: 107.5746\n",
            "Epoch 29/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 97.9879 - val_loss: 104.4188\n",
            "Epoch 30/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 95.8728 - val_loss: 103.3414\n",
            "Epoch 31/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 94.1935 - val_loss: 102.4120\n",
            "Epoch 32/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 92.8506 - val_loss: 100.2499\n",
            "Epoch 33/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 91.4025 - val_loss: 96.7839\n",
            "Epoch 34/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 90.4537 - val_loss: 95.5025\n",
            "Epoch 35/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 89.0418 - val_loss: 95.1490\n",
            "Epoch 36/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 88.4187 - val_loss: 93.4442\n",
            "Epoch 37/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 87.2561 - val_loss: 90.9067\n",
            "Epoch 38/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 86.4773 - val_loss: 89.3394\n",
            "Epoch 39/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 85.6612 - val_loss: 87.8681\n",
            "Epoch 40/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 84.6734 - val_loss: 86.8417\n",
            "Epoch 41/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 83.7537 - val_loss: 85.3999\n",
            "Epoch 42/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 82.9516 - val_loss: 84.3518\n",
            "Epoch 43/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 82.0499 - val_loss: 83.2863\n",
            "Epoch 44/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 81.3563 - val_loss: 82.3675\n",
            "Epoch 45/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 80.7239 - val_loss: 81.4483\n",
            "Epoch 46/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 80.0552 - val_loss: 80.8056\n",
            "Epoch 47/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 79.4869 - val_loss: 80.0159\n",
            "Epoch 48/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 79.0816 - val_loss: 79.3195\n",
            "Epoch 49/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 78.6631 - val_loss: 78.6499\n",
            "Epoch 50/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 77.8544 - val_loss: 78.1581\n",
            "Epoch 51/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 77.5797 - val_loss: 77.4989\n",
            "Epoch 52/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 76.9931 - val_loss: 77.1163\n",
            "Epoch 53/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 76.7905 - val_loss: 76.6206\n",
            "Epoch 54/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 76.4024 - val_loss: 76.3196\n",
            "Epoch 55/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 76.0368 - val_loss: 75.8324\n",
            "Epoch 56/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 75.6296 - val_loss: 75.3531\n",
            "Epoch 57/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 75.5859 - val_loss: 74.9334\n",
            "Epoch 58/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 75.1318 - val_loss: 74.6877\n",
            "Epoch 59/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 74.9263 - val_loss: 74.6196\n",
            "Epoch 60/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.6753 - val_loss: 74.3431\n",
            "Epoch 61/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.5858 - val_loss: 74.0520\n",
            "Epoch 62/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 74.1916 - val_loss: 73.6283\n",
            "Epoch 63/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.0503 - val_loss: 73.1265\n",
            "Epoch 64/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 74.0646 - val_loss: 72.9317\n",
            "Epoch 65/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 73.7472 - val_loss: 72.6519\n",
            "Epoch 66/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.6283 - val_loss: 72.5306\n",
            "Epoch 67/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.3640 - val_loss: 72.7633\n",
            "Epoch 68/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.6696 - val_loss: 71.7930\n",
            "Epoch 69/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 73.2992 - val_loss: 71.8908\n",
            "Epoch 70/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 72.9578 - val_loss: 72.0410\n",
            "Epoch 71/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.9218 - val_loss: 71.5370\n",
            "Epoch 72/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.7536 - val_loss: 70.8679\n",
            "Epoch 73/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.5696 - val_loss: 70.6929\n",
            "Epoch 74/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 73.4119 - val_loss: 71.7175\n",
            "Epoch 75/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 72.2385 - val_loss: 70.1672\n",
            "Epoch 76/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 72.4007 - val_loss: 69.8585\n",
            "Epoch 77/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 72.1396 - val_loss: 69.8593\n",
            "Epoch 78/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 72.1460 - val_loss: 69.4389\n",
            "Epoch 79/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.0562 - val_loss: 69.9750\n",
            "Epoch 80/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.8146 - val_loss: 69.3868\n",
            "Epoch 81/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.7947 - val_loss: 68.9239\n",
            "Epoch 82/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 71.9983 - val_loss: 69.1244\n",
            "Epoch 83/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.4531 - val_loss: 69.4223\n",
            "Epoch 84/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 71.6338 - val_loss: 69.0922\n",
            "Epoch 85/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 71.6783 - val_loss: 68.3502\n",
            "Epoch 86/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 71.2362 - val_loss: 69.3116\n",
            "Epoch 87/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 71.1102 - val_loss: 68.7112\n",
            "Epoch 88/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.9921 - val_loss: 68.3902\n",
            "Epoch 89/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 71.0998 - val_loss: 68.2326\n",
            "Epoch 90/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 71.0010 - val_loss: 68.7803\n",
            "Epoch 91/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 70.9396 - val_loss: 68.0848\n",
            "Epoch 92/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.9802 - val_loss: 67.9169\n",
            "Epoch 93/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.8338 - val_loss: 67.8152\n",
            "Epoch 94/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 71.0320 - val_loss: 68.0033\n",
            "Epoch 95/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.2181 - val_loss: 68.8621\n",
            "Epoch 96/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6352 - val_loss: 67.8932\n",
            "Epoch 97/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6516 - val_loss: 67.3514\n",
            "Epoch 98/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 70.4442 - val_loss: 68.5111\n",
            "Epoch 99/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6013 - val_loss: 68.0042\n",
            "Epoch 100/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.4337 - val_loss: 67.5542\n",
            "Epoch 101/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.5886 - val_loss: 67.5853\n",
            "Epoch 102/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 70.3187 - val_loss: 68.1439\n",
            "Epoch 103/500\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 70.4978 - val_loss: 67.3635\n",
            "Epoch 104/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 70.3895 - val_loss: 68.4205\n",
            "Epoch 105/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 70.3609 - val_loss: 67.2856\n",
            "Epoch 106/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 70.2563 - val_loss: 67.3755\n",
            "Epoch 107/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 70.1548 - val_loss: 68.0972\n",
            "Epoch 108/500\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 70.7712 - val_loss: 66.8577\n",
            "Epoch 109/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.0301 - val_loss: 67.6414\n",
            "Epoch 110/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 70.2689 - val_loss: 67.2747\n",
            "Epoch 111/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 70.2346 - val_loss: 66.6601\n",
            "Epoch 112/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 70.1553 - val_loss: 67.3458\n",
            "Epoch 113/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.9318 - val_loss: 67.1573\n",
            "Epoch 114/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.9481 - val_loss: 66.8068\n",
            "Epoch 115/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.4024 - val_loss: 66.4992\n",
            "Epoch 116/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 70.2641 - val_loss: 67.5933\n",
            "Epoch 117/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.9912 - val_loss: 66.8532\n",
            "Epoch 118/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 69.8079 - val_loss: 67.5285\n",
            "Epoch 119/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 69.8083 - val_loss: 66.8951\n",
            "Epoch 120/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 69.9846 - val_loss: 67.1580\n",
            "Epoch 121/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.7941 - val_loss: 66.6060\n",
            "Epoch 122/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.7240 - val_loss: 67.0015\n",
            "Epoch 123/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 69.8280 - val_loss: 66.8312\n",
            "Epoch 124/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.9845 - val_loss: 66.2943\n",
            "Epoch 125/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.9703 - val_loss: 67.5098\n",
            "Epoch 126/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.5662 - val_loss: 66.6169\n",
            "Epoch 127/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.7955 - val_loss: 66.5008\n",
            "Epoch 128/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.6552 - val_loss: 67.4046\n",
            "Epoch 129/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 69.7773 - val_loss: 67.1441\n",
            "Epoch 130/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.5847 - val_loss: 66.9207\n",
            "Epoch 131/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.9877 - val_loss: 66.6421\n",
            "Epoch 132/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 69.5582 - val_loss: 66.1031\n",
            "Epoch 133/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.8752 - val_loss: 66.2839\n",
            "Epoch 134/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.7422 - val_loss: 67.2115\n",
            "Epoch 135/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.7655 - val_loss: 66.3735\n",
            "Epoch 136/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.7337 - val_loss: 66.5703\n",
            "Epoch 137/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 70.2056 - val_loss: 68.3716\n",
            "Epoch 138/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.5456 - val_loss: 66.4535\n",
            "Epoch 139/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 69.4834 - val_loss: 66.5634\n",
            "Epoch 140/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.7026 - val_loss: 67.4645\n",
            "Epoch 141/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.4341 - val_loss: 66.4465\n",
            "Epoch 142/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 69.4099 - val_loss: 66.3676\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 63.8734\n",
            "63.87338638305664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "_zK-2K83wk4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2014 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2014)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jryrf2yLwk4v",
        "outputId": "f14482ae-595e-4f61-a285-7044431e243b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "7/7 [==============================] - 2s 34ms/step - loss: 566.7394 - val_loss: 562.6801\n",
            "Epoch 2/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 566.4142 - val_loss: 562.3809\n",
            "Epoch 3/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 566.0861 - val_loss: 562.0047\n",
            "Epoch 4/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 565.6485 - val_loss: 561.4926\n",
            "Epoch 5/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 565.0327 - val_loss: 560.7680\n",
            "Epoch 6/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 564.1539 - val_loss: 559.7130\n",
            "Epoch 7/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 562.8539 - val_loss: 558.1393\n",
            "Epoch 8/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 560.9222 - val_loss: 555.8013\n",
            "Epoch 9/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 558.0677 - val_loss: 552.3707\n",
            "Epoch 10/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 553.8757 - val_loss: 547.4130\n",
            "Epoch 11/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 547.8483 - val_loss: 540.3395\n",
            "Epoch 12/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 539.3327 - val_loss: 530.4506\n",
            "Epoch 13/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 527.6240 - val_loss: 516.8655\n",
            "Epoch 14/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 511.5043 - val_loss: 498.8151\n",
            "Epoch 15/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 490.6360 - val_loss: 475.1698\n",
            "Epoch 16/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 463.0512 - val_loss: 444.8167\n",
            "Epoch 17/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 427.9606 - val_loss: 406.3441\n",
            "Epoch 18/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 384.1786 - val_loss: 358.2346\n",
            "Epoch 19/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 329.5724 - val_loss: 299.3355\n",
            "Epoch 20/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 264.8365 - val_loss: 228.8388\n",
            "Epoch 21/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 191.6500 - val_loss: 154.9684\n",
            "Epoch 22/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 131.8769 - val_loss: 115.1978\n",
            "Epoch 23/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 110.6182 - val_loss: 117.1563\n",
            "Epoch 24/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 114.5527 - val_loss: 116.3578\n",
            "Epoch 25/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 109.8683 - val_loss: 109.8865\n",
            "Epoch 26/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 105.5204 - val_loss: 108.0323\n",
            "Epoch 27/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 104.0845 - val_loss: 106.7389\n",
            "Epoch 28/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 102.0870 - val_loss: 104.9040\n",
            "Epoch 29/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 100.4393 - val_loss: 104.4376\n",
            "Epoch 30/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 98.9406 - val_loss: 103.2791\n",
            "Epoch 31/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 97.6793 - val_loss: 102.0312\n",
            "Epoch 32/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 96.6755 - val_loss: 100.3803\n",
            "Epoch 33/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 95.7711 - val_loss: 99.0604\n",
            "Epoch 34/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 94.7404 - val_loss: 97.6098\n",
            "Epoch 35/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 94.0202 - val_loss: 95.7730\n",
            "Epoch 36/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 93.2055 - val_loss: 94.3929\n",
            "Epoch 37/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 92.2489 - val_loss: 93.6184\n",
            "Epoch 38/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 91.4713 - val_loss: 92.2661\n",
            "Epoch 39/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 90.4906 - val_loss: 91.5745\n",
            "Epoch 40/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 89.7263 - val_loss: 90.4696\n",
            "Epoch 41/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 88.7841 - val_loss: 89.6033\n",
            "Epoch 42/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 88.1464 - val_loss: 89.3444\n",
            "Epoch 43/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 87.1837 - val_loss: 88.1119\n",
            "Epoch 44/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 86.6346 - val_loss: 87.1202\n",
            "Epoch 45/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 85.8120 - val_loss: 85.6312\n",
            "Epoch 46/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 84.8977 - val_loss: 85.2214\n",
            "Epoch 47/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 84.0941 - val_loss: 84.4432\n",
            "Epoch 48/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 83.4517 - val_loss: 83.7396\n",
            "Epoch 49/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 82.8545 - val_loss: 82.8223\n",
            "Epoch 50/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 82.0913 - val_loss: 81.4441\n",
            "Epoch 51/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 81.3486 - val_loss: 80.8086\n",
            "Epoch 52/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 80.8022 - val_loss: 80.4499\n",
            "Epoch 53/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 80.0676 - val_loss: 79.3446\n",
            "Epoch 54/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 79.6182 - val_loss: 78.8472\n",
            "Epoch 55/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 79.1401 - val_loss: 78.3638\n",
            "Epoch 56/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 78.7343 - val_loss: 78.4632\n",
            "Epoch 57/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 78.2553 - val_loss: 77.9675\n",
            "Epoch 58/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 77.8236 - val_loss: 77.0195\n",
            "Epoch 59/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 77.7233 - val_loss: 76.5971\n",
            "Epoch 60/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 77.1357 - val_loss: 76.6339\n",
            "Epoch 61/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 76.8962 - val_loss: 75.7734\n",
            "Epoch 62/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 76.8027 - val_loss: 75.6188\n",
            "Epoch 63/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 76.2768 - val_loss: 75.7583\n",
            "Epoch 64/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 76.6315 - val_loss: 75.6343\n",
            "Epoch 65/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 75.9388 - val_loss: 75.2035\n",
            "Epoch 66/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 75.7276 - val_loss: 74.4782\n",
            "Epoch 67/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 75.6482 - val_loss: 74.4839\n",
            "Epoch 68/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 75.2278 - val_loss: 74.0219\n",
            "Epoch 69/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 75.2046 - val_loss: 74.0831\n",
            "Epoch 70/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 75.4267 - val_loss: 74.1169\n",
            "Epoch 71/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.9322 - val_loss: 73.1743\n",
            "Epoch 72/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 74.8748 - val_loss: 72.9745\n",
            "Epoch 73/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 74.2617 - val_loss: 72.9190\n",
            "Epoch 74/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.3188 - val_loss: 72.6566\n",
            "Epoch 75/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.2709 - val_loss: 72.9874\n",
            "Epoch 76/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 74.0696 - val_loss: 72.7773\n",
            "Epoch 77/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.8060 - val_loss: 72.5945\n",
            "Epoch 78/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.6267 - val_loss: 72.2766\n",
            "Epoch 79/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 74.3001 - val_loss: 72.5408\n",
            "Epoch 80/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 73.3883 - val_loss: 71.5521\n",
            "Epoch 81/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.9124 - val_loss: 71.8873\n",
            "Epoch 82/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.6530 - val_loss: 72.6006\n",
            "Epoch 83/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.4958 - val_loss: 72.4736\n",
            "Epoch 84/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.8568 - val_loss: 71.7263\n",
            "Epoch 85/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.7953 - val_loss: 71.7087\n",
            "Epoch 86/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.6605 - val_loss: 71.0662\n",
            "Epoch 87/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 72.5389 - val_loss: 70.7310\n",
            "Epoch 88/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.7236 - val_loss: 71.1259\n",
            "Epoch 89/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.3839 - val_loss: 71.3212\n",
            "Epoch 90/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.0617 - val_loss: 71.3013\n",
            "Epoch 91/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.0573 - val_loss: 71.5734\n",
            "Epoch 92/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.0344 - val_loss: 71.0364\n",
            "Epoch 93/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.9035 - val_loss: 70.9119\n",
            "Epoch 94/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 72.0743 - val_loss: 70.5133\n",
            "Epoch 95/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.7182 - val_loss: 70.7815\n",
            "Epoch 96/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 71.6917 - val_loss: 71.0134\n",
            "Epoch 97/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 71.6047 - val_loss: 70.8666\n",
            "Epoch 98/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.2912 - val_loss: 70.5385\n",
            "Epoch 99/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 71.2663 - val_loss: 70.1867\n",
            "Epoch 100/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.4534 - val_loss: 69.9234\n",
            "Epoch 101/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.9868 - val_loss: 70.6647\n",
            "Epoch 102/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.8301 - val_loss: 71.8832\n",
            "Epoch 103/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.8307 - val_loss: 70.1672\n",
            "Epoch 104/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.7684 - val_loss: 70.1249\n",
            "Epoch 105/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 71.5609 - val_loss: 71.7197\n",
            "Epoch 106/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 71.1636 - val_loss: 70.2042\n",
            "Epoch 107/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.2024 - val_loss: 69.9644\n",
            "Epoch 108/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.8087 - val_loss: 69.9430\n",
            "Epoch 109/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6885 - val_loss: 69.4930\n",
            "Epoch 110/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.1489 - val_loss: 69.5250\n",
            "Epoch 111/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.8906 - val_loss: 69.4210\n",
            "Epoch 112/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.6898 - val_loss: 69.9951\n",
            "Epoch 113/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6019 - val_loss: 70.3773\n",
            "Epoch 114/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.3807 - val_loss: 70.4377\n",
            "Epoch 115/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 70.5046 - val_loss: 70.0342\n",
            "Epoch 116/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.4160 - val_loss: 70.1288\n",
            "Epoch 117/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 70.2508 - val_loss: 69.8160\n",
            "Epoch 118/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.3682 - val_loss: 69.8188\n",
            "Epoch 119/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.5773 - val_loss: 69.8359\n",
            "Epoch 120/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.1109 - val_loss: 69.3616\n",
            "Epoch 121/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.9142 - val_loss: 69.4009\n",
            "Epoch 122/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.8357 - val_loss: 70.4850\n",
            "Epoch 123/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 71.3679 - val_loss: 70.5649\n",
            "Epoch 124/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 70.1146 - val_loss: 69.6157\n",
            "Epoch 125/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 71.0744 - val_loss: 70.7627\n",
            "Epoch 126/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.2601 - val_loss: 70.4784\n",
            "Epoch 127/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.0822 - val_loss: 69.4754\n",
            "Epoch 128/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.8492 - val_loss: 69.7487\n",
            "Epoch 129/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 69.9246 - val_loss: 69.7400\n",
            "Epoch 130/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 69.8560 - val_loss: 69.6663\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 67.3879\n",
            "67.38787078857422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "ezC5jZtqwk4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2014 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2014)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21511b88-1ebd-4723-b8c6-80a61f45cd48",
        "id": "-FeCP6tCwk4v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "7/7 [==============================] - 1s 53ms/step - loss: 566.5654 - val_loss: 562.4951\n",
            "Epoch 2/500\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 566.1370 - val_loss: 562.0150\n",
            "Epoch 3/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 565.5546 - val_loss: 561.3242\n",
            "Epoch 4/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 564.6951 - val_loss: 560.2905\n",
            "Epoch 5/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 563.3940 - val_loss: 558.7430\n",
            "Epoch 6/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 561.4703 - val_loss: 556.4117\n",
            "Epoch 7/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 558.5735 - val_loss: 552.9141\n",
            "Epoch 8/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 554.2157 - val_loss: 547.7513\n",
            "Epoch 9/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 547.8369 - val_loss: 540.3026\n",
            "Epoch 10/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 538.8280 - val_loss: 529.6605\n",
            "Epoch 11/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 526.0515 - val_loss: 514.8115\n",
            "Epoch 12/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 508.3002 - val_loss: 494.4594\n",
            "Epoch 13/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 484.3800 - val_loss: 467.1821\n",
            "Epoch 14/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 452.3009 - val_loss: 431.4832\n",
            "Epoch 15/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 411.0854 - val_loss: 385.2078\n",
            "Epoch 16/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 357.3865 - val_loss: 326.4400\n",
            "Epoch 17/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 291.7316 - val_loss: 261.1069\n",
            "Epoch 18/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 220.3429 - val_loss: 205.8524\n",
            "Epoch 19/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 169.1281 - val_loss: 174.4997\n",
            "Epoch 20/500\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 148.7094 - val_loss: 166.3524\n",
            "Epoch 21/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 143.4895 - val_loss: 164.3342\n",
            "Epoch 22/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 139.1596 - val_loss: 157.2943\n",
            "Epoch 23/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 133.3534 - val_loss: 150.3156\n",
            "Epoch 24/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 127.2359 - val_loss: 145.8794\n",
            "Epoch 25/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 123.4023 - val_loss: 142.6037\n",
            "Epoch 26/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 120.0519 - val_loss: 138.6262\n",
            "Epoch 27/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 116.5258 - val_loss: 134.4532\n",
            "Epoch 28/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 113.6548 - val_loss: 130.6384\n",
            "Epoch 29/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 110.9936 - val_loss: 127.7205\n",
            "Epoch 30/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 108.0338 - val_loss: 123.9588\n",
            "Epoch 31/500\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 105.7070 - val_loss: 120.6058\n",
            "Epoch 32/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 103.4656 - val_loss: 116.7268\n",
            "Epoch 33/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 101.1866 - val_loss: 113.9533\n",
            "Epoch 34/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 99.3362 - val_loss: 110.5277\n",
            "Epoch 35/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 97.5092 - val_loss: 107.2926\n",
            "Epoch 36/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 95.6743 - val_loss: 104.4901\n",
            "Epoch 37/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 94.4670 - val_loss: 102.0546\n",
            "Epoch 38/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 92.5411 - val_loss: 99.5535\n",
            "Epoch 39/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 91.2419 - val_loss: 96.9350\n",
            "Epoch 40/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 89.8863 - val_loss: 93.9547\n",
            "Epoch 41/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 88.6921 - val_loss: 91.8111\n",
            "Epoch 42/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 87.2800 - val_loss: 89.9446\n",
            "Epoch 43/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 86.5781 - val_loss: 88.3850\n",
            "Epoch 44/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 85.0652 - val_loss: 86.9086\n",
            "Epoch 45/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 84.2170 - val_loss: 85.4207\n",
            "Epoch 46/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 83.2071 - val_loss: 83.8793\n",
            "Epoch 47/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 82.0927 - val_loss: 82.1246\n",
            "Epoch 48/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 81.2287 - val_loss: 80.9526\n",
            "Epoch 49/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 80.7147 - val_loss: 80.6392\n",
            "Epoch 50/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 79.6657 - val_loss: 78.6928\n",
            "Epoch 51/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 79.0115 - val_loss: 77.8879\n",
            "Epoch 52/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 78.0561 - val_loss: 77.0038\n",
            "Epoch 53/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 77.6816 - val_loss: 77.2254\n",
            "Epoch 54/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 77.0841 - val_loss: 75.4697\n",
            "Epoch 55/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 76.4216 - val_loss: 75.1796\n",
            "Epoch 56/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 75.7101 - val_loss: 74.6639\n",
            "Epoch 57/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 75.4929 - val_loss: 74.2688\n",
            "Epoch 58/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.8462 - val_loss: 73.5098\n",
            "Epoch 59/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 75.0088 - val_loss: 73.0623\n",
            "Epoch 60/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 74.3286 - val_loss: 72.9840\n",
            "Epoch 61/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 74.1597 - val_loss: 72.3076\n",
            "Epoch 62/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 73.9335 - val_loss: 72.1071\n",
            "Epoch 63/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 73.5259 - val_loss: 71.7473\n",
            "Epoch 64/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 73.1275 - val_loss: 71.6364\n",
            "Epoch 65/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.8872 - val_loss: 72.2439\n",
            "Epoch 66/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 72.8725 - val_loss: 71.3796\n",
            "Epoch 67/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 72.7487 - val_loss: 70.9730\n",
            "Epoch 68/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 72.1875 - val_loss: 71.1801\n",
            "Epoch 69/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 72.1247 - val_loss: 71.1831\n",
            "Epoch 70/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 71.9027 - val_loss: 70.1909\n",
            "Epoch 71/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 71.7337 - val_loss: 70.3844\n",
            "Epoch 72/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 71.2615 - val_loss: 69.8425\n",
            "Epoch 73/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 71.1443 - val_loss: 69.3813\n",
            "Epoch 74/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 71.2654 - val_loss: 69.6276\n",
            "Epoch 75/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 70.8152 - val_loss: 69.3852\n",
            "Epoch 76/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 70.7301 - val_loss: 68.9155\n",
            "Epoch 77/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6775 - val_loss: 68.7271\n",
            "Epoch 78/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.4534 - val_loss: 68.8979\n",
            "Epoch 79/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.0972 - val_loss: 69.2093\n",
            "Epoch 80/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.2448 - val_loss: 68.4481\n",
            "Epoch 81/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 70.6045 - val_loss: 68.1312\n",
            "Epoch 82/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.7730 - val_loss: 68.2284\n",
            "Epoch 83/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.8013 - val_loss: 68.2642\n",
            "Epoch 84/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 70.2117 - val_loss: 67.6735\n",
            "Epoch 85/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.5440 - val_loss: 68.7543\n",
            "Epoch 86/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 69.8686 - val_loss: 68.4787\n",
            "Epoch 87/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 69.2734 - val_loss: 67.1511\n",
            "Epoch 88/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 69.5217 - val_loss: 67.3776\n",
            "Epoch 89/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 69.1579 - val_loss: 67.5916\n",
            "Epoch 90/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 69.0645 - val_loss: 67.1207\n",
            "Epoch 91/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 69.4881 - val_loss: 68.1675\n",
            "Epoch 92/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 69.1426 - val_loss: 67.3222\n",
            "Epoch 93/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.9588 - val_loss: 67.3381\n",
            "Epoch 94/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 68.6858 - val_loss: 66.9188\n",
            "Epoch 95/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 68.7776 - val_loss: 66.9845\n",
            "Epoch 96/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.7415 - val_loss: 67.0253\n",
            "Epoch 97/500\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 68.6743 - val_loss: 66.8859\n",
            "Epoch 98/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 68.5158 - val_loss: 66.6264\n",
            "Epoch 99/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.5089 - val_loss: 66.9086\n",
            "Epoch 100/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.6060 - val_loss: 66.5729\n",
            "Epoch 101/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 68.8097 - val_loss: 67.7883\n",
            "Epoch 102/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.7648 - val_loss: 66.4475\n",
            "Epoch 103/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 68.1323 - val_loss: 66.7582\n",
            "Epoch 104/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 68.8397 - val_loss: 67.2363\n",
            "Epoch 105/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 69.6822 - val_loss: 66.5540\n",
            "Epoch 106/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.8664 - val_loss: 67.7099\n",
            "Epoch 107/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.8769 - val_loss: 66.8395\n",
            "Epoch 108/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 68.0775 - val_loss: 66.4190\n",
            "Epoch 109/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 67.7046 - val_loss: 66.7992\n",
            "Epoch 110/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 67.8649 - val_loss: 66.5526\n",
            "Epoch 111/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.5706 - val_loss: 66.2870\n",
            "Epoch 112/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.6751 - val_loss: 66.2490\n",
            "Epoch 113/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.3920 - val_loss: 66.4223\n",
            "Epoch 114/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 67.3160 - val_loss: 66.1300\n",
            "Epoch 115/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.5252 - val_loss: 66.3226\n",
            "Epoch 116/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.4925 - val_loss: 65.9923\n",
            "Epoch 117/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.1332 - val_loss: 66.3820\n",
            "Epoch 118/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 67.1583 - val_loss: 66.5221\n",
            "Epoch 119/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.2251 - val_loss: 66.1409\n",
            "Epoch 120/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 67.1562 - val_loss: 66.0050\n",
            "Epoch 121/500\n",
            "7/7 [==============================] - 0s 9ms/step - loss: 67.1955 - val_loss: 66.1567\n",
            "Epoch 122/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.2587 - val_loss: 67.2168\n",
            "Epoch 123/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 67.7127 - val_loss: 66.0298\n",
            "Epoch 124/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 66.9829 - val_loss: 66.1827\n",
            "Epoch 125/500\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 66.8442 - val_loss: 65.8442\n",
            "Epoch 126/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 66.9660 - val_loss: 65.9185\n",
            "Epoch 127/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 67.0541 - val_loss: 66.6153\n",
            "Epoch 128/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 66.9658 - val_loss: 66.3177\n",
            "Epoch 129/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 67.0175 - val_loss: 66.0230\n",
            "Epoch 130/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 66.8718 - val_loss: 65.9176\n",
            "Epoch 131/500\n",
            "7/7 [==============================] - 0s 10ms/step - loss: 66.6856 - val_loss: 66.2354\n",
            "Epoch 132/500\n",
            "7/7 [==============================] - 0s 8ms/step - loss: 66.7007 - val_loss: 66.2440\n",
            "Epoch 133/500\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 66.4815 - val_loss: 65.9831\n",
            "Epoch 134/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 66.4292 - val_loss: 66.2102\n",
            "Epoch 135/500\n",
            "7/7 [==============================] - 0s 11ms/step - loss: 66.7125 - val_loss: 66.2376\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 65.6756\n",
            "65.67560577392578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2015"
      ],
      "metadata": {
        "id": "zvzclKFE0b9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "_Klk15WkwrgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2015\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2015 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2015)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e4cc96-03aa-4a1a-ed5d-730b92e737f5",
        "id": "CXvo4B9lwrgY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 39ms/step - loss: 596.0302 - val_loss: 599.6404\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 595.6147 - val_loss: 599.2271\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 595.1700 - val_loss: 598.7238\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 594.6141 - val_loss: 598.0638\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 593.8555 - val_loss: 597.1643\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 592.8172 - val_loss: 595.9252\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 591.3865 - val_loss: 594.2122\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 589.3907 - val_loss: 591.8755\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 586.6041 - val_loss: 588.6786\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 582.8991 - val_loss: 584.3667\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 577.8227 - val_loss: 578.5591\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 571.0647 - val_loss: 570.9164\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 562.1971 - val_loss: 560.9600\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 550.4800 - val_loss: 548.0346\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 535.6593 - val_loss: 531.6031\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 516.9007 - val_loss: 510.9409\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 493.2711 - val_loss: 485.3870\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 464.3299 - val_loss: 453.9327\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 429.0157 - val_loss: 416.1492\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 387.0941 - val_loss: 370.7653\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 337.0037 - val_loss: 316.6867\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 280.3132 - val_loss: 253.0794\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 221.7809 - val_loss: 184.7437\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 167.0574 - val_loss: 127.2920\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 133.6995 - val_loss: 103.1204\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 130.2930 - val_loss: 103.5097\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 128.8063 - val_loss: 104.9854\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 121.9966 - val_loss: 105.5620\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 117.4477 - val_loss: 106.5922\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 117.0307 - val_loss: 107.0032\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 115.7173 - val_loss: 106.1132\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 113.6097 - val_loss: 104.9847\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 112.2941 - val_loss: 103.4578\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 111.0001 - val_loss: 102.2001\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 109.6754 - val_loss: 100.2554\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 108.4672 - val_loss: 98.2906\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 107.3606 - val_loss: 96.5322\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 106.0926 - val_loss: 95.1939\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 105.3783 - val_loss: 94.0556\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.9612 - val_loss: 93.1531\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.0786 - val_loss: 92.1764\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.8544 - val_loss: 91.2895\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 100.8130 - val_loss: 90.4761\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.8502 - val_loss: 89.4023\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 98.9983 - val_loss: 88.0476\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 98.0721 - val_loss: 87.1287\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 97.1362 - val_loss: 86.0236\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 96.4964 - val_loss: 84.6840\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 95.6509 - val_loss: 83.6329\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.8148 - val_loss: 83.1791\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 94.5205 - val_loss: 82.5243\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 93.3021 - val_loss: 81.6436\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 92.7699 - val_loss: 81.4879\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.0894 - val_loss: 80.3095\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.1349 - val_loss: 79.4381\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 90.6574 - val_loss: 78.6729\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.8194 - val_loss: 78.4293\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.4032 - val_loss: 78.7266\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.0335 - val_loss: 78.2497\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.3213 - val_loss: 77.1966\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.9229 - val_loss: 76.6853\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.4133 - val_loss: 76.4774\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 86.9962 - val_loss: 76.1318\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.4592 - val_loss: 76.1395\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.1166 - val_loss: 76.0563\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.8076 - val_loss: 75.9198\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.3744 - val_loss: 75.5476\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.4684 - val_loss: 74.9162\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.6375 - val_loss: 74.3694\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.9137 - val_loss: 74.1616\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.7813 - val_loss: 73.6417\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.8385 - val_loss: 73.6729\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.4504 - val_loss: 73.6768\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.2173 - val_loss: 73.3281\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.9710 - val_loss: 73.1720\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.7938 - val_loss: 73.3472\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.7705 - val_loss: 72.9758\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 82.3709 - val_loss: 72.8210\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 82.0358 - val_loss: 72.7042\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.8561 - val_loss: 72.6428\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.7459 - val_loss: 72.4908\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 81.9422 - val_loss: 72.9496\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.9888 - val_loss: 72.3142\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.2576 - val_loss: 71.7329\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 81.2033 - val_loss: 71.5430\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.1265 - val_loss: 71.9679\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.0428 - val_loss: 71.4256\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.9208 - val_loss: 71.6169\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.6533 - val_loss: 72.1606\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.6343 - val_loss: 71.5819\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.6334 - val_loss: 72.0458\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.3788 - val_loss: 72.1740\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 80.2241 - val_loss: 72.0233\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 80.2719 - val_loss: 71.4743\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 80.1371 - val_loss: 71.8411\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 80.0855 - val_loss: 71.7602\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.1396 - val_loss: 71.8837\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 84.3510\n",
            "84.35102844238281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "IzFSgPqXwrgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2015 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2015)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gup1J82wrgY",
        "outputId": "5e30fd41-51e7-4ade-d4c9-3c213ffdfa98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 37ms/step - loss: 595.8918 - val_loss: 599.5577\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 595.5134 - val_loss: 599.1786\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 595.0712 - val_loss: 598.6981\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 594.4931 - val_loss: 598.0301\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 593.6555 - val_loss: 597.0474\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 592.4557 - val_loss: 595.6097\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 590.7053 - val_loss: 593.4893\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 588.1790 - val_loss: 590.5106\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 584.6476 - val_loss: 586.3602\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 579.7302 - val_loss: 580.6290\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 573.0983 - val_loss: 572.8519\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 564.0289 - val_loss: 562.5505\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 551.8741 - val_loss: 549.0279\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 536.4391 - val_loss: 531.5483\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 516.4117 - val_loss: 509.2638\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 491.0570 - val_loss: 481.0587\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 459.3089 - val_loss: 446.0772\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 419.9421 - val_loss: 403.0406\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 372.5120 - val_loss: 350.8592\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 317.9380 - val_loss: 288.6178\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 255.7279 - val_loss: 217.2294\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 199.6240 - val_loss: 150.6781\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 159.6402 - val_loss: 114.8793\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 142.8152 - val_loss: 107.9269\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 139.2214 - val_loss: 111.1938\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 133.0704 - val_loss: 109.0724\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 126.2835 - val_loss: 106.5673\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 123.6281 - val_loss: 105.8440\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 121.3762 - val_loss: 104.9600\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 119.0865 - val_loss: 105.2808\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 116.4902 - val_loss: 105.0440\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 114.5828 - val_loss: 103.5220\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 112.7390 - val_loss: 101.4056\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 111.0742 - val_loss: 98.2928\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 109.7089 - val_loss: 96.5101\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 107.8635 - val_loss: 95.4461\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 106.5816 - val_loss: 95.4478\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 105.2858 - val_loss: 94.2752\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.9320 - val_loss: 93.8495\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 102.5336 - val_loss: 92.4160\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 101.2781 - val_loss: 91.7808\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 100.3097 - val_loss: 91.2340\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 99.4498 - val_loss: 90.0787\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 98.2054 - val_loss: 88.3437\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 97.0945 - val_loss: 86.6020\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.9718 - val_loss: 84.6129\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 94.8290 - val_loss: 83.3296\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 93.8914 - val_loss: 82.0802\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 93.2353 - val_loss: 80.6210\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 92.0179 - val_loss: 80.4726\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 91.3213 - val_loss: 80.3990\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.5600 - val_loss: 79.2019\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.8232 - val_loss: 78.6273\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.1055 - val_loss: 78.4721\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.5325 - val_loss: 77.0163\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 87.9808 - val_loss: 76.3449\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.2622 - val_loss: 76.2643\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.9454 - val_loss: 76.3520\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 86.2461 - val_loss: 75.7019\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.8210 - val_loss: 75.6571\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.3221 - val_loss: 75.7365\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.0244 - val_loss: 75.5620\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.3809 - val_loss: 75.3369\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 83.9904 - val_loss: 75.3237\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.6758 - val_loss: 75.0090\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.1207 - val_loss: 74.8798\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.1041 - val_loss: 74.6441\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 82.1601 - val_loss: 74.0538\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 82.2976 - val_loss: 73.6544\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.9602 - val_loss: 73.8091\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.4711 - val_loss: 74.2009\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 81.3510 - val_loss: 74.3975\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 81.0837 - val_loss: 74.5700\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 81.3181 - val_loss: 74.1722\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.9221 - val_loss: 74.2358\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.5353 - val_loss: 73.1658\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.5006 - val_loss: 72.1984\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.7417 - val_loss: 72.5981\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.3533 - val_loss: 73.7274\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 80.0054 - val_loss: 74.5566\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.8438 - val_loss: 74.2543\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.7244 - val_loss: 73.9637\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.4521 - val_loss: 74.1056\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.5909 - val_loss: 74.2168\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.5654 - val_loss: 73.7359\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 79.4826 - val_loss: 74.1329\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.0791 - val_loss: 74.4179\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 83.2670\n",
            "83.26700592041016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "1P34lEFSwrgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2015 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2015)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85972972-ec84-4166-d70b-ee1185dbeaee",
        "id": "AI9XDTnbwrgY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 2s 60ms/step - loss: 596.0286 - val_loss: 599.6813\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 595.6890 - val_loss: 599.3556\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 595.3361 - val_loss: 598.9603\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 594.8764 - val_loss: 598.4205\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 594.2293 - val_loss: 597.6653\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 593.3428 - val_loss: 596.5906\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 592.0689 - val_loss: 595.0419\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 590.2217 - val_loss: 592.8117\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 587.5318 - val_loss: 589.6477\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 583.8516 - val_loss: 585.2280\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 578.6229 - val_loss: 579.1691\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 571.6147 - val_loss: 570.9726\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 562.0054 - val_loss: 560.0811\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 549.6796 - val_loss: 545.8220\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 533.1001 - val_loss: 527.5147\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 512.3739 - val_loss: 504.2814\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 485.7706 - val_loss: 475.0373\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 452.5483 - val_loss: 439.0896\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 413.1087 - val_loss: 395.2475\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 366.4081 - val_loss: 342.2358\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 313.5964 - val_loss: 281.2463\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 262.5939 - val_loss: 217.5213\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 217.1049 - val_loss: 152.1300\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 179.7093 - val_loss: 125.9791\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 161.0414 - val_loss: 127.7709\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 158.8647 - val_loss: 133.1043\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 152.8693 - val_loss: 132.3584\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 147.4095 - val_loss: 129.7984\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 142.5824 - val_loss: 127.1059\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 139.2243 - val_loss: 123.7981\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 136.0498 - val_loss: 120.1396\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 133.0457 - val_loss: 117.0889\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 130.4568 - val_loss: 114.2247\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 127.8252 - val_loss: 112.9150\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 125.4783 - val_loss: 111.5242\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 123.3255 - val_loss: 110.1384\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 121.5510 - val_loss: 109.7713\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 119.8064 - val_loss: 108.4304\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 118.1661 - val_loss: 106.7096\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 116.3932 - val_loss: 103.8951\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 114.4382 - val_loss: 102.2354\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 112.9050 - val_loss: 100.6267\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 111.3568 - val_loss: 99.1236\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 109.9986 - val_loss: 96.9427\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 108.7033 - val_loss: 97.0805\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 107.0432 - val_loss: 95.3044\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 105.8707 - val_loss: 94.5100\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 104.4393 - val_loss: 92.2768\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.0186 - val_loss: 91.2532\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 101.7989 - val_loss: 90.4370\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 100.8139 - val_loss: 89.2607\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.3711 - val_loss: 88.7206\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 98.1902 - val_loss: 87.6077\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 97.1927 - val_loss: 87.8546\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 96.1796 - val_loss: 86.0305\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.9016 - val_loss: 84.0319\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 93.9476 - val_loss: 83.4389\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 93.1303 - val_loss: 83.1857\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.0400 - val_loss: 81.9995\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 91.3843 - val_loss: 80.3036\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 90.6379 - val_loss: 79.0538\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 89.7416 - val_loss: 79.5270\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 89.0271 - val_loss: 78.6552\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 88.6282 - val_loss: 77.5191\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 87.5134 - val_loss: 78.1315\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 87.9892 - val_loss: 77.8422\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 86.8483 - val_loss: 77.1976\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.5547 - val_loss: 76.3319\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 86.0662 - val_loss: 76.4626\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.1997 - val_loss: 76.3009\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.3088 - val_loss: 75.6780\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.9815 - val_loss: 74.7250\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.6841 - val_loss: 74.5985\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 84.2935 - val_loss: 74.3844\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 83.9788 - val_loss: 74.1708\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.7115 - val_loss: 74.1900\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.4227 - val_loss: 74.3635\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.2761 - val_loss: 74.9073\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.2994 - val_loss: 73.8328\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.2150 - val_loss: 74.4603\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 82.6865 - val_loss: 74.2755\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 82.7188 - val_loss: 75.6965\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 82.5877 - val_loss: 74.5000\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 81.8856 - val_loss: 74.1600\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.2545 - val_loss: 73.4892\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 81.8245 - val_loss: 73.1209\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.6763 - val_loss: 73.7206\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.3748 - val_loss: 73.8886\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.4118 - val_loss: 74.8261\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.1800 - val_loss: 74.2179\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.0199 - val_loss: 73.6094\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 81.0965 - val_loss: 73.6474\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.6120 - val_loss: 74.4588\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.9569 - val_loss: 74.8942\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.5748 - val_loss: 74.0498\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.5070 - val_loss: 73.3426\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 83.0324\n",
            "83.0324478149414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2016"
      ],
      "metadata": {
        "id": "uQBKCdmE0ghZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "RiROkjF9wxX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2016\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2016 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2016)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ea851e-f34b-494c-f0ac-37a9342a1735",
        "id": "LD6n23-NwxX4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 58ms/step - loss: 628.7567 - val_loss: 619.8478\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 628.2905 - val_loss: 619.3873\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 627.7943 - val_loss: 618.8364\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 627.1697 - val_loss: 618.1133\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 626.3196 - val_loss: 617.1015\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 625.1337 - val_loss: 615.7007\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 623.5093 - val_loss: 613.7540\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 621.2144 - val_loss: 611.0546\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 618.0712 - val_loss: 607.3110\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 613.7142 - val_loss: 602.2293\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 607.8477 - val_loss: 595.4136\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 600.0001 - val_loss: 586.4464\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 589.7933 - val_loss: 574.8423\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 576.5615 - val_loss: 559.9666\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 559.6301 - val_loss: 541.2324\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 538.6089 - val_loss: 517.7136\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 512.5163 - val_loss: 488.7029\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 480.1648 - val_loss: 453.3234\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 440.4738 - val_loss: 410.6694\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 394.4205 - val_loss: 359.5427\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 339.5714 - val_loss: 299.4666\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 276.1903 - val_loss: 234.3260\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 206.9402 - val_loss: 171.5936\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 150.4747 - val_loss: 140.0243\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 127.0211 - val_loss: 135.1409\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 123.8478 - val_loss: 137.4861\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 122.8875 - val_loss: 134.0319\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 118.2452 - val_loss: 131.2311\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 115.5272 - val_loss: 129.3622\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 113.9419 - val_loss: 128.0393\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 112.5925 - val_loss: 127.3359\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 111.0724 - val_loss: 126.4643\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 109.8863 - val_loss: 125.1351\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 108.4414 - val_loss: 123.5667\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 107.2528 - val_loss: 122.0804\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 106.0356 - val_loss: 120.7493\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 104.9474 - val_loss: 119.5294\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 103.6192 - val_loss: 118.8143\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 102.8562 - val_loss: 117.6751\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 101.6462 - val_loss: 116.4782\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 100.6616 - val_loss: 115.7012\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.2445 - val_loss: 114.4039\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 98.3402 - val_loss: 112.9038\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 97.3884 - val_loss: 112.2122\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 96.2375 - val_loss: 112.0595\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 96.1817 - val_loss: 111.3483\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.1047 - val_loss: 109.0268\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 93.8249 - val_loss: 108.7046\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 92.5034 - val_loss: 108.7794\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 91.9264 - val_loss: 107.9896\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 91.0532 - val_loss: 106.3001\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.0695 - val_loss: 105.6272\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.5101 - val_loss: 104.4949\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 88.3127 - val_loss: 105.0029\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 88.0064 - val_loss: 104.2103\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.9573 - val_loss: 102.6772\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.7145 - val_loss: 101.3565\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.3329 - val_loss: 102.1112\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.2389 - val_loss: 101.3740\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 84.4269 - val_loss: 99.9761\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 84.0024 - val_loss: 99.0658\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.1927 - val_loss: 99.7564\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.6012 - val_loss: 99.5698\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.8620 - val_loss: 97.9361\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 82.1389 - val_loss: 97.8752\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.8793 - val_loss: 97.5723\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 81.5455 - val_loss: 96.7309\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 81.0465 - val_loss: 96.7652\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.8466 - val_loss: 96.8470\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.5229 - val_loss: 96.2923\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.3171 - val_loss: 95.3269\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.1044 - val_loss: 96.0681\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.8063 - val_loss: 95.1697\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 79.9251 - val_loss: 94.0663\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.2598 - val_loss: 94.9348\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.0910 - val_loss: 94.7080\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.6420 - val_loss: 93.8209\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.6281 - val_loss: 93.6345\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.3471 - val_loss: 94.2022\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.4074 - val_loss: 93.7270\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.1963 - val_loss: 92.5220\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.1639 - val_loss: 93.0428\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 77.6934 - val_loss: 93.7787\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.6748 - val_loss: 92.1007\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.6949 - val_loss: 92.6602\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.6279 - val_loss: 93.1402\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.1900 - val_loss: 91.7421\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.2675 - val_loss: 90.9852\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 77.3887 - val_loss: 91.9765\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.8867 - val_loss: 91.8232\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 76.6617 - val_loss: 92.1950\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.4669 - val_loss: 92.0131\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 76.6127 - val_loss: 92.2691\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.2530 - val_loss: 91.0751\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.3289 - val_loss: 90.9305\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.4377 - val_loss: 92.2558\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.1199 - val_loss: 90.2552\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.9759 - val_loss: 90.5424\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.8082 - val_loss: 90.7177\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.6587 - val_loss: 90.2886\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.8349 - val_loss: 90.4284\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.9354 - val_loss: 92.3610\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.0834 - val_loss: 90.9892\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.4194 - val_loss: 89.5587\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.7698 - val_loss: 89.7528\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 75.5676 - val_loss: 89.4102\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.2355 - val_loss: 90.0297\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.1922 - val_loss: 90.6767\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.2004 - val_loss: 89.9524\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.9945 - val_loss: 90.3745\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.0034 - val_loss: 90.8115\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.0579 - val_loss: 89.5063\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 74.9750 - val_loss: 89.4070\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.2429 - val_loss: 90.7482\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 74.7850 - val_loss: 89.2009\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 74.9225 - val_loss: 88.9081\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 74.5377 - val_loss: 89.7374\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.8350 - val_loss: 90.3515\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.6349 - val_loss: 90.1956\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 74.4000 - val_loss: 89.6969\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.5421 - val_loss: 89.3363\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 74.8167 - val_loss: 88.9164\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.2740 - val_loss: 89.0702\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.3117 - val_loss: 89.0624\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 74.2955 - val_loss: 89.5877\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.2084 - val_loss: 89.4978\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 80.7184\n",
            "80.71843719482422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "ApMAW1V5wxX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2016 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2016)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLIX4LppwxX5",
        "outputId": "28444c3d-7b9c-43c0-9cd0-9944db3e99ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 38ms/step - loss: 628.7721 - val_loss: 619.9274\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 628.3926 - val_loss: 619.5414\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 627.9645 - val_loss: 619.0496\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 627.3806 - val_loss: 618.3630\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 626.5627 - val_loss: 617.3695\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 625.3637 - val_loss: 615.9443\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 623.6544 - val_loss: 613.9126\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 621.2194 - val_loss: 611.0048\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 617.7256 - val_loss: 606.8641\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 612.8189 - val_loss: 601.1004\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 605.9990 - val_loss: 593.2540\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 596.9581 - val_loss: 582.7849\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 584.7224 - val_loss: 569.0773\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 568.9960 - val_loss: 551.3392\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 548.9902 - val_loss: 528.7110\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 523.4202 - val_loss: 500.3680\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 491.7697 - val_loss: 465.3363\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 453.4872 - val_loss: 423.0963\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 408.2309 - val_loss: 371.8478\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 354.3726 - val_loss: 312.1264\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 290.2338 - val_loss: 247.6773\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 223.4974 - val_loss: 188.0921\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 166.5551 - val_loss: 156.1959\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 134.5199 - val_loss: 152.2571\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 131.6401 - val_loss: 151.4553\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 129.3037 - val_loss: 145.1806\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 121.7383 - val_loss: 139.0717\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 119.9496 - val_loss: 136.2022\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 119.7570 - val_loss: 134.6300\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 118.0629 - val_loss: 134.1253\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 115.5599 - val_loss: 133.3802\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 113.8965 - val_loss: 131.9552\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 112.1382 - val_loss: 130.0275\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 110.8856 - val_loss: 128.3644\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 109.2415 - val_loss: 126.3738\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 107.7271 - val_loss: 124.3902\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 106.6058 - val_loss: 123.3711\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 105.4421 - val_loss: 122.2246\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 104.1678 - val_loss: 121.2709\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 102.9320 - val_loss: 119.9274\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 101.6198 - val_loss: 117.9974\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 100.4379 - val_loss: 117.3954\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 99.2875 - val_loss: 116.6807\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 98.2671 - val_loss: 115.2924\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 96.9316 - val_loss: 113.8674\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 96.4764 - val_loss: 112.5026\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.8210 - val_loss: 111.9104\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 94.6626 - val_loss: 111.4532\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 93.5381 - val_loss: 109.7097\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 92.2176 - val_loss: 108.7142\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 91.8885 - val_loss: 107.5851\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 90.9048 - val_loss: 107.3064\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 90.3186 - val_loss: 107.0481\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 89.6342 - val_loss: 106.1308\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 88.8423 - val_loss: 104.5639\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 88.1671 - val_loss: 103.5230\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 87.6606 - val_loss: 103.0801\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 87.0477 - val_loss: 102.4333\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 86.7623 - val_loss: 101.5528\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 86.1585 - val_loss: 101.1462\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 85.7562 - val_loss: 100.0034\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.0270 - val_loss: 99.9970\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.7731 - val_loss: 100.1053\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.2522 - val_loss: 99.0605\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 83.9865 - val_loss: 97.7898\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.4791 - val_loss: 97.5536\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 82.9553 - val_loss: 97.0635\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 82.4482 - val_loss: 96.7359\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 82.2496 - val_loss: 96.2499\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 82.0565 - val_loss: 96.8838\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.6213 - val_loss: 95.1049\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 81.2885 - val_loss: 93.3719\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 80.6974 - val_loss: 93.6105\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 80.5622 - val_loss: 94.2931\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.6186 - val_loss: 92.0146\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.1612 - val_loss: 91.9952\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.6018 - val_loss: 92.6029\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 79.8175 - val_loss: 92.9368\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 80.0847 - val_loss: 90.7613\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.7820 - val_loss: 92.2491\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.2073 - val_loss: 92.3976\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.9744 - val_loss: 90.6668\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.5166 - val_loss: 90.5963\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.3349 - val_loss: 91.4519\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.1357 - val_loss: 90.6657\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.5322 - val_loss: 89.7647\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.5232 - val_loss: 90.9239\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.8238 - val_loss: 90.2766\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 77.3354 - val_loss: 89.9014\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.3922 - val_loss: 89.3823\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 77.2506 - val_loss: 89.8063\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 77.5381 - val_loss: 88.8804\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.2708 - val_loss: 90.1237\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 77.3173 - val_loss: 89.4151\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 76.8247 - val_loss: 89.5505\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.3331 - val_loss: 89.4755\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.8478 - val_loss: 88.1218\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.7272 - val_loss: 88.8395\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.4745 - val_loss: 88.9859\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.2919 - val_loss: 88.8038\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.4171 - val_loss: 88.7804\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.5655 - val_loss: 88.8463\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.2231 - val_loss: 88.2935\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.1757 - val_loss: 88.3345\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.2967 - val_loss: 88.0012\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.2672 - val_loss: 88.6040\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.9878 - val_loss: 88.0620\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.2142 - val_loss: 88.0412\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 75.9424 - val_loss: 88.4166\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 76.0170 - val_loss: 87.9156\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 76.3403 - val_loss: 88.4899\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.9637 - val_loss: 87.8980\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 75.9788 - val_loss: 88.0584\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.9773 - val_loss: 88.6115\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.2180 - val_loss: 87.1602\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.2300 - val_loss: 87.1512\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 75.6058 - val_loss: 89.0706\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.2868 - val_loss: 88.1330\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 76.2405 - val_loss: 87.5505\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.0357 - val_loss: 88.5204\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.0903 - val_loss: 87.3482\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.7670 - val_loss: 86.9929\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.7819 - val_loss: 87.9758\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.1567 - val_loss: 87.7441\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 75.7671 - val_loss: 88.5297\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.7306 - val_loss: 87.1777\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.2440 - val_loss: 87.4949\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.6331 - val_loss: 87.6110\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.6068 - val_loss: 87.8243\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.6465 - val_loss: 86.9761\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.5768 - val_loss: 87.0118\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.6932 - val_loss: 87.5544\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.4092 - val_loss: 87.8411\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.6080 - val_loss: 87.3320\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.5242 - val_loss: 87.1658\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.3496 - val_loss: 87.6885\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.3805 - val_loss: 87.7634\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.4565 - val_loss: 87.3726\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.3398 - val_loss: 87.8255\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.6924 - val_loss: 87.2310\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 80.1691\n",
            "80.16911315917969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "GnFt9HtLwxX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2016 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2016)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6577af7c-0cd1-45f0-ba97-d54bd017d165",
        "id": "y_r6oLLYwxX5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 41ms/step - loss: 628.6396 - val_loss: 619.7618\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 628.2066 - val_loss: 619.2933\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 627.6760 - val_loss: 618.6780\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 626.9600 - val_loss: 617.8401\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 625.9876 - val_loss: 616.6687\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 624.5919 - val_loss: 614.9785\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 622.6127 - val_loss: 612.5400\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 619.7043 - val_loss: 609.0078\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 615.4998 - val_loss: 604.0231\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 609.6675 - val_loss: 597.1298\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 601.5878 - val_loss: 587.8296\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 591.0228 - val_loss: 575.4500\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 576.9261 - val_loss: 559.2393\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 558.4680 - val_loss: 538.4380\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 535.1089 - val_loss: 511.9871\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 505.5288 - val_loss: 479.1854\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 469.8653 - val_loss: 438.8906\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 427.3911 - val_loss: 389.8484\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 375.9483 - val_loss: 333.3509\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 316.7208 - val_loss: 272.4124\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 246.6953 - val_loss: 220.1226\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 194.2971 - val_loss: 191.1505\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 169.6059 - val_loss: 184.9939\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 167.1330 - val_loss: 185.4924\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 165.0790 - val_loss: 181.9530\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 159.3909 - val_loss: 174.2576\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 153.1088 - val_loss: 167.3837\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 148.4949 - val_loss: 162.4151\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 144.3365 - val_loss: 159.0681\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 140.3950 - val_loss: 156.0756\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 137.5060 - val_loss: 153.5560\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 133.9759 - val_loss: 151.1234\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 131.0479 - val_loss: 148.2908\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 128.1266 - val_loss: 145.7776\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 125.4041 - val_loss: 143.1404\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 123.2011 - val_loss: 140.2042\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 120.8706 - val_loss: 138.2443\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 118.4244 - val_loss: 136.1691\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 116.2712 - val_loss: 133.3899\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 114.0026 - val_loss: 131.1640\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 111.8155 - val_loss: 128.9309\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 109.8895 - val_loss: 127.2554\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 107.9476 - val_loss: 125.1295\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 106.0567 - val_loss: 123.4583\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 104.1291 - val_loss: 121.5673\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 102.2318 - val_loss: 119.5881\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 100.5443 - val_loss: 118.6369\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.2204 - val_loss: 117.2767\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 97.6179 - val_loss: 115.0508\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.5237 - val_loss: 113.8518\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 94.9754 - val_loss: 113.6308\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 93.8647 - val_loss: 111.9217\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 92.5926 - val_loss: 109.8974\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 91.4409 - val_loss: 108.9429\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 90.3587 - val_loss: 107.9634\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.7852 - val_loss: 106.5619\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.8840 - val_loss: 104.4139\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 88.0543 - val_loss: 104.1259\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 87.0630 - val_loss: 103.6856\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 86.2956 - val_loss: 102.7213\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.7489 - val_loss: 101.4811\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.0186 - val_loss: 101.1002\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.4105 - val_loss: 100.0844\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 83.8630 - val_loss: 99.5004\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.4631 - val_loss: 98.9315\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 83.0643 - val_loss: 98.1133\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.7787 - val_loss: 97.7564\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.2336 - val_loss: 96.0295\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 81.8125 - val_loss: 94.9183\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.8525 - val_loss: 96.0610\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 81.4707 - val_loss: 95.3828\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.7554 - val_loss: 94.4612\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.4215 - val_loss: 94.0858\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 80.2155 - val_loss: 93.7573\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 79.9797 - val_loss: 93.2935\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 79.7384 - val_loss: 92.6908\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 79.5727 - val_loss: 92.6040\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 79.6324 - val_loss: 93.1097\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 79.3234 - val_loss: 91.9104\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 79.2537 - val_loss: 91.5543\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 78.7772 - val_loss: 92.1738\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 78.5568 - val_loss: 91.5306\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 78.2485 - val_loss: 91.9252\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.1427 - val_loss: 91.6937\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 77.9436 - val_loss: 91.5541\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 77.7395 - val_loss: 91.0978\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 77.4596 - val_loss: 90.6671\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 77.4554 - val_loss: 90.6057\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 77.3198 - val_loss: 90.9526\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.4528 - val_loss: 91.5770\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.7251 - val_loss: 90.7841\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 77.1872 - val_loss: 89.8094\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 76.7609 - val_loss: 90.1434\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 76.9864 - val_loss: 90.2797\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 76.1916 - val_loss: 89.8348\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.0846 - val_loss: 89.9564\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 76.0745 - val_loss: 90.0256\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 76.0969 - val_loss: 89.3406\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 76.2122 - val_loss: 89.3358\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.6471 - val_loss: 89.5232\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 75.8663 - val_loss: 89.3238\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 75.6832 - val_loss: 88.6936\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 75.9120 - val_loss: 89.3727\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 75.6123 - val_loss: 89.2878\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 75.4715 - val_loss: 88.7913\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.5525 - val_loss: 88.3848\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 75.7382 - val_loss: 89.1868\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.6672 - val_loss: 88.6867\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.5467 - val_loss: 87.9023\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.2867 - val_loss: 88.6253\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.0967 - val_loss: 89.0184\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.1846 - val_loss: 88.6461\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 74.8899 - val_loss: 87.6886\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 74.9739 - val_loss: 88.0747\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.9122 - val_loss: 88.6067\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.8034 - val_loss: 88.6800\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 74.8870 - val_loss: 88.5629\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.2244 - val_loss: 88.1164\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.6447 - val_loss: 88.7397\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 74.8847 - val_loss: 88.3133\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.8211 - val_loss: 87.5227\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.3262 - val_loss: 87.8611\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.4109 - val_loss: 89.9369\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.9681 - val_loss: 88.0131\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.1715 - val_loss: 87.1709\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.6105 - val_loss: 88.2272\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.2323 - val_loss: 88.7379\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 74.9717 - val_loss: 87.1867\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.2915 - val_loss: 88.2790\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.5854 - val_loss: 88.7717\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.1824 - val_loss: 88.0401\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.4241 - val_loss: 87.8084\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.2376 - val_loss: 87.5525\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.1651 - val_loss: 87.5808\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.6672 - val_loss: 87.5112\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 80.9283\n",
            "80.92826080322266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2017"
      ],
      "metadata": {
        "id": "c_SyYl_W0i2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "_EX700Lkw31a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2017\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2017 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2017)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceef4f43-09b8-43dd-a87f-bb3245ec646b",
        "id": "AJFpcuLEw31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 38ms/step - loss: 631.7662 - val_loss: 626.4670\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 631.4796 - val_loss: 626.1520\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 631.1606 - val_loss: 625.7556\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 630.7340 - val_loss: 625.2089\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 630.1523 - val_loss: 624.4146\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 629.2870 - val_loss: 623.2535\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 627.9930 - val_loss: 621.5574\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 626.1197 - val_loss: 619.0544\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 623.3788 - val_loss: 615.4376\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 619.4279 - val_loss: 610.3317\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 613.9403 - val_loss: 603.2204\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 606.3494 - val_loss: 593.5853\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 596.0522 - val_loss: 580.7488\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 582.4655 - val_loss: 563.9729\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 564.8915 - val_loss: 542.3248\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 542.2109 - val_loss: 514.8751\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 513.7882 - val_loss: 480.3292\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 477.5117 - val_loss: 437.7444\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 433.3407 - val_loss: 385.5181\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 379.3684 - val_loss: 324.6722\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 315.2178 - val_loss: 254.1817\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 244.2846 - val_loss: 183.6349\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 175.1456 - val_loss: 121.1059\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 131.1475 - val_loss: 104.9582\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 124.5288 - val_loss: 117.3537\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 129.3011 - val_loss: 114.3737\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 123.9749 - val_loss: 104.6050\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 120.9832 - val_loss: 98.2245\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 119.7575 - val_loss: 99.3963\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 119.0219 - val_loss: 98.3461\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 117.9145 - val_loss: 97.1680\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 116.7569 - val_loss: 95.9368\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 116.1155 - val_loss: 95.2248\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 115.0587 - val_loss: 94.7532\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 114.0850 - val_loss: 93.6584\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 113.2568 - val_loss: 93.1004\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 112.5482 - val_loss: 92.5429\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 111.8688 - val_loss: 91.8167\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 111.1300 - val_loss: 90.7880\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 110.4473 - val_loss: 89.9861\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 109.7151 - val_loss: 89.2341\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 109.2851 - val_loss: 88.6156\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 108.6011 - val_loss: 88.3595\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 108.0361 - val_loss: 87.8033\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 107.4352 - val_loss: 86.8687\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 106.8845 - val_loss: 86.8441\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 106.2975 - val_loss: 86.9078\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 105.7504 - val_loss: 86.8922\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 105.2944 - val_loss: 86.3752\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 105.0211 - val_loss: 86.7159\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 103.9393 - val_loss: 85.9998\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 103.2832 - val_loss: 85.1549\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.0210 - val_loss: 85.3677\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 102.5635 - val_loss: 84.6615\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 102.0206 - val_loss: 85.8487\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.5513 - val_loss: 86.0164\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 101.2676 - val_loss: 85.5040\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 100.4040 - val_loss: 86.6335\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 100.0767 - val_loss: 86.6435\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.5049 - val_loss: 85.6112\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 99.3344 - val_loss: 84.9249\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 98.6835 - val_loss: 85.0798\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 98.3802 - val_loss: 86.1763\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 98.4261 - val_loss: 86.1441\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 94.9788\n",
            "94.97879791259766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "5JsTnY0Ow31b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2017 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2017)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vaO6_f6w31b",
        "outputId": "6eafa200-2abf-4aa8-cfca-f94498b6d5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 41ms/step - loss: 631.6672 - val_loss: 626.3650\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 631.3467 - val_loss: 626.0325\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 631.0002 - val_loss: 625.6143\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 630.5494 - val_loss: 625.0610\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 629.9421 - val_loss: 624.2820\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 629.0856 - val_loss: 623.1799\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 627.8690 - val_loss: 621.6122\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 626.1468 - val_loss: 619.4051\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 623.7421 - val_loss: 616.3278\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 620.3217 - val_loss: 612.1284\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 615.7605 - val_loss: 606.4433\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 609.6407 - val_loss: 598.8408\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 601.3728 - val_loss: 588.8626\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 590.7257 - val_loss: 575.9189\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 576.8179 - val_loss: 559.4421\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 559.3840 - val_loss: 538.6804\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 537.4150 - val_loss: 512.9586\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 510.4560 - val_loss: 481.2970\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 477.2266 - val_loss: 442.7975\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 436.6903 - val_loss: 396.5023\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 388.6014 - val_loss: 341.4076\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 330.9395 - val_loss: 280.5695\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 268.0929 - val_loss: 216.6712\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 206.6026 - val_loss: 158.8849\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 161.0896 - val_loss: 116.7569\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 136.1015 - val_loss: 101.0829\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 128.3582 - val_loss: 102.0537\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 125.5627 - val_loss: 98.3737\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 121.8122 - val_loss: 92.9821\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 118.7616 - val_loss: 92.5905\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 117.4032 - val_loss: 92.5916\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 116.8344 - val_loss: 92.6424\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 115.3402 - val_loss: 90.2237\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 114.2884 - val_loss: 88.5689\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 113.4410 - val_loss: 87.8605\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 112.5355 - val_loss: 86.7786\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 111.4699 - val_loss: 86.0089\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 110.5193 - val_loss: 85.7980\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 110.0089 - val_loss: 85.0002\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 108.7473 - val_loss: 85.3476\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 107.8950 - val_loss: 85.5002\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 107.2421 - val_loss: 85.2030\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 106.3221 - val_loss: 84.7877\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 105.5942 - val_loss: 84.2097\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 104.8979 - val_loss: 83.4967\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 104.4433 - val_loss: 83.2923\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 103.8507 - val_loss: 82.7380\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 103.3650 - val_loss: 82.7855\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 102.5309 - val_loss: 83.0430\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 102.0673 - val_loss: 82.8766\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 101.3976 - val_loss: 82.2982\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 100.7581 - val_loss: 82.2971\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 100.3639 - val_loss: 82.3181\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.9294 - val_loss: 82.3321\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.6412 - val_loss: 82.5100\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 98.9490 - val_loss: 82.6174\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 98.5801 - val_loss: 82.2121\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 98.1285 - val_loss: 81.9506\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 97.5839 - val_loss: 81.7977\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 97.1089 - val_loss: 81.7680\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 96.7018 - val_loss: 81.7977\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 96.3955 - val_loss: 81.6025\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.0694 - val_loss: 81.9944\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 95.8990 - val_loss: 82.2598\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 95.3466 - val_loss: 82.5522\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 95.4670 - val_loss: 82.3446\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 94.8261 - val_loss: 82.5459\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.7512 - val_loss: 82.4129\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.4013 - val_loss: 82.2343\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 94.4434 - val_loss: 81.7253\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 93.9000 - val_loss: 81.8823\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 93.8043 - val_loss: 81.4655\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 93.5471 - val_loss: 81.8266\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 93.4024 - val_loss: 82.8275\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 93.6119 - val_loss: 83.2076\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 93.2081 - val_loss: 82.7494\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 92.9627 - val_loss: 82.4703\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.7432 - val_loss: 82.2178\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.3263 - val_loss: 82.0466\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 92.3539 - val_loss: 82.2835\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 92.3033 - val_loss: 81.5405\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 91.9129 - val_loss: 82.1848\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 83.2710\n",
            "83.27098083496094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "hRXF0upkw31b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2017 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2017)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27be6597-3428-451d-9c6d-bd18f7c64cbc",
        "id": "93mOBrM8w31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 43ms/step - loss: 631.8103 - val_loss: 626.5223\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 631.5361 - val_loss: 626.2269\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 631.2476 - val_loss: 625.8610\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 630.8644 - val_loss: 625.3659\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 630.3408 - val_loss: 624.6464\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 629.5547 - val_loss: 623.5719\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 628.4160 - val_loss: 621.9789\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 626.7010 - val_loss: 619.6406\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 624.2044 - val_loss: 616.2769\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 620.6294 - val_loss: 611.4829\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 615.5931 - val_loss: 604.7397\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 608.4341 - val_loss: 595.4138\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 598.9191 - val_loss: 582.7625\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 585.6705 - val_loss: 565.9227\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 568.5302 - val_loss: 543.7421\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 545.8391 - val_loss: 515.1216\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 516.7297 - val_loss: 478.7360\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 480.3917 - val_loss: 432.7698\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 434.1966 - val_loss: 376.0403\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 378.7784 - val_loss: 312.2170\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 314.3634 - val_loss: 241.7202\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 250.2108 - val_loss: 188.4458\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 191.2073 - val_loss: 151.9832\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 157.1764 - val_loss: 152.4938\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 154.7497 - val_loss: 156.5621\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 157.1120 - val_loss: 153.9944\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 151.6799 - val_loss: 139.3880\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 144.3202 - val_loss: 130.6110\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 140.4810 - val_loss: 123.7430\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 138.0663 - val_loss: 118.6496\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 135.3533 - val_loss: 115.8591\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 132.8978 - val_loss: 114.9533\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 130.5216 - val_loss: 113.2213\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 128.4978 - val_loss: 111.1729\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 126.5972 - val_loss: 108.7017\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 125.0392 - val_loss: 105.9799\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 123.6218 - val_loss: 105.1447\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 122.1817 - val_loss: 102.5633\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 121.0197 - val_loss: 101.7988\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 119.6528 - val_loss: 99.6737\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 118.3569 - val_loss: 97.2375\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 117.0434 - val_loss: 96.1997\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 115.8257 - val_loss: 93.9565\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 114.7524 - val_loss: 92.6431\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 113.5833 - val_loss: 90.7494\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 112.4143 - val_loss: 89.9991\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 111.4016 - val_loss: 88.7951\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 110.2696 - val_loss: 87.9075\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 109.3333 - val_loss: 86.8152\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 108.5335 - val_loss: 85.9117\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 107.5031 - val_loss: 85.3395\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 107.3289 - val_loss: 84.8334\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 106.1244 - val_loss: 83.6848\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 105.1616 - val_loss: 83.3708\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 104.7536 - val_loss: 82.7527\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 104.0138 - val_loss: 82.2162\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.2571 - val_loss: 82.0920\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 102.6307 - val_loss: 81.6110\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 102.2674 - val_loss: 81.1654\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.4147 - val_loss: 81.2241\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.0509 - val_loss: 80.7653\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 100.5722 - val_loss: 80.5523\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 99.7982 - val_loss: 80.7431\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.4724 - val_loss: 79.8592\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 98.8757 - val_loss: 80.3034\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 98.4258 - val_loss: 80.8745\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 98.0254 - val_loss: 81.2072\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 97.5799 - val_loss: 81.1739\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 96.9686 - val_loss: 80.6952\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 96.7114 - val_loss: 80.5849\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.4294 - val_loss: 81.1099\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 95.7743 - val_loss: 80.3586\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 95.4097 - val_loss: 80.4741\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 95.0962 - val_loss: 81.0609\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 90.9667\n",
            "90.96668243408203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2018"
      ],
      "metadata": {
        "id": "S9c7Bseq0lIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "jsRxONggw_67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2018\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2018 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2018)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e0504e-85ac-4756-c48f-8356c00f6632",
        "id": "lshzDhlvw_67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 43ms/step - loss: 634.4641 - val_loss: 636.1481\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 634.1873 - val_loss: 635.8307\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 633.8641 - val_loss: 635.4312\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 633.4492 - val_loss: 634.8895\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 632.8661 - val_loss: 634.1277\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 632.0513 - val_loss: 633.0471\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 630.9034 - val_loss: 631.5143\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 629.2639 - val_loss: 629.3315\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 626.9370 - val_loss: 626.2391\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 623.5828 - val_loss: 621.9583\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 619.1055 - val_loss: 616.0742\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 612.8237 - val_loss: 608.2160\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 604.6012 - val_loss: 597.7770\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 593.7180 - val_loss: 584.0853\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 579.3502 - val_loss: 566.5256\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 561.2946 - val_loss: 544.2495\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 538.3903 - val_loss: 516.3505\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 509.7339 - val_loss: 481.9282\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 474.8098 - val_loss: 439.6464\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 431.5431 - val_loss: 388.5854\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 379.1738 - val_loss: 328.2204\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 317.4779 - val_loss: 259.2352\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 248.5435 - val_loss: 183.0356\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 181.0561 - val_loss: 141.7505\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 143.4886 - val_loss: 142.9746\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 133.0003 - val_loss: 153.0124\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 136.0344 - val_loss: 153.7038\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 134.4187 - val_loss: 146.2948\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 129.2929 - val_loss: 133.3757\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 125.9193 - val_loss: 127.1553\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 125.1345 - val_loss: 124.7105\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 124.1066 - val_loss: 123.0753\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 122.7749 - val_loss: 122.9243\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 121.5559 - val_loss: 121.8363\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 120.2732 - val_loss: 120.5380\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 119.0351 - val_loss: 118.6532\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 117.8040 - val_loss: 116.4443\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 116.7667 - val_loss: 114.7628\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 115.9534 - val_loss: 113.0332\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 114.3994 - val_loss: 113.0556\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 113.1827 - val_loss: 111.6757\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 112.0943 - val_loss: 109.9220\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 110.7857 - val_loss: 109.3007\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 109.7131 - val_loss: 107.9705\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 108.4940 - val_loss: 106.6921\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 107.4985 - val_loss: 105.4449\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 106.4970 - val_loss: 103.4825\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 105.5301 - val_loss: 102.7102\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 104.4198 - val_loss: 100.1445\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 103.4964 - val_loss: 98.6544\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 102.3715 - val_loss: 97.5319\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 101.6960 - val_loss: 97.0754\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.4760 - val_loss: 95.1756\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.7323 - val_loss: 94.9558\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 99.0438 - val_loss: 94.3743\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 98.4145 - val_loss: 93.9519\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 97.7091 - val_loss: 91.8495\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 96.9107 - val_loss: 90.3690\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.2235 - val_loss: 89.7748\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.6835 - val_loss: 89.3637\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.1978 - val_loss: 89.2845\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 94.5936 - val_loss: 88.9110\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 94.3153 - val_loss: 87.2363\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 94.2525 - val_loss: 87.6428\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 93.0800 - val_loss: 86.7262\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 92.6691 - val_loss: 85.7775\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.3499 - val_loss: 86.3171\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 91.6599 - val_loss: 86.1143\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.5203 - val_loss: 86.6585\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 91.2965 - val_loss: 86.4581\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.7448 - val_loss: 85.0544\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.5232 - val_loss: 84.0382\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 90.0204 - val_loss: 84.1958\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.6296 - val_loss: 84.1245\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.3900 - val_loss: 84.1927\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.1554 - val_loss: 83.4787\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 88.9129 - val_loss: 84.1395\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 88.5604 - val_loss: 83.3667\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 88.0801 - val_loss: 83.6527\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 88.6454 - val_loss: 84.9504\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.8044 - val_loss: 82.6868\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.5483 - val_loss: 82.0930\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.4993 - val_loss: 83.0451\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.1894 - val_loss: 82.4612\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.1545 - val_loss: 82.4487\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.8674 - val_loss: 81.0824\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 86.6288 - val_loss: 80.6409\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.4999 - val_loss: 81.2216\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.6503 - val_loss: 81.6546\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.6615 - val_loss: 79.6203\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.2507 - val_loss: 80.6940\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.7716 - val_loss: 80.6634\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.8166 - val_loss: 80.5493\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.7181 - val_loss: 79.9851\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.6426 - val_loss: 80.0769\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.5175 - val_loss: 81.3860\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 85.2798 - val_loss: 81.3804\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 85.2279 - val_loss: 80.5971\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.1214 - val_loss: 80.2863\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.1514 - val_loss: 80.8356\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 87.5785\n",
            "87.57853698730469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "aLtSuMNgw_68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2018 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2018)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJGekATlw_68",
        "outputId": "bb7a1f0b-f6b5-4db3-bda4-6b6d20399211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 2s 42ms/step - loss: 634.5247 - val_loss: 636.1696\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 634.1943 - val_loss: 635.8130\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 633.8467 - val_loss: 635.3909\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 633.4031 - val_loss: 634.8208\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 632.7867 - val_loss: 634.0321\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 631.9432 - val_loss: 632.9344\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 630.7554 - val_loss: 631.4019\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 629.0942 - val_loss: 629.2473\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 626.7285 - val_loss: 626.2349\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 623.4697 - val_loss: 622.0831\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 618.9244 - val_loss: 616.4449\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 612.9202 - val_loss: 608.8739\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 604.7493 - val_loss: 598.9493\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 594.2487 - val_loss: 586.0477\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 580.5663 - val_loss: 569.5667\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 563.3448 - val_loss: 548.7758\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 541.2593 - val_loss: 522.9370\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 514.2911 - val_loss: 490.8145\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 480.6095 - val_loss: 451.5175\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 439.8144 - val_loss: 403.7669\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 390.3881 - val_loss: 346.3853\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 330.5588 - val_loss: 281.6785\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 262.7068 - val_loss: 211.2169\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 190.8237 - val_loss: 162.5018\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 146.5866 - val_loss: 149.6440\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 136.2176 - val_loss: 149.7025\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 135.9118 - val_loss: 146.7640\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 132.5293 - val_loss: 140.3711\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 129.4597 - val_loss: 132.8631\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 127.3502 - val_loss: 128.9359\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 125.9615 - val_loss: 127.6462\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 123.9882 - val_loss: 126.0245\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 122.3667 - val_loss: 124.8924\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 121.1246 - val_loss: 123.5049\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 119.9368 - val_loss: 121.2506\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 118.1943 - val_loss: 120.2040\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 117.0181 - val_loss: 119.5488\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 115.6846 - val_loss: 117.8374\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 114.4354 - val_loss: 114.9160\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 113.3981 - val_loss: 113.0572\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 112.1085 - val_loss: 110.8511\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 111.2505 - val_loss: 110.5636\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 109.5617 - val_loss: 108.1935\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 108.4823 - val_loss: 107.7101\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 107.2492 - val_loss: 106.2925\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 106.2078 - val_loss: 104.3669\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 105.2277 - val_loss: 102.7105\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 104.1452 - val_loss: 101.0101\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 103.1719 - val_loss: 99.4249\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 102.3447 - val_loss: 97.9944\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.4677 - val_loss: 96.9733\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 100.5945 - val_loss: 96.1147\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 99.8632 - val_loss: 95.1188\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.1983 - val_loss: 94.5203\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 98.3231 - val_loss: 94.1583\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 97.6574 - val_loss: 93.1595\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 97.1172 - val_loss: 91.9555\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 96.4309 - val_loss: 91.1128\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 95.8860 - val_loss: 89.6434\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.3506 - val_loss: 89.4348\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 94.5158 - val_loss: 88.9870\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 94.0852 - val_loss: 88.3270\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 93.4496 - val_loss: 86.8796\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 92.9627 - val_loss: 86.2154\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.5083 - val_loss: 85.5676\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 92.0630 - val_loss: 85.3824\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 91.6266 - val_loss: 84.9963\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 91.2674 - val_loss: 84.0066\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.6542 - val_loss: 82.9546\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 90.4609 - val_loss: 81.8983\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 90.2062 - val_loss: 82.2761\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.7396 - val_loss: 81.4455\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.5695 - val_loss: 80.6811\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 89.2201 - val_loss: 80.2598\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.6532 - val_loss: 79.7476\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 88.6907 - val_loss: 79.8903\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 88.5169 - val_loss: 79.1278\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 88.4804 - val_loss: 80.4268\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 87.9312 - val_loss: 80.6030\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 87.9284 - val_loss: 80.0063\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 87.8139 - val_loss: 80.3804\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.4540 - val_loss: 79.7892\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.1919 - val_loss: 80.2435\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 87.1284 - val_loss: 80.3783\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 86.9582 - val_loss: 80.3256\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.6434 - val_loss: 80.6855\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.1386 - val_loss: 78.8096\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 86.7417 - val_loss: 78.1712\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 86.6121 - val_loss: 79.5814\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.9318 - val_loss: 80.5764\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 86.7031 - val_loss: 79.0941\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 85.9333 - val_loss: 78.6539\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.8633 - val_loss: 79.1336\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.0064 - val_loss: 79.6448\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.8999 - val_loss: 79.5198\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.9242 - val_loss: 78.0257\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.5991 - val_loss: 77.8743\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.6539 - val_loss: 78.6244\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.5505 - val_loss: 78.8055\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 85.3529 - val_loss: 78.0448\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.5558 - val_loss: 78.1860\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.1519 - val_loss: 77.0977\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.3752 - val_loss: 77.2297\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.1845 - val_loss: 78.6672\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.0930 - val_loss: 80.4775\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.1155 - val_loss: 79.9017\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.8147 - val_loss: 78.3996\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 84.8227 - val_loss: 77.1162\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.7766 - val_loss: 77.2832\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 84.9072 - val_loss: 77.5669\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.0487 - val_loss: 77.8965\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.4756 - val_loss: 78.2624\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 85.5038\n",
            "85.50379180908203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "L5U3K8oIw_68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2018 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2018)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9588bee9-b3ec-4375-ee73-02690859ce8b",
        "id": "Q54Br87uw_68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 39ms/step - loss: 634.4171 - val_loss: 635.9717\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 633.9418 - val_loss: 635.4472\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 633.3925 - val_loss: 634.7659\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 632.6450 - val_loss: 633.7902\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 631.5639 - val_loss: 632.3706\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 629.9955 - val_loss: 630.3165\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 627.7177 - val_loss: 627.4024\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 624.5395 - val_loss: 623.2989\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 620.0956 - val_loss: 617.6321\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 613.8967 - val_loss: 609.9146\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 605.5384 - val_loss: 599.5375\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 594.4374 - val_loss: 585.7913\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 579.7410 - val_loss: 567.8394\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 560.7017 - val_loss: 544.6482\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 536.1310 - val_loss: 515.1507\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 504.7618 - val_loss: 478.1535\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 466.2763 - val_loss: 432.2472\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 419.7217 - val_loss: 376.3759\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 364.5364 - val_loss: 313.0206\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 302.5933 - val_loss: 245.1920\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 234.2137 - val_loss: 187.3918\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 183.7831 - val_loss: 162.6002\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 168.9431 - val_loss: 161.4913\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 166.7782 - val_loss: 163.4528\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 164.9368 - val_loss: 160.4256\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 158.6068 - val_loss: 154.1176\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 153.1878 - val_loss: 150.9807\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 150.2729 - val_loss: 148.4152\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 147.5312 - val_loss: 145.7295\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 145.0762 - val_loss: 142.0674\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 142.6697 - val_loss: 138.7351\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 140.3990 - val_loss: 135.4446\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 138.0986 - val_loss: 133.2105\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 136.0698 - val_loss: 133.1152\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 134.7102 - val_loss: 132.5900\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 132.9065 - val_loss: 129.6552\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 131.4245 - val_loss: 127.4324\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 129.6168 - val_loss: 126.8620\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 127.9036 - val_loss: 126.6650\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 126.6817 - val_loss: 126.7921\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 125.4225 - val_loss: 126.1053\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 124.1124 - val_loss: 125.9537\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 122.3097 - val_loss: 123.8015\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 120.9714 - val_loss: 122.3457\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 119.5665 - val_loss: 121.3131\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 118.3163 - val_loss: 120.3947\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 116.9835 - val_loss: 118.5366\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 115.5331 - val_loss: 116.9508\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 114.5565 - val_loss: 116.8640\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 112.9948 - val_loss: 114.9379\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 111.9039 - val_loss: 112.6272\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 110.2761 - val_loss: 110.7846\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 109.3860 - val_loss: 109.7437\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 107.9379 - val_loss: 107.8134\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 106.9266 - val_loss: 104.9850\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 105.4392 - val_loss: 104.7692\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 104.6610 - val_loss: 104.3375\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 103.5397 - val_loss: 102.7717\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 102.1646 - val_loss: 101.4875\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 101.4363 - val_loss: 102.3888\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 100.4538 - val_loss: 99.7313\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 99.3785 - val_loss: 96.8669\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 98.7193 - val_loss: 95.5470\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 97.4183 - val_loss: 94.3965\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.5383 - val_loss: 94.3107\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 95.8345 - val_loss: 93.8046\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 95.2866 - val_loss: 93.1660\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 94.4766 - val_loss: 92.1690\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 93.5019 - val_loss: 93.3149\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 93.2350 - val_loss: 92.5644\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 92.5762 - val_loss: 90.1164\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.9546 - val_loss: 89.3209\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.3003 - val_loss: 89.8181\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.2677 - val_loss: 89.6009\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 90.4752 - val_loss: 88.0354\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 90.3503 - val_loss: 85.9358\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 89.7051 - val_loss: 88.1922\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 89.0149 - val_loss: 87.4097\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.0269 - val_loss: 86.6898\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.8669 - val_loss: 83.8081\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.2941 - val_loss: 84.9845\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 87.8437 - val_loss: 85.9637\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 87.8393 - val_loss: 85.0425\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 87.5112 - val_loss: 84.3657\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.1188 - val_loss: 84.2859\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.0744 - val_loss: 84.0315\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 86.8367 - val_loss: 82.3436\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 86.6649 - val_loss: 82.1029\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.4743 - val_loss: 82.8812\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.1824 - val_loss: 83.0255\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.2994 - val_loss: 83.0064\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.9625 - val_loss: 81.7029\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.3893 - val_loss: 82.2278\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.8581 - val_loss: 81.7252\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.7635 - val_loss: 81.1581\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.3904 - val_loss: 80.5705\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.3536 - val_loss: 79.6856\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.5162 - val_loss: 79.5166\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.0799 - val_loss: 82.4087\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.0708 - val_loss: 82.5675\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.2470 - val_loss: 81.4674\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 84.5918 - val_loss: 79.7235\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.5739 - val_loss: 78.9496\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.5974 - val_loss: 82.3002\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 84.7770 - val_loss: 83.7439\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.5299 - val_loss: 81.9282\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.9852 - val_loss: 81.0953\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.7809 - val_loss: 82.9110\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 84.4277 - val_loss: 82.3071\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.2931 - val_loss: 79.7130\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.3441 - val_loss: 78.6660\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.1845 - val_loss: 80.9927\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 84.0832 - val_loss: 80.1003\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.9700 - val_loss: 80.5857\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.8334 - val_loss: 80.7758\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 84.4719 - val_loss: 79.7520\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.7376 - val_loss: 78.9959\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.1504 - val_loss: 80.7820\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.4866 - val_loss: 81.4650\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.7814 - val_loss: 81.1543\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 83.6642 - val_loss: 80.8222\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 85.0144\n",
            "85.014404296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2019"
      ],
      "metadata": {
        "id": "Rfn6fHWt0nXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "qmWWbJe2xFOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with inputs and outputs\n",
        "df = collisions2019\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_da_2019 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_da_2019)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7348d25-0da6-435f-bf03-d3a19f5094ff",
        "id": "BUZZ-Vw8xFOp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 40ms/step - loss: 583.8527 - val_loss: 590.4064\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 583.4555 - val_loss: 589.9513\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 582.9665 - val_loss: 589.3509\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 582.3026 - val_loss: 588.5182\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 581.3734 - val_loss: 587.3040\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 580.0069 - val_loss: 585.5125\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 577.9570 - val_loss: 582.8386\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 574.9501 - val_loss: 578.9448\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 570.5172 - val_loss: 573.4581\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 564.5927 - val_loss: 565.8599\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 556.1938 - val_loss: 555.6127\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 545.0388 - val_loss: 542.0282\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 530.3536 - val_loss: 524.3412\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 511.2985 - val_loss: 501.7717\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 487.1559 - val_loss: 473.1901\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 456.4915 - val_loss: 437.6188\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 418.6850 - val_loss: 393.6657\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 372.4374 - val_loss: 339.9341\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 315.7208 - val_loss: 275.3648\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 250.0036 - val_loss: 202.7596\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 181.9447 - val_loss: 144.2340\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 133.1719 - val_loss: 122.3351\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 122.0423 - val_loss: 124.0956\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 124.5731 - val_loss: 124.4935\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 122.7866 - val_loss: 119.4345\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 116.9725 - val_loss: 116.2785\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 115.1979 - val_loss: 115.5368\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 113.7812 - val_loss: 114.4999\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 112.1975 - val_loss: 112.5860\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 110.5248 - val_loss: 110.2727\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 108.7978 - val_loss: 108.1110\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 107.3089 - val_loss: 106.3301\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 105.7742 - val_loss: 104.7116\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 104.2281 - val_loss: 103.0844\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 102.8627 - val_loss: 101.2225\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 101.1980 - val_loss: 99.3479\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 99.6709 - val_loss: 97.6312\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 98.0491 - val_loss: 95.6194\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 96.5776 - val_loss: 93.9810\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 95.0636 - val_loss: 92.5139\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 93.6240 - val_loss: 91.7453\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 92.1645 - val_loss: 90.2540\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 91.0817 - val_loss: 88.8678\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 89.9821 - val_loss: 87.6264\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.7835 - val_loss: 86.3152\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.8137 - val_loss: 85.3083\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 87.0597 - val_loss: 84.5217\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 86.3156 - val_loss: 83.8050\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 85.6294 - val_loss: 82.9789\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 85.0927 - val_loss: 81.8803\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.5662 - val_loss: 82.1215\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.2170 - val_loss: 81.7133\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 83.2360 - val_loss: 80.5942\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.6248 - val_loss: 80.4111\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 82.1863 - val_loss: 80.1928\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 81.9078 - val_loss: 79.9290\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.3379 - val_loss: 78.8977\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.8119 - val_loss: 79.1830\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 80.4615 - val_loss: 78.6291\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 80.0357 - val_loss: 77.7310\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.8989 - val_loss: 77.3294\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.5025 - val_loss: 77.8027\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.2397 - val_loss: 77.3900\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.9263 - val_loss: 77.3958\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 78.5079 - val_loss: 76.7875\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.2725 - val_loss: 76.4702\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 77.9604 - val_loss: 76.7188\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.8031 - val_loss: 77.1071\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.6191 - val_loss: 76.8800\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 77.0759 - val_loss: 76.0093\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.0253 - val_loss: 76.0797\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.9895 - val_loss: 76.0185\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 76.4160 - val_loss: 76.3255\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.5298 - val_loss: 76.3443\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 76.1102 - val_loss: 75.7526\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.9042 - val_loss: 75.2621\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.8026 - val_loss: 76.0304\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.5041 - val_loss: 76.2465\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.2932 - val_loss: 76.2030\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.1000 - val_loss: 75.9313\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 74.8843 - val_loss: 75.5715\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 74.8733 - val_loss: 75.2430\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 74.7426 - val_loss: 75.6766\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 74.6618 - val_loss: 75.2799\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 74.5419 - val_loss: 75.5343\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 74.3429 - val_loss: 75.6654\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.4503 - val_loss: 75.2951\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.9715 - val_loss: 74.1114\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.1987 - val_loss: 74.3281\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.2113 - val_loss: 74.9933\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.1599 - val_loss: 74.6412\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.0225 - val_loss: 74.9894\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.9643 - val_loss: 74.9102\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 74.1377 - val_loss: 74.6673\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 73.7752 - val_loss: 74.5651\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.7597 - val_loss: 74.0524\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 73.6849 - val_loss: 74.3898\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 73.5786 - val_loss: 74.3585\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 73.5918 - val_loss: 74.6692\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.9697 - val_loss: 74.9248\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 73.4600 - val_loss: 74.4581\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.4835 - val_loss: 73.8847\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.4974 - val_loss: 74.1041\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 73.4290 - val_loss: 74.3917\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.6159 - val_loss: 73.9437\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 73.3838 - val_loss: 74.0760\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 19ms/step - loss: 73.2656 - val_loss: 73.8738\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 73.1532 - val_loss: 73.9196\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 73.2720 - val_loss: 73.6471\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 72.8815 - val_loss: 73.6355\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 73.0897 - val_loss: 73.9150\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.0014 - val_loss: 73.7472\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 73.0915 - val_loss: 73.8317\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 72.8703 - val_loss: 74.2306\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.3076 - val_loss: 73.7616\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.8384 - val_loss: 74.9215\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.1334 - val_loss: 73.7829\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 72.6106 - val_loss: 73.3566\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 73.1015 - val_loss: 73.2448\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.8149 - val_loss: 74.3901\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 72.8799 - val_loss: 75.1216\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 72.8982 - val_loss: 74.2820\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.6519 - val_loss: 74.5353\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.8850 - val_loss: 74.3537\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.5217 - val_loss: 73.9813\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.6939 - val_loss: 73.8639\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 72.5958 - val_loss: 74.6262\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.7440 - val_loss: 74.0065\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 72.8404 - val_loss: 73.5980\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 74.2566\n",
            "74.25664520263672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "6-vpzx9uxFOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2019 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2019)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P81apEsYxFOq",
        "outputId": "e1855456-50d8-4360-b0ba-a5099ed92ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 39ms/step - loss: 583.8965 - val_loss: 590.5008\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 583.5633 - val_loss: 590.1128\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 583.1177 - val_loss: 589.5745\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 582.5013 - val_loss: 588.8044\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 581.6105 - val_loss: 587.7010\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 580.3372 - val_loss: 586.1274\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 578.5296 - val_loss: 583.8606\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 575.9067 - val_loss: 580.5814\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 572.0884 - val_loss: 575.9038\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 566.8242 - val_loss: 569.3987\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 559.4782 - val_loss: 560.5405\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 549.5760 - val_loss: 548.6921\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 536.4778 - val_loss: 533.0865\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 519.2127 - val_loss: 512.9017\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 497.2097 - val_loss: 486.9920\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 468.9602 - val_loss: 454.3324\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 433.6494 - val_loss: 413.7281\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 389.9745 - val_loss: 363.8150\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 336.7520 - val_loss: 303.2634\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 273.4929 - val_loss: 230.6502\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 203.1224 - val_loss: 156.7563\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 143.9816 - val_loss: 116.0540\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 116.8683 - val_loss: 116.1297\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 119.8300 - val_loss: 120.7925\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 120.3688 - val_loss: 115.6343\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 113.9368 - val_loss: 109.9650\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 111.3743 - val_loss: 108.0436\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 111.4458 - val_loss: 107.1515\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 109.6125 - val_loss: 106.1839\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 107.9871 - val_loss: 105.1757\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 106.2025 - val_loss: 104.1575\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 105.2660 - val_loss: 103.1402\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 103.9756 - val_loss: 101.9028\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 102.9218 - val_loss: 100.6587\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 101.7519 - val_loss: 99.4636\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 100.5443 - val_loss: 98.3726\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.6105 - val_loss: 97.7355\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 98.5974 - val_loss: 96.6248\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 97.6421 - val_loss: 95.1756\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.2870 - val_loss: 94.1341\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 95.4265 - val_loss: 93.1204\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 94.3151 - val_loss: 92.1459\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 93.6547 - val_loss: 91.2184\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 92.9248 - val_loss: 89.9001\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 91.6339 - val_loss: 89.1230\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 90.4601 - val_loss: 88.1441\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.6561 - val_loss: 86.9480\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 88.8356 - val_loss: 86.0942\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 87.9156 - val_loss: 85.1401\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 87.1666 - val_loss: 84.3800\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 86.9792 - val_loss: 83.4526\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 85.5027 - val_loss: 82.9722\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.0758 - val_loss: 83.0095\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.4955 - val_loss: 82.0217\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.9001 - val_loss: 80.8617\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 83.2906 - val_loss: 80.2771\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 82.8304 - val_loss: 80.3403\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 82.2570 - val_loss: 79.4530\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.4703 - val_loss: 79.0423\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.1724 - val_loss: 78.7890\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.4066 - val_loss: 78.5331\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.9450 - val_loss: 78.1702\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 79.4415 - val_loss: 77.7096\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.2762 - val_loss: 77.3223\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.8340 - val_loss: 77.3065\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 78.3162 - val_loss: 77.4690\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.2476 - val_loss: 77.4147\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 77.8682 - val_loss: 77.0436\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 77.7577 - val_loss: 76.9416\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 77.1153 - val_loss: 76.5290\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 77.0276 - val_loss: 76.3955\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.7399 - val_loss: 76.4902\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.7221 - val_loss: 76.1039\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.5266 - val_loss: 75.8427\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.0764 - val_loss: 75.9637\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.8666 - val_loss: 75.9035\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 75.6507 - val_loss: 75.8641\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 75.4885 - val_loss: 75.8194\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.7613 - val_loss: 75.6459\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.3166 - val_loss: 75.4609\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 75.1950 - val_loss: 75.2881\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 75.2751 - val_loss: 75.5632\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 74.6271 - val_loss: 75.4200\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.7375 - val_loss: 75.5822\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 74.6761 - val_loss: 75.3813\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.4411 - val_loss: 75.7681\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.3526 - val_loss: 75.2759\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.8993 - val_loss: 75.1125\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.1015 - val_loss: 75.4277\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 73.9772 - val_loss: 74.8710\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.7716 - val_loss: 75.1674\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.8412 - val_loss: 75.5486\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 73.4477 - val_loss: 75.5179\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.3300 - val_loss: 75.3813\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.3302 - val_loss: 74.9132\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 73.6134 - val_loss: 74.3126\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.0467 - val_loss: 74.2276\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.8907 - val_loss: 74.7863\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.1371 - val_loss: 74.3908\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 72.5051 - val_loss: 74.4987\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.2222 - val_loss: 74.6329\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.0844 - val_loss: 75.5784\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 72.7773 - val_loss: 74.6687\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.9166 - val_loss: 73.5738\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 72.5367 - val_loss: 73.8467\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 72.4146 - val_loss: 73.8102\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 72.3315 - val_loss: 74.2729\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.1982 - val_loss: 74.1737\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 72.7319 - val_loss: 73.2443\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.7042 - val_loss: 74.2197\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.4837 - val_loss: 73.9550\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 71.9121 - val_loss: 73.6664\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 72.0071 - val_loss: 73.4644\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 71.8247 - val_loss: 74.0657\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 71.8572 - val_loss: 73.8763\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 72.0450 - val_loss: 73.4102\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 71.7602 - val_loss: 73.6541\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 71.7341 - val_loss: 73.5097\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 71.5246 - val_loss: 73.3650\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 75.2310\n",
            "75.23103332519531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "_UbJwyovxFOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2019 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2019)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1744cd-3fbc-43c2-c99d-9540a33b3b0b",
        "id": "VeDTNpAjxFOq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 38ms/step - loss: 583.9755 - val_loss: 590.5839\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 583.6876 - val_loss: 590.2715\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 583.3714 - val_loss: 589.8983\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 582.9858 - val_loss: 589.4075\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 582.4519 - val_loss: 588.7173\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 581.6932 - val_loss: 587.7269\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 580.5861 - val_loss: 586.2725\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 578.9841 - val_loss: 584.1461\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 576.6559 - val_loss: 581.1038\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 573.3933 - val_loss: 576.8041\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 568.7272 - val_loss: 570.8839\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 562.4324 - val_loss: 562.8348\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 553.8026 - val_loss: 552.0930\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 542.4735 - val_loss: 537.9035\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 527.5103 - val_loss: 519.3930\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 508.1709 - val_loss: 495.5761\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 483.2967 - val_loss: 465.4151\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 452.2022 - val_loss: 427.6677\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 413.0219 - val_loss: 380.9226\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 366.0253 - val_loss: 323.6691\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 309.0011 - val_loss: 257.0103\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 246.4683 - val_loss: 201.9297\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 184.4666 - val_loss: 173.4527\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 154.1663 - val_loss: 170.1049\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 149.5844 - val_loss: 172.1801\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 147.5882 - val_loss: 166.2730\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 141.9807 - val_loss: 157.9083\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 136.6025 - val_loss: 151.2129\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 133.9946 - val_loss: 146.9041\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 131.6255 - val_loss: 143.7339\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 129.4963 - val_loss: 140.4153\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 126.5716 - val_loss: 137.1361\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 124.1286 - val_loss: 133.6670\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 121.8645 - val_loss: 131.1733\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 120.0186 - val_loss: 128.4746\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 117.9181 - val_loss: 125.9690\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 116.2637 - val_loss: 123.0900\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 114.3188 - val_loss: 120.4600\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 112.3802 - val_loss: 117.8751\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 110.6620 - val_loss: 115.6409\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 108.8931 - val_loss: 113.4475\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 107.0453 - val_loss: 110.9419\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 105.8473 - val_loss: 108.6770\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 104.1190 - val_loss: 106.4818\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 102.2742 - val_loss: 104.7085\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 101.0350 - val_loss: 103.2463\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 99.3119 - val_loss: 100.8340\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 97.9313 - val_loss: 98.5938\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 96.3252 - val_loss: 96.7963\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 95.0608 - val_loss: 95.2581\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 93.6607 - val_loss: 93.4271\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 92.3260 - val_loss: 91.5586\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 91.0518 - val_loss: 89.8989\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 89.7072 - val_loss: 88.4301\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 88.5356 - val_loss: 87.3682\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 87.4703 - val_loss: 86.5302\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 86.8127 - val_loss: 85.8679\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 85.8154 - val_loss: 84.0651\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 84.7892 - val_loss: 82.7513\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 84.1440 - val_loss: 82.3209\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 83.3386 - val_loss: 81.0409\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 82.5117 - val_loss: 80.4223\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 81.8261 - val_loss: 79.6023\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 81.6366 - val_loss: 79.9239\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 80.7982 - val_loss: 77.9802\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 80.3662 - val_loss: 77.0154\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.9614 - val_loss: 77.3199\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.4728 - val_loss: 77.4707\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 79.1248 - val_loss: 76.9751\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.4891 - val_loss: 77.4657\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 78.1647 - val_loss: 77.5051\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 78.0057 - val_loss: 78.4416\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.9040 - val_loss: 78.0938\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 77.3444 - val_loss: 78.0702\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 77.0909 - val_loss: 77.3507\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 76.8293 - val_loss: 76.6374\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 77.0352 - val_loss: 77.0451\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 76.2678 - val_loss: 76.7945\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.9309 - val_loss: 76.7330\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 76.0130 - val_loss: 76.3177\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 76.0990 - val_loss: 77.0383\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.5377 - val_loss: 76.7754\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 75.8212 - val_loss: 76.1989\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 75.4506 - val_loss: 75.7421\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 75.4295 - val_loss: 75.3609\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.8022 - val_loss: 75.9008\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.6149 - val_loss: 76.3033\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 74.5687 - val_loss: 76.7111\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.3734 - val_loss: 75.6043\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 74.3501 - val_loss: 75.1261\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 74.0875 - val_loss: 75.6103\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 73.9749 - val_loss: 75.4783\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.9907 - val_loss: 75.2818\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.9489 - val_loss: 75.2979\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.7900 - val_loss: 75.2612\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 73.7373 - val_loss: 75.1617\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.4855 - val_loss: 75.2200\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.6456 - val_loss: 75.3827\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 73.3344 - val_loss: 75.3842\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 73.2678 - val_loss: 75.8112\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 76.1820\n",
            "76.18201446533203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Collisions in 2020"
      ],
      "metadata": {
        "id": "D7HG9gXq0pzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "4gO3G2EGxKyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Dataframe with inputs and outputs\n",
        "df = collisions2012\n",
        "\n",
        "X = df[['max', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2020 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2020)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1095938b-136d-48be-ec51-470689bdba52",
        "id": "2ZaAKcuwxKyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 2s 145ms/step - loss: 548.9036 - val_loss: 561.3947\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 548.7592 - val_loss: 561.2681\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 548.6322 - val_loss: 561.1433\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 548.5085 - val_loss: 561.0173\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 548.3779 - val_loss: 560.8831\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 548.2408 - val_loss: 560.7319\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 548.0889 - val_loss: 560.5628\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 547.9142 - val_loss: 560.3714\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 547.7188 - val_loss: 560.1517\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 547.4951 - val_loss: 559.8977\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 547.2363 - val_loss: 559.6016\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 546.9292 - val_loss: 559.2523\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 546.5719 - val_loss: 558.8364\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 546.1351 - val_loss: 558.3395\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 545.6192 - val_loss: 557.7449\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 545.0063 - val_loss: 557.0385\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 544.2890 - val_loss: 556.1957\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 543.4224 - val_loss: 555.2004\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 542.3824 - val_loss: 554.0251\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 541.1782 - val_loss: 552.6341\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 539.7619 - val_loss: 550.9902\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 538.0532 - val_loss: 549.0692\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 536.1151 - val_loss: 546.8121\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 533.7380 - val_loss: 544.1913\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 531.1028 - val_loss: 541.1346\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 528.0142 - val_loss: 537.5944\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 524.3384 - val_loss: 533.5220\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 520.2178 - val_loss: 528.8381\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 515.4342 - val_loss: 523.4808\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 510.0016 - val_loss: 517.3904\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 503.8029 - val_loss: 510.4839\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 496.7765 - val_loss: 502.6845\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 488.9909 - val_loss: 493.8945\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 480.0335 - val_loss: 484.0512\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 470.1527 - val_loss: 473.0201\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 459.0345 - val_loss: 460.7156\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 446.7797 - val_loss: 447.0194\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 432.8503 - val_loss: 431.9019\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 417.7484 - val_loss: 415.1555\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 400.9266 - val_loss: 396.7148\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 382.3866 - val_loss: 376.4590\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 362.2455 - val_loss: 354.1919\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 339.9111 - val_loss: 329.8771\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 315.4174 - val_loss: 303.4021\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 289.0092 - val_loss: 274.5501\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 260.9439 - val_loss: 243.2627\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 231.3427 - val_loss: 214.6679\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 204.0186 - val_loss: 186.4469\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 178.2436 - val_loss: 158.2437\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 157.2644 - val_loss: 134.8580\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 140.2310 - val_loss: 118.1979\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 128.5870 - val_loss: 113.2514\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 123.0765 - val_loss: 113.7888\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 123.3394 - val_loss: 114.8545\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 123.4973 - val_loss: 115.3570\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 122.2593 - val_loss: 114.4736\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 120.5884 - val_loss: 113.0588\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 118.0119 - val_loss: 110.4605\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 115.3710 - val_loss: 107.1647\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 112.1021 - val_loss: 105.4523\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 109.0804 - val_loss: 103.6724\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 107.9974 - val_loss: 102.5134\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 106.9191 - val_loss: 101.3307\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 105.7377 - val_loss: 99.6951\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 103.8587 - val_loss: 98.3204\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 102.6113 - val_loss: 96.9800\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 102.0242 - val_loss: 96.2347\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 101.0944 - val_loss: 95.3615\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 100.1294 - val_loss: 94.5884\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 98.9396 - val_loss: 93.5717\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 97.9659 - val_loss: 92.8894\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 97.3593 - val_loss: 92.3601\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 96.5809 - val_loss: 91.5725\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 95.7757 - val_loss: 90.4763\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 94.9427 - val_loss: 89.5400\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 94.3588 - val_loss: 88.3637\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 93.6618 - val_loss: 87.3108\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 93.1963 - val_loss: 86.3728\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 92.6774 - val_loss: 85.7475\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 92.2488 - val_loss: 85.2242\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 91.6743 - val_loss: 84.5617\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 91.0851 - val_loss: 84.1379\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 90.6685 - val_loss: 83.5061\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 90.2546 - val_loss: 82.9224\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 90.0838 - val_loss: 82.1341\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 89.3653 - val_loss: 81.6906\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 88.9172 - val_loss: 81.2331\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 88.5776 - val_loss: 80.9052\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 88.0987 - val_loss: 80.7709\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 87.8117 - val_loss: 80.5460\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 87.2772 - val_loss: 80.0043\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 87.1333 - val_loss: 79.4074\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 86.4367 - val_loss: 79.0261\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 85.8908 - val_loss: 78.7277\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 85.7032 - val_loss: 78.4573\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 85.1961 - val_loss: 78.3046\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 84.7396 - val_loss: 77.9398\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 84.3116 - val_loss: 77.7982\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 83.9262 - val_loss: 77.6848\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 83.4486 - val_loss: 77.4054\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 83.0037 - val_loss: 77.0552\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 82.7034 - val_loss: 76.7512\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 82.3094 - val_loss: 76.3315\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 81.8554 - val_loss: 76.0743\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 81.7391 - val_loss: 75.5835\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 81.1408 - val_loss: 75.3889\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.7887 - val_loss: 75.0806\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 80.4231 - val_loss: 75.0026\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 80.0863 - val_loss: 74.7937\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 79.7981 - val_loss: 74.4314\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 79.3939 - val_loss: 74.1186\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 79.0925 - val_loss: 74.0127\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 78.7965 - val_loss: 74.0706\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 78.5150 - val_loss: 74.1505\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 78.2469 - val_loss: 74.1603\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 78.0415 - val_loss: 73.8367\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 77.8471 - val_loss: 73.4367\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 77.5329 - val_loss: 73.3223\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 77.2185 - val_loss: 73.0787\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.1484 - val_loss: 72.8485\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 76.9784 - val_loss: 72.9890\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 76.6831 - val_loss: 72.9732\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 76.4148 - val_loss: 72.8167\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 76.1873 - val_loss: 72.5539\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 76.0397 - val_loss: 72.4386\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 75.8654 - val_loss: 72.2840\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 75.6229 - val_loss: 72.4717\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 75.5996 - val_loss: 72.4192\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 75.2701 - val_loss: 72.0251\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 74.9947 - val_loss: 71.9486\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.8194 - val_loss: 71.7891\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 74.6464 - val_loss: 71.8129\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.5062 - val_loss: 71.4813\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 74.3779 - val_loss: 71.3592\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 74.0863 - val_loss: 71.3243\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.9448 - val_loss: 71.4682\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.8507 - val_loss: 71.3053\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 73.8390 - val_loss: 71.1540\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.5976 - val_loss: 70.5775\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.5622 - val_loss: 70.5200\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.2895 - val_loss: 70.7706\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 73.0544 - val_loss: 71.0321\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 73.0026 - val_loss: 70.5533\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.7447 - val_loss: 70.8006\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 72.6218 - val_loss: 70.7464\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 72.4777 - val_loss: 71.3657\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 72.4225 - val_loss: 71.4042\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.3559 - val_loss: 71.4842\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.2414 - val_loss: 71.9455\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.1044 - val_loss: 71.6649\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 73.9433\n",
            "73.94332122802734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "_JJgkctdxKyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[2],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_min_da_2020 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_min_da_2020)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJAL63ZNxKyW",
        "outputId": "07735067-c0bd-4ae4-99cc-8a4ebede91dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 1s 89ms/step - loss: 548.7887 - val_loss: 561.2319\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 548.5966 - val_loss: 561.0512\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 548.4153 - val_loss: 560.8570\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 548.2092 - val_loss: 560.6304\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 547.9733 - val_loss: 560.3719\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 547.6923 - val_loss: 560.0678\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 547.3611 - val_loss: 559.7048\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 546.9750 - val_loss: 559.2608\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 546.5081 - val_loss: 558.7237\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 545.9304 - val_loss: 558.0757\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 545.2422 - val_loss: 557.2996\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 544.4020 - val_loss: 556.3673\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 543.4247 - val_loss: 555.2372\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 542.2203 - val_loss: 553.8760\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 540.7864 - val_loss: 552.2390\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 539.0351 - val_loss: 550.2809\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 536.9715 - val_loss: 547.9448\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 534.5323 - val_loss: 545.1624\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 531.5969 - val_loss: 541.8918\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 528.1679 - val_loss: 538.0379\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 524.0810 - val_loss: 533.5306\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 519.4066 - val_loss: 528.2716\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 513.9178 - val_loss: 522.1621\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 507.5020 - val_loss: 515.1185\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 500.2563 - val_loss: 507.0071\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 491.9012 - val_loss: 497.7031\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 482.1883 - val_loss: 487.1147\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 471.2383 - val_loss: 475.1031\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 458.8785 - val_loss: 461.4970\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 444.8746 - val_loss: 446.1377\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 429.0978 - val_loss: 428.9108\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 411.3368 - val_loss: 409.6166\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 391.6893 - val_loss: 388.0016\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 369.5443 - val_loss: 363.9424\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 344.9087 - val_loss: 337.2041\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 317.7968 - val_loss: 307.5565\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 289.0247 - val_loss: 275.2799\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 261.1557 - val_loss: 240.8410\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 232.4661 - val_loss: 205.6832\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 204.0267 - val_loss: 178.3055\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 175.7086 - val_loss: 161.6559\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 151.8689 - val_loss: 150.1942\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 137.2068 - val_loss: 141.8943\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 127.2878 - val_loss: 140.5371\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 126.6440 - val_loss: 141.6296\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 44ms/step - loss: 127.0898 - val_loss: 142.1598\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 128.2513 - val_loss: 141.0668\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 126.7907 - val_loss: 138.6427\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 123.9802 - val_loss: 134.5898\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 120.1009 - val_loss: 129.9935\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 116.7595 - val_loss: 125.8824\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 114.9647 - val_loss: 122.0332\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 112.0386 - val_loss: 118.7596\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 110.6602 - val_loss: 116.0111\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 109.4557 - val_loss: 113.8650\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 108.4383 - val_loss: 112.1115\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 107.0074 - val_loss: 110.5734\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 105.9125 - val_loss: 109.4672\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 104.0433 - val_loss: 108.2978\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 102.6958 - val_loss: 107.1066\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 101.4558 - val_loss: 106.0033\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 36ms/step - loss: 100.4747 - val_loss: 104.8394\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 99.4269 - val_loss: 103.6734\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 98.4094 - val_loss: 102.4280\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 97.5747 - val_loss: 101.0007\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 96.4857 - val_loss: 100.0237\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 95.6806 - val_loss: 99.0617\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 94.8109 - val_loss: 98.2587\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 93.8095 - val_loss: 97.5637\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 93.0488 - val_loss: 97.0134\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 92.2361 - val_loss: 96.5295\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 91.4392 - val_loss: 95.9430\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 90.7321 - val_loss: 95.4355\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 35ms/step - loss: 89.9734 - val_loss: 94.9043\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 89.3882 - val_loss: 94.5085\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 88.7056 - val_loss: 94.1698\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 88.0607 - val_loss: 93.6522\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 87.4260 - val_loss: 93.3908\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 86.7179 - val_loss: 92.8274\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 86.3599 - val_loss: 92.3496\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 85.7877 - val_loss: 91.3914\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 85.3219 - val_loss: 90.8074\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 84.8400 - val_loss: 90.2743\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 84.2965 - val_loss: 90.0005\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 83.8551 - val_loss: 89.6911\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 83.4421 - val_loss: 89.1140\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 82.9781 - val_loss: 88.4909\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 82.6450 - val_loss: 88.2039\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 82.2943 - val_loss: 87.5934\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 81.9674 - val_loss: 87.1890\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 81.5365 - val_loss: 86.7622\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 81.0780 - val_loss: 86.0875\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 80.7509 - val_loss: 85.5266\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 80.6846 - val_loss: 85.0631\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.1742 - val_loss: 84.7574\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 79.9129 - val_loss: 84.8723\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 79.7902 - val_loss: 84.5224\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 79.2413 - val_loss: 83.5589\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 78.9812 - val_loss: 82.9887\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 78.6682 - val_loss: 82.7353\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 78.2193 - val_loss: 82.6002\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.9338 - val_loss: 82.5512\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.7413 - val_loss: 82.0122\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 77.3361 - val_loss: 81.4547\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.1211 - val_loss: 81.2865\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 76.7864 - val_loss: 80.8674\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 76.6388 - val_loss: 80.4300\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 76.3568 - val_loss: 80.1030\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 76.0530 - val_loss: 79.8351\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 75.7850 - val_loss: 79.6240\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 75.5413 - val_loss: 79.1516\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 75.3321 - val_loss: 78.6791\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 75.1107 - val_loss: 78.3248\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 74.9760 - val_loss: 77.9754\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.7888 - val_loss: 78.2584\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 74.5620 - val_loss: 77.9064\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 74.3563 - val_loss: 77.6502\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 74.2536 - val_loss: 77.0542\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.0448 - val_loss: 76.8643\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.8282 - val_loss: 76.8581\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 73.5656 - val_loss: 76.8517\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 73.5846 - val_loss: 76.5930\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 73.3410 - val_loss: 76.3986\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 73.1529 - val_loss: 76.2778\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.0624 - val_loss: 75.9866\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 72.8238 - val_loss: 75.9519\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 72.6545 - val_loss: 75.7818\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.5271 - val_loss: 75.5345\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.4985 - val_loss: 75.4535\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.2216 - val_loss: 75.2444\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.2024 - val_loss: 75.2065\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.0011 - val_loss: 75.0496\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 72.1494 - val_loss: 75.0270\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.8384 - val_loss: 75.2971\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 71.8563 - val_loss: 74.9612\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 71.6431 - val_loss: 74.6989\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 71.6862 - val_loss: 74.3620\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 71.4137 - val_loss: 74.3449\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 71.2425 - val_loss: 74.3979\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 71.1840 - val_loss: 74.4481\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.1280 - val_loss: 74.1523\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 70.9685 - val_loss: 73.9277\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.8803 - val_loss: 73.6773\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.9374 - val_loss: 73.5426\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 70.8882 - val_loss: 73.9544\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.6852 - val_loss: 73.7561\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 70.8015 - val_loss: 73.4068\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 70.5065 - val_loss: 73.8485\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 70.3798 - val_loss: 73.9509\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.3822 - val_loss: 73.7980\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 70.1750 - val_loss: 73.6929\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.1535 - val_loss: 73.5065\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 70.0558 - val_loss: 73.5621\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 70.0047 - val_loss: 73.1766\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 69.9682 - val_loss: 73.1654\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 69.7864 - val_loss: 73.6492\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 69.7508 - val_loss: 73.9641\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 69.7131 - val_loss: 73.8756\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 69.7186 - val_loss: 74.1879\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 69.7588 - val_loss: 73.5387\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 69.4399 - val_loss: 73.4356\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 69.3187 - val_loss: 73.6574\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 69.2775 - val_loss: 73.9888\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 69.1611 - val_loss: 73.9449\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 69.0864 - val_loss: 73.9795\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 71.7998\n",
            "71.79981994628906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Max/Min and Day vs. NUM_COLLISIONS"
      ],
      "metadata": {
        "id": "IjNNWmvhxKyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['max', 'min', 'da']]\n",
        "y = df['NUM_COLLISIONS']\n",
        "# Normalize the input features\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.7, random_state=42)\n",
        "\n",
        "# Normalize the input features for training, validation, and testing sets\n",
        "X_train = (X_train - X.mean()) / X.std()\n",
        "X_val = (X_val - X.mean()) / X.std()\n",
        "X_test = (X_test - X.mean()) / X.std()\n",
        "\n",
        "# Create the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(units=128, activation='relu', input_shape=[3],\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=32, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_absolute_error')\n",
        "\n",
        "# Train the model with a validation set\n",
        "history = model.fit(X_train, y_train, epochs=500, validation_data=(X_val, y_val),\n",
        "                     callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "mae_max_min_da_2020 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(mae_max_min_da_2020)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c871258-57ab-4f45-8654-876ff0c1a085",
        "id": "coFL8ImnxKyW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "3/3 [==============================] - 1s 91ms/step - loss: 548.8345 - val_loss: 561.2792\n",
            "Epoch 2/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 548.6279 - val_loss: 561.0961\n",
            "Epoch 3/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 548.4563 - val_loss: 560.9266\n",
            "Epoch 4/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 548.2819 - val_loss: 560.7415\n",
            "Epoch 5/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 548.0958 - val_loss: 560.5263\n",
            "Epoch 6/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 547.8713 - val_loss: 560.2724\n",
            "Epoch 7/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 547.6100 - val_loss: 559.9682\n",
            "Epoch 8/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 547.2969 - val_loss: 559.5970\n",
            "Epoch 9/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 546.9061 - val_loss: 559.1438\n",
            "Epoch 10/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 546.4333 - val_loss: 558.5894\n",
            "Epoch 11/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 545.8607 - val_loss: 557.9152\n",
            "Epoch 12/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 545.1493 - val_loss: 557.0932\n",
            "Epoch 13/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 544.2906 - val_loss: 556.0853\n",
            "Epoch 14/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 543.2147 - val_loss: 554.8510\n",
            "Epoch 15/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 541.9719 - val_loss: 553.3422\n",
            "Epoch 16/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 540.3949 - val_loss: 551.5082\n",
            "Epoch 17/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 538.5402 - val_loss: 549.2936\n",
            "Epoch 18/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 536.2667 - val_loss: 546.6448\n",
            "Epoch 19/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 533.5026 - val_loss: 543.4841\n",
            "Epoch 20/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 530.3539 - val_loss: 539.7212\n",
            "Epoch 21/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 526.4850 - val_loss: 535.2970\n",
            "Epoch 22/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 521.9644 - val_loss: 530.1104\n",
            "Epoch 23/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 516.8060 - val_loss: 524.0384\n",
            "Epoch 24/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 510.6557 - val_loss: 517.0068\n",
            "Epoch 25/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 503.6289 - val_loss: 508.8643\n",
            "Epoch 26/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 495.2535 - val_loss: 499.4980\n",
            "Epoch 27/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 485.8853 - val_loss: 488.7310\n",
            "Epoch 28/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 475.2006 - val_loss: 476.4013\n",
            "Epoch 29/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 462.9932 - val_loss: 462.3974\n",
            "Epoch 30/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 449.0170 - val_loss: 446.5646\n",
            "Epoch 31/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 433.3045 - val_loss: 428.6593\n",
            "Epoch 32/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 415.4225 - val_loss: 408.5396\n",
            "Epoch 33/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 395.1299 - val_loss: 385.9964\n",
            "Epoch 34/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 373.3023 - val_loss: 360.6419\n",
            "Epoch 35/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 347.9249 - val_loss: 332.5178\n",
            "Epoch 36/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 320.0457 - val_loss: 301.2673\n",
            "Epoch 37/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 292.9825 - val_loss: 267.5258\n",
            "Epoch 38/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 265.5852 - val_loss: 232.0451\n",
            "Epoch 39/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 236.7182 - val_loss: 202.6988\n",
            "Epoch 40/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 209.5011 - val_loss: 178.4439\n",
            "Epoch 41/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 181.6832 - val_loss: 166.1287\n",
            "Epoch 42/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 165.4954 - val_loss: 156.9390\n",
            "Epoch 43/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 153.7316 - val_loss: 152.3334\n",
            "Epoch 44/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 148.4063 - val_loss: 150.6752\n",
            "Epoch 45/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 144.0039 - val_loss: 154.7380\n",
            "Epoch 46/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 143.9724 - val_loss: 158.3481\n",
            "Epoch 47/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 144.4446 - val_loss: 159.3672\n",
            "Epoch 48/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 142.5831 - val_loss: 155.4669\n",
            "Epoch 49/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 139.3178 - val_loss: 150.7462\n",
            "Epoch 50/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 136.1236 - val_loss: 144.2812\n",
            "Epoch 51/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 133.2114 - val_loss: 138.3395\n",
            "Epoch 52/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 130.5241 - val_loss: 134.5808\n",
            "Epoch 53/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 129.3388 - val_loss: 131.5733\n",
            "Epoch 54/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 127.3779 - val_loss: 129.2616\n",
            "Epoch 55/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 125.7071 - val_loss: 127.0696\n",
            "Epoch 56/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 123.7366 - val_loss: 125.2132\n",
            "Epoch 57/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 122.2575 - val_loss: 123.4519\n",
            "Epoch 58/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 120.6136 - val_loss: 121.9270\n",
            "Epoch 59/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 119.1297 - val_loss: 119.7737\n",
            "Epoch 60/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 117.5252 - val_loss: 117.8764\n",
            "Epoch 61/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 115.9628 - val_loss: 115.3701\n",
            "Epoch 62/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 114.4562 - val_loss: 113.2243\n",
            "Epoch 63/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 113.1387 - val_loss: 110.1981\n",
            "Epoch 64/500\n",
            "3/3 [==============================] - 0s 31ms/step - loss: 111.7151 - val_loss: 107.7596\n",
            "Epoch 65/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 110.4048 - val_loss: 106.0358\n",
            "Epoch 66/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 109.2170 - val_loss: 104.4245\n",
            "Epoch 67/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 108.1040 - val_loss: 103.2859\n",
            "Epoch 68/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 106.7416 - val_loss: 102.6777\n",
            "Epoch 69/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 105.5415 - val_loss: 102.1322\n",
            "Epoch 70/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 104.2747 - val_loss: 101.7879\n",
            "Epoch 71/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 103.3218 - val_loss: 101.1242\n",
            "Epoch 72/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 102.3561 - val_loss: 99.8631\n",
            "Epoch 73/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 101.4604 - val_loss: 98.6194\n",
            "Epoch 74/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 100.6470 - val_loss: 97.1223\n",
            "Epoch 75/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 99.8639 - val_loss: 95.9409\n",
            "Epoch 76/500\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 98.9856 - val_loss: 95.1751\n",
            "Epoch 77/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 98.3691 - val_loss: 94.4672\n",
            "Epoch 78/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 97.3569 - val_loss: 93.5975\n",
            "Epoch 79/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 96.5779 - val_loss: 92.8943\n",
            "Epoch 80/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 95.6004 - val_loss: 92.2668\n",
            "Epoch 81/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 94.9738 - val_loss: 91.6127\n",
            "Epoch 82/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 94.1071 - val_loss: 90.9880\n",
            "Epoch 83/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 93.4326 - val_loss: 90.2385\n",
            "Epoch 84/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 92.7625 - val_loss: 89.5969\n",
            "Epoch 85/500\n",
            "3/3 [==============================] - 0s 37ms/step - loss: 91.8789 - val_loss: 89.0760\n",
            "Epoch 86/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 91.2297 - val_loss: 88.3150\n",
            "Epoch 87/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 90.5140 - val_loss: 87.6857\n",
            "Epoch 88/500\n",
            "3/3 [==============================] - 0s 34ms/step - loss: 89.9388 - val_loss: 87.0830\n",
            "Epoch 89/500\n",
            "3/3 [==============================] - 0s 45ms/step - loss: 89.4450 - val_loss: 86.4496\n",
            "Epoch 90/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 89.2149 - val_loss: 85.9766\n",
            "Epoch 91/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 88.5076 - val_loss: 85.3479\n",
            "Epoch 92/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 88.1497 - val_loss: 84.9548\n",
            "Epoch 93/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 87.6115 - val_loss: 84.9597\n",
            "Epoch 94/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 87.0833 - val_loss: 84.7654\n",
            "Epoch 95/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 86.6932 - val_loss: 84.4968\n",
            "Epoch 96/500\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 86.6193 - val_loss: 84.1111\n",
            "Epoch 97/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 85.9199 - val_loss: 83.5692\n",
            "Epoch 98/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 85.5230 - val_loss: 83.1406\n",
            "Epoch 99/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 85.1807 - val_loss: 82.8622\n",
            "Epoch 100/500\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 84.9679 - val_loss: 82.5653\n",
            "Epoch 101/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 84.4513 - val_loss: 82.3497\n",
            "Epoch 102/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 84.0388 - val_loss: 82.3574\n",
            "Epoch 103/500\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 83.7725 - val_loss: 81.9145\n",
            "Epoch 104/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 83.2436 - val_loss: 81.4631\n",
            "Epoch 105/500\n",
            "3/3 [==============================] - 0s 33ms/step - loss: 82.9246 - val_loss: 81.1282\n",
            "Epoch 106/500\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 82.6654 - val_loss: 80.7175\n",
            "Epoch 107/500\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 82.4594 - val_loss: 80.6765\n",
            "Epoch 108/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 82.1044 - val_loss: 80.3268\n",
            "Epoch 109/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 81.6830 - val_loss: 80.3758\n",
            "Epoch 110/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 81.2873 - val_loss: 80.1539\n",
            "Epoch 111/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 80.9569 - val_loss: 79.9348\n",
            "Epoch 112/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 80.8335 - val_loss: 79.7085\n",
            "Epoch 113/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.5290 - val_loss: 79.7254\n",
            "Epoch 114/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.1060 - val_loss: 79.6124\n",
            "Epoch 115/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 80.0856 - val_loss: 79.1919\n",
            "Epoch 116/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 79.7511 - val_loss: 79.1545\n",
            "Epoch 117/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 79.4054 - val_loss: 78.9618\n",
            "Epoch 118/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 79.1482 - val_loss: 78.4693\n",
            "Epoch 119/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 78.9713 - val_loss: 78.4084\n",
            "Epoch 120/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 78.6844 - val_loss: 78.2049\n",
            "Epoch 121/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 78.3084 - val_loss: 78.0942\n",
            "Epoch 122/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 78.7375 - val_loss: 78.1634\n",
            "Epoch 123/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.8448 - val_loss: 77.7888\n",
            "Epoch 124/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 77.6769 - val_loss: 77.5224\n",
            "Epoch 125/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 77.4645 - val_loss: 77.4458\n",
            "Epoch 126/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 77.1763 - val_loss: 77.3254\n",
            "Epoch 127/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 76.9977 - val_loss: 77.2489\n",
            "Epoch 128/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 76.8182 - val_loss: 77.3509\n",
            "Epoch 129/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 76.9051 - val_loss: 77.2015\n",
            "Epoch 130/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 76.5311 - val_loss: 76.6205\n",
            "Epoch 131/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 76.2739 - val_loss: 76.3388\n",
            "Epoch 132/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 76.1360 - val_loss: 75.9140\n",
            "Epoch 133/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 75.9753 - val_loss: 75.8926\n",
            "Epoch 134/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 75.7088 - val_loss: 75.9868\n",
            "Epoch 135/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 75.4002 - val_loss: 75.9808\n",
            "Epoch 136/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 75.3209 - val_loss: 75.8357\n",
            "Epoch 137/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 75.1562 - val_loss: 75.7809\n",
            "Epoch 138/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 75.1864 - val_loss: 75.7885\n",
            "Epoch 139/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 74.8647 - val_loss: 75.3224\n",
            "Epoch 140/500\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 74.6126 - val_loss: 75.0665\n",
            "Epoch 141/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.4630 - val_loss: 74.8725\n",
            "Epoch 142/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.3397 - val_loss: 74.7497\n",
            "Epoch 143/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.1379 - val_loss: 74.7452\n",
            "Epoch 144/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 74.1594 - val_loss: 74.5709\n",
            "Epoch 145/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 73.8325 - val_loss: 74.5760\n",
            "Epoch 146/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 73.7932 - val_loss: 74.3059\n",
            "Epoch 147/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.5011 - val_loss: 74.1453\n",
            "Epoch 148/500\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 73.3660 - val_loss: 73.9929\n",
            "Epoch 149/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 73.3937 - val_loss: 73.7768\n",
            "Epoch 150/500\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 73.2504 - val_loss: 74.1741\n",
            "Epoch 151/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 73.1284 - val_loss: 74.3996\n",
            "Epoch 152/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 73.0397 - val_loss: 73.8211\n",
            "Epoch 153/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.8713 - val_loss: 73.4783\n",
            "Epoch 154/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 72.6532 - val_loss: 73.4610\n",
            "Epoch 155/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 72.4234 - val_loss: 73.4477\n",
            "Epoch 156/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 72.3194 - val_loss: 73.5304\n",
            "Epoch 157/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 72.1975 - val_loss: 73.6050\n",
            "Epoch 158/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 72.1649 - val_loss: 73.5394\n",
            "Epoch 159/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 71.9159 - val_loss: 73.5578\n",
            "Epoch 160/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.8019 - val_loss: 73.3064\n",
            "Epoch 161/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.6979 - val_loss: 73.1546\n",
            "Epoch 162/500\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 71.6911 - val_loss: 72.5934\n",
            "Epoch 163/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 71.4811 - val_loss: 72.7477\n",
            "Epoch 164/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.3211 - val_loss: 72.9573\n",
            "Epoch 165/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 71.2926 - val_loss: 73.5767\n",
            "Epoch 166/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 71.1389 - val_loss: 73.4724\n",
            "Epoch 167/500\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 71.1375 - val_loss: 72.8894\n",
            "Epoch 168/500\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 71.0245 - val_loss: 73.0313\n",
            "Epoch 169/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.9706 - val_loss: 73.7879\n",
            "Epoch 170/500\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 70.6904 - val_loss: 73.3980\n",
            "Epoch 171/500\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 70.5582 - val_loss: 73.4652\n",
            "Epoch 172/500\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 70.4036 - val_loss: 73.9749\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 73.7302\n",
            "73.73019409179688\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "UghKC0MVztfh",
        "aBjI0wSM638g",
        "j-hsFl117MJe",
        "Spq1bktW7Q2j",
        "MSG5qLJBzy8q",
        "KL577hdTwbZ9",
        "mWbUtdw5wbZ9",
        "6KiZoYwnwbZ9",
        "Q-yqMBp90LlF",
        "zvzclKFE0b9W",
        "_Klk15WkwrgX",
        "5JsTnY0Ow31b",
        "5mZ-m8AiwLXK"
      ],
      "authorship_tag": "ABX9TyMvsDt+Npwe1ielY2rf5lh/",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}